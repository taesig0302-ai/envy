
import os
import json
import re
import time
from datetime import datetime, timedelta

import pandas as pd
import requests
import streamlit as st
import altair as alt

APP_NAME = "ENVY"

# =====================
# Utilities
# =====================
def header():
    cols = st.columns([1,8,1])
    with cols[0]:
        # logo optional
        if Path("envy_logo.png").exists():
            st.image("envy_logo.png", use_column_width=True)
        else:
            st.markdown(f"### **{APP_NAME}**")
    with cols[1]:
        st.markdown(
            "<h2 style='margin:0'>ì‹¤ì‹œê°„ í™˜ìœ¨ + ğŸ“Š ë§ˆì§„ + ğŸ“ˆ ë°ì´í„°ë© + ğŸ›’ 11ë²ˆê°€ + âœï¸ ìƒí’ˆëª…(API)</h2>",
            unsafe_allow_html=True,
        )
    st.write("")

@st.cache_data(ttl=60*30)
def fetch_usdkrw():
    urls = [
        "https://api.exchangerate.host/latest?base=USD&symbols=KRW",
        "https://open.er-api.com/v6/latest/USD",
    ]
    for u in urls:
        try:
            r = requests.get(u, timeout=8)
            if r.ok:
                j = r.json()
                # exchangerate.host
                if "rates" in j and "KRW" in j["rates"]:
                    return float(j["rates"]["KRW"])
                # er-api
                if j.get("result") == "success" and "rates" in j:
                    return float(j["rates"]["KRW"])
        except Exception:
            pass
    return None

# =====================
# DataLab (mock + CSV + hooks)
# =====================
CATEGORY_SEEDS = {
    "íŒ¨ì…˜ì˜ë¥˜":["ë§¨íˆ¬ë§¨","ìŠ¬ë™ìŠ¤","ì²­ë°”ì§€","ì¹´ë¼í‹°","ë°”ëŒë§‰ì´","ë‹ˆíŠ¸","ê°€ë””ê±´","ë¡±ìŠ¤ì»¤íŠ¸","ë¶€ì¸ ì»·","ì™€ì´ë“œíŒ¬ì¸ ","ì¡°ê±°íŒ¬ì¸ ","ë°•ì‹œí‹°","íŒ¨ë”©ì¡°ë¼","í•˜í”„ì½”íŠ¸","í”Œë¦¬ì¸ ìŠ¤ì»¤íŠ¸","íŠ¸ë ˆì´ë‹ì…‹","ê³¨ë´íŒ¬ì¸ ","ìƒˆí‹´ìŠ¤ì»¤íŠ¸","ë¡±ê°€ë””ê±´","í¬ë¡­ë‹ˆíŠ¸"],
    "ìŠ¤í¬ì¸ /ë ˆì €":["ëŸ°ë‹í™”","í…Œë‹ˆìŠ¤ë¼ì¼“","ìš”ê°€ë³µ","ì¶•êµ¬ê³µ","í—¬ìŠ¤ì¥ê°‘","ë“±ì‚°ìŠ¤í‹±","ìº í•‘ì²´ì–´","ìì „ê±°í—¬ë©§","ìˆ˜ì˜ë³µ","ì•„ë…¸ë½","ë³´ë“œì›¨ì–´","ìŠ¤í‚¤ì¥ê°‘","ì•„ì´ì  ","ì²´ìœ¡ë³µ","ì‹¸ì´í´ìŠˆì¦ˆ","ë°œì—´ë‚´ì˜","ìŠ¤í¬ì¸ ë¸Œë¼","ìŠ¤í¬ì¸ ë ˆê¹…ìŠ¤","ê¸°ëŠ¥í‹°ì…”ì¸ ","ë°°êµ¬ê³µ"],
    "ì‹í’ˆ":["ë¼ë©´","ì»¤í”¼","ì°¸ì¹˜","ìŠ¤íŒ¸","ì´ˆì½œë¦¿","ê³¼ì","ì¹˜ì¦ˆ","ê¹€","ì–´ë¬µ","ìº”í–„","ê¹€ì¹˜","ì‹œë¦¬ì–¼","ê¿€","ì½©ë‚˜ë¬¼","ë‘ìœ ","ëƒ‰ë™ë§Œë‘","ìš°ìœ ","ì†Œì‹œì§€","ìŠ¤í…Œë¹„ì•„í† ë§ˆí† ","ê³ êµ¬ë§ˆ"],
}

def mock_ratios_from_keywords(keywords):
    rows = []
    base = 50
    for kw in keywords:
        seed = sum(bytearray(kw.encode("utf-8"))) % 30
        d1 = base + (seed % 11)
        d7 = base + (seed % 17) + 5
        d30 = base + (seed % 23) + 10
        rows.append({"keyword": kw, "day1": d1, "day7": d7, "day30": d30})
    return pd.DataFrame(rows)

def clean_keyword(s:str)->str:
    ss = s.strip()
    # ê°„ë‹¨ ì •ê·œí™”: ì¤‘ë³µê³µë°± ì œê±°, ìŠ¬ë˜ì‹œ/í•˜ì´í”ˆ í†µì¼
    ss = re.sub(r"[\/\-]+", " ", ss)
    ss = re.sub(r"\s+", " ", ss)
    return ss

def normalize_keywords(keywords):
    # ë™ì˜ì–´/ì² ì ë³€í˜• ë§µ (ì˜ˆì‹œ)
    norm_map = {
        "ë§¨íˆ¬ë§¨":"ë§¨íˆ¬ë§¨",
        "ë§¨íˆ¬ ë§¨":"ë§¨íˆ¬ë§¨",
        "í‹°ì…”ì¸ ":"í‹°ì…”ì¸ ",
        "í‹° ìƒ¤ì¸ ":"í‹°ì…”ì¸ ",
        "ë°ë‹˜ íŒ¬ì¸ ":"ì²­ë°”ì§€",
        "ë°ë‹˜":"ì²­ë°”ì§€",
        "ë°”ì´í¬ ì‡¼ì¸ ":"ë°”ì´í¬ì‡¼ì¸ ",
    }
    out = []
    for k in keywords:
        k2 = clean_keyword(k)
        out.append(norm_map.get(k2, k2))
    return out

def parse_uploaded_csv(file):
    # ê¸°ëŒ€ í¬ë§·: keyword[,day1,day7,day30] â€” ì—†ìœ¼ë©´ ëª¨ì˜ ê°’ ìƒì„±
    try:
        df = pd.read_csv(file)
        if "keyword" in df.columns:
            for c in ["day1","day7","day30"]:
                if c not in df.columns:
                    # ëª¨ì˜ë¡œ ì±„ìš°ê¸°
                    tmp = mock_ratios_from_keywords(df["keyword"].tolist())
                    df = df.merge(tmp, on="keyword", how="left")
                    break
            df["keyword"] = df["keyword"].astype(str).apply(clean_keyword)
            return df[["keyword","day1","day7","day30"]]
    except Exception:
        pass
    return None

# =====================
# 11st
# =====================
def fetch_11st_rows(proxy_base:str, ua:str):
    headers = {"User-Agent": ua} if ua else {}
    target = "https://m.11st.co.kr/browsing/AmazonBest"
    text = ""
    try:
        if proxy_base:
            url = proxy_base + target
            text = requests.get(url, headers=headers, timeout=8).text
        else:
            text = requests.get(target, headers=headers, timeout=8).text
    except Exception:
        text = ""

    rows = []
    try:
        names = re.findall(r'\\"productName\\"\\s*:\\s*\\"([^\\"]{3,120})\\"', text)
        prices = re.findall(r'\\"finalPrice\\"\\s*:\\s*\\"?(\\d[\\d,]{2,})\\"?', text)
        links  = re.findall(r'\\"detailUrl\\"\\s*:\\s*\\"([^\\"]+)\\"', text)
        for i, n in enumerate(names[:20]):
            price = prices[i] if i < len(prices) else ""
            link  = links[i]  if i < len(links)  else ""
            rows.append({"rank": i+1, "product": n, "price": price.replace(",", ""), "link": link})
    except Exception:
        rows = []

    if not rows:
        rows = [
            {"rank":1,"product":"ì• í”Œ ì—ì–´íŒŸ Pro (2ì„¸ëŒ€)","price":"329000","link":""},
            {"rank":2,"product":"ì‚¼ì„± ê°¤ëŸ­ì‹œ S23 256GB","price":"998000","link":""},
            {"rank":3,"product":"ë‚˜ì´í‚¤ ìš´ë™í™” ë ˆë³¼ë£¨ì…˜","price":"89000","link":""},
            {"rank":4,"product":"LG ë…¸íŠ¸ë¶ 16í˜• ì´ˆê²½ëŸ‰","price":"1399000","link":""},
            {"rank":5,"product":"ìŠ¤íƒ€ë²…ìŠ¤ í…€ë¸”ëŸ¬ 473ml","price":"23000","link":""},
        ]
    return pd.DataFrame(rows)

# =====================
# Title generation + ê¸ˆì¹™ì–´ ìë™ëŒ€ì²´
# =====================
DEFAULT_FORBIDDEN = [
    "ìµœê³ ","ìœ ì¼","ì™„ì¹˜","100%","ì „ë¶€ë‹¤","êµ­ë‚´ìµœì´ˆ","ì„¸ê³„ìµœì´ˆ","ë³´ì¥","í™˜ë¶ˆë³´ì¥",
    "ì´ˆíŠ¹ê°€","íŒŒê²©ì„¸ì¼","ê³µì§œ","ë¬´ë£Œ","ë¤","ëŒ€ë°•","ë¯¸ì¹œ","ê·¹ê°•","ì••ë„ì ",
    "ë§Œë³‘í†µì¹˜","íš¨ëŠ¥","ì¹˜ë£Œ","ì¦‰ì‹œíš¨ê³¼","í™•ì‹¤","ì ˆëŒ€","ë¬´ì¡°ê±´","ì•ˆì „ë³´ì¥",
]

DEFAULT_REPLACE_MAP = {
    "ë¬´ë£Œ":"ë¬´ìƒ",
    "ê³µì§œ":"ë¬´ìƒ",
    "ëŒ€ë°•":"ì¸ê¸°",
    "ë¯¸ì¹œ":"ê°•ë ¥",
    "íŒŒê²©ì„¸ì¼":"íŠ¹ê°€",
    "ë³´ì¥":"ì œê³µ",
    "ìµœê³ ":"ìš°ìˆ˜",
    "ì„¸ê³„ìµœì´ˆ":"ìƒˆë¡œìš´",
    "êµ­ë‚´ìµœì´ˆ":"ìƒˆë¡œìš´",
}

def normalize_title(s:str)->str:
    # ì´ëª¨ì§€/íŠ¹ìˆ˜ë¬¸ì ì¼ë¶€ ì œê±° (ê°„ë‹¨)
    s = re.sub(r"[\u2600-\u27BF\u1F300-\u1F9FF]+", "", s)  # emojis (rough)
    s = s.replace("  ", " ")
    s = re.sub(r"\s+", " ", s).strip(" -_/Â·")
    return s.strip()

def apply_forbidden_map(text:str, forbidden:list, repl_map:dict):
    out = text
    # ìš°ì„  ëŒ€ì²´ ë§µ ì ìš©
    for bad, repl in repl_map.items():
        try:
            out = re.sub(re.escape(bad), repl, out, flags=re.IGNORECASE)
        except Exception:
            pass
    # ë‚¨ì€ ê¸ˆì¹™ì–´ëŠ” ì œê±°
    for bad in forbidden:
        if bad in repl_map:  # ì´ë¯¸ ì²˜ë¦¬ë¨
            continue
        try:
            out = re.sub(re.escape(bad), "", out, flags=re.IGNORECASE)
        except Exception:
            pass
    out = normalize_title(out)
    return out

def title_bytes(s:str)->int:
    return len(s.encode("utf-8"))

def rule_candidates(brand, base_text, keywords, n=5):
    if not keywords:
        keywords = ["ì‹ ìƒ","ì¸ê¸°"]
    rule = [f"{brand} {base_text} {k}".strip() if brand else f"{base_text} {k}".strip() for k in keywords]
    return rule[:n]

def call_openai(api_key, prompt):
    # SDK ìš°ì„  â†’ ì‹¤íŒ¨ ì‹œ HTTP fallback
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"user","content":prompt}],
            temperature=0.7,
        )
        return resp.choices[0].message.content.strip()
    except Exception:
        try:
            url = "https://api.openai.com/v1/chat/completions"
            headers = {"Authorization": f"Bearer {api_key}", "Content-Type":"application/json"}
            payload = {"model":"gpt-4o-mini","messages":[{"role":"user","content":prompt}], "temperature":0.7}
            r = requests.post(url, headers=headers, json=payload, timeout=30)
            r.raise_for_status()
            return r.json()["choices"][0]["message"]["content"].strip()
        except Exception as e:
            raise RuntimeError(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")

# =====================
# Streamlit UI
# =====================
def main():
    header()

    # Sidebar: í™˜ìœ¨
    st.sidebar.markdown("### í™˜ìœ¨ ê³„ì‚°ê¸°")
    amount = st.sidebar.number_input("í˜„ì§€ ê¸ˆì•¡", value=1.00, step=1.0, min_value=0.0)
    base_ccy = st.sidebar.selectbox("í˜„ì§€ í†µí™”", ["USD ($)","EUR (â‚¬)","JPY (Â¥)","CNY (Â¥)"])
    usdkrw = fetch_usdkrw()
    if usdkrw and base_ccy.startswith("USD"):
        st.sidebar.success(f"USDâ†’KRW: ï¿¦{usdkrw:,.2f}\n\nì˜ˆìƒ ì›í™”: **ï¿¦{amount*usdkrw:,.0f}**")
    elif not usdkrw:
        st.sidebar.error("í™˜ìœ¨ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨")

    # Main layout
    col1, col2 = st.columns([7,5])

    # -------- DataLab --------
    with col1:
        st.subheader("ğŸ“ˆ ë„¤ì´ë²„ ë°ì´í„°ë© (Top20 + 1/7/30 ê·¸ë˜í”„)")

        cat = st.selectbox("ì¹´í…Œê³ ë¦¬ ì„ íƒ", list(CATEGORY_SEEDS.keys()), index=0)
        seeds_default = CATEGORY_SEEDS[cat]

        # ì—…ë¡œë“œë¡œ ì§ì ‘ ì‹œë“œ/ì§€í‘œ ì…ë ¥ ì§€ì›
        up = st.file_uploader("í‚¤ì›Œë“œ ì‹œë“œ ì—…ë¡œë“œ (CSV, ì„ íƒ) â€” ì»¬ëŸ¼: keyword[,day1,day7,day30]", type=["csv"])
        if up:
            df = parse_uploaded_csv(up)
            if df is None:
                st.warning("CSV í•´ì„ ì‹¤íŒ¨. ê¸°ë³¸ ì‹œë“œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                df = mock_ratios_from_keywords(seeds_default)
        else:
            # ëª¨ì˜ ì§€í‘œ
            df = mock_ratios_from_keywords(seeds_default)

        # í‚¤ì›Œë“œ ì •ê·œí™”/ì¤‘ë³µ ì œê±°
        df["keyword"] = normalize_keywords(df["keyword"].astype(str))
        df = df.drop_duplicates("keyword")

        tabs = st.tabs(["1ì¼", "7ì¼", "30ì¼", "ë¹„êµ(1/7/30)"])

        def plot_single(field, title):
            d = df[["keyword", field]].rename(columns={field:"ratio"})
            d = d.sort_values("ratio", ascending=False).head(20)
            chart = alt.Chart(d).mark_bar().encode(
                x=alt.X("ratio:Q", title="ratio"),
                y=alt.Y("keyword:N", sort='-x', title="keyword"),
                tooltip=["keyword","ratio"]
            ).properties(height=520, title=title)
            st.altair_chart(chart, use_container_width=True)

        with tabs[0]:
            plot_single("day1", "ìµœê·¼ 1ì¼ í‰ê·  ratio (Top20)")
        with tabs[1]:
            plot_single("day7", "ìµœê·¼ 7ì¼ í‰ê·  ratio (Top20)")
        with tabs[2]:
            plot_single("day30", "ìµœê·¼ 30ì¼ í‰ê·  ratio (Top20)")
        with tabs[3]:
            dd = df.melt(id_vars=["keyword"], value_vars=["day1","day7","day30"], var_name="period", value_name="ratio")
            dd = dd.sort_values("ratio", ascending=False).groupby("period").head(20)
            chart = alt.Chart(dd).mark_bar().encode(
                x=alt.X("ratio:Q"),
                y=alt.Y("keyword:N", sort='-x'),
                color=alt.Color("period:N"),
                tooltip=["keyword","period","ratio"]
            ).properties(height=520, title="1/7/30ì¼ ë¹„êµ (Top20 ê° ê¸°ê°„ ìƒìœ„)")
            st.altair_chart(chart, use_container_width=True)

    # -------- 11ë²ˆê°€ --------
    with col2:
        st.subheader("ğŸ›’ 11ë²ˆê°€ AmazonBest")
        with st.sidebar.expander("ğŸ›’ 11ë²ˆê°€ ì˜µì…˜", expanded=False):
            proxy_base = st.text_input("í”„ë¡ì‹œ ë² ì´ìŠ¤ URL", value=st.session_state.get("e11_proxy", ""))
            ua = st.text_input("User-Agent (ì„ íƒ)", value=st.session_state.get("e11_ua", "Mozilla/5.0"))
            st.session_state["e11_proxy"] = proxy_base
            st.session_state["e11_ua"] = ua

        st.link_button("ğŸ”— ìƒˆì°½ì—ì„œ 11ë²ˆê°€ ì—´ê¸°", "https://m.11st.co.kr/browsing/AmazonBest")
        rows = fetch_11st_rows(st.session_state.get("e11_proxy",""), st.session_state.get("e11_ua",""))
        st.caption("í”„ë¡ì‹œ/ì§ê²° ê²°ê³¼ (ì°¨ë‹¨ ì‹œ ìƒ˜í”Œ í´ë°±)")
        st.dataframe(rows, use_container_width=True, height=440)
        with st.expander("ğŸ§ª iframeìœ¼ë¡œ ì§ì ‘ ë³´ê¸° (í™˜ê²½ì— ë”°ë¼ ì°¨ë‹¨)", expanded=False):
            html = """
            <iframe src='https://m.11st.co.kr/browsing/AmazonBest'
                    width='100%' height='760' frameborder='0'
                    referrerpolicy='no-referrer'
                    sandbox='allow-same-origin allow-scripts allow-popups allow-forms'>
            </iframe>"""
            st.components.v1.html(html, height=780)

    st.divider()

    # -------- Title generator + forbidden filter --------
    st.subheader("âœï¸ ìƒí’ˆëª… ìƒì„±ê¸° + ê¸ˆì¹™ì–´ ìë™ëŒ€ì²´")
    mode = st.radio("ëª¨ë“œ ì„ íƒ", ["ê·œì¹™ ê¸°ë°˜(ë¬´ë£Œ)", "OpenAI API ì‚¬ìš©"], horizontal=True)
    brand = st.text_input("ë¸Œëœë“œ")
    base_text = st.text_input("ê¸°ë³¸ ë¬¸ì¥")
    raw_keywords = st.text_input("í‚¤ì›Œë“œ(ì‰¼í‘œ , ë¡œ êµ¬ë¶„)")
    cnt = st.slider("ìƒì„± ê°œìˆ˜", 3, 10, 5)

    with st.expander("ğŸ›¡ï¸ ê¸ˆì¹™ì–´/ëŒ€ì²´ì–´ ì„¤ì •", expanded=True):
        colA, colB, colC = st.columns([4,4,2])
        with colA:
            forb = st.text_area("ê¸ˆì¹™ì–´ ëª©ë¡(ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)", value="\n".join(DEFAULT_FORBIDDEN), height=160)
            forbidden = [w.strip() for w in forb.splitlines() if w.strip()]
        with colB:
            repl_lines = st.text_area("ëŒ€ì²´ ë§µ(í˜•ì‹: ì›ë¬¸=>ëŒ€ì²´ì–´, ì¤„ë°”ê¿ˆ)", value="\n".join([f"{k}=>{v}" for k,v in DEFAULT_REPLACE_MAP.items()]), height=160)
            repl_map = {}
            for line in repl_lines.splitlines():
                if "=>" in line:
                    a,b = line.split("=>",1)
                    repl_map[a.strip()] = b.strip()
        with colC:
            max_bytes = st.number_input("ë°”ì´íŠ¸ ì œí•œ(UTF-8)", min_value=10, max_value=120, value=60, step=2)
            hard_trim = st.checkbox("ì œí•œ ì´ˆê³¼ ì‹œ ìë™ ìë¥´ê¸°", value=True)

    api_key = ""
    if mode == "OpenAI API ì‚¬ìš©":
        with st.expander("ğŸ” OpenAI API ì„¤ì • (ì„ íƒ)"):
            api_key = st.text_input("OpenAI API Key (sk-â€¦)", type="password")
            if api_key:
                st.session_state["OPENAI_API_KEY"] = api_key
        api_key = api_key or st.session_state.get("OPENAI_API_KEY", "") or os.getenv("OPENAI_API_KEY","")

    if st.button("ì œëª© ìƒì„±", type="primary"):
        keywords = [k.strip() for k in raw_keywords.split(",") if k.strip()]
        titles = rule_candidates(brand, base_text, keywords, n=cnt)

        if mode == "OpenAI API ì‚¬ìš©" and api_key:
            prompt = (
                "ë‹¹ì‹ ì€ í•œêµ­ ì´ì»¤ë¨¸ìŠ¤ ìƒí’ˆëª… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì¡°ê±´ìœ¼ë¡œ "
                f"{cnt}ê°œì˜ ìƒí’ˆëª…ì„ ë§Œë“œì„¸ìš”.\n"
                f"- ë¸Œëœë“œ: {brand or 'ì—†ìŒ'}\n"
                f"- ê¸°ë³¸ ë¬¸ì¥: {base_text}\n"
                f"- í‚¤ì›Œë“œ í›„ë³´: {', '.join(keywords) or 'ì‹ ìƒ, ì¸ê¸°'}\n"
                "- í•œêµ­ì–´, 28~36ì, ê´‘ê³ ì„± ê¸ˆì§€ì–´ ê¸ˆì§€, í•µì‹¬ í‚¤ì›Œë“œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨\n"
                "- JSON ë°°ì—´ë§Œ ê²°ê³¼ë¡œ ì¶œë ¥"
            )
            try:
                resp = call_openai(api_key, prompt)
                try:
                    arr = json.loads(resp)
                    if isinstance(arr, list) and arr:
                        titles = arr[:cnt]
                except Exception:
                    # ì¤„ë°”ê¿ˆ ë¦¬ìŠ¤íŠ¸ í—ˆìš©
                    lines = [s.strip("-â€¢ ").strip() for s in re.split(r"[\n\r]+", resp) if s.strip()]
                    if lines:
                        titles = lines[:cnt]
            except Exception as e:
                st.warning(f"OpenAI ì‹¤íŒ¨: {e}. ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")

        # ê¸ˆì¹™ì–´ ìë™ ëŒ€ì²´ ì ìš©
        after = []
        for t in titles:
            t1 = apply_forbidden_map(t, forbidden, repl_map)
            if title_bytes(t1) > max_bytes and hard_trim:
                # ë°”ì´íŠ¸ ì´ˆê³¼ ì‹œ ë¶€ë“œëŸ½ê²Œ ìë¥´ê¸°
                b = t1.encode("utf-8")
                b = b[:max_bytes]
                # ê¹¨ì§„ ë©€í‹°ë°”ì´íŠ¸ ì»· ë³´ì •
                while True:
                    try:
                        t1 = b.decode("utf-8")
                        break
                    except UnicodeDecodeError:
                        b = b[:-1]
            after.append(t1)

        df = pd.DataFrame({
            "ì›ë³¸": titles,
            "ì ìš©í›„": after,
            "chars": [len(s) for s in after],
            "bytes(UTF-8)": [title_bytes(s) for s in after],
        })
        st.success("ìƒì„± ì™„ë£Œ (ê¸ˆì¹™ì–´ ìë™ëŒ€ì²´ ì ìš©)")
        st.dataframe(df, use_container_width=True, height=330)
        st.download_button("CSVë¡œ ë‚´ë³´ë‚´ê¸°", df.to_csv(index=False).encode("utf-8-sig"), file_name="titles_filtered.csv", mime="text/csv")

if __name__ == "__main__":
    header  # keep
    main()
