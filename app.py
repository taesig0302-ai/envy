# -*- coding: utf-8 -*-
"""
ENVY â€“ v9
- í™˜ìœ¨ 30ë¶„ ìºì‹œ + 2ì¤‘ í´ë°±
- ë§ˆì§„ ê³„ì‚°ê¸°
- ë‹¤í¬ëª¨ë“œ
- ë°ì´í„°ë©: "API-ì „ìš©" ëª¨ë“œ (ì¹´í…Œê³ ë¦¬â†’Top20(ì‚¬ì „)â†’1/7/30ì¼ í‰ê·  ratio)
  * CSV ì‚¬ì „ ì—…ë¡œë“œë¡œ ë™ì˜ì–´/ì •ê·œí™” í™•ì¥
- 11ë²ˆê°€: iframe/ìƒˆì°½/ìš°íšŒ(í”„ë¡ì‹œ) ì˜µì…˜ + í´ë°± ìƒ˜í”Œ
- ì œëª© ìƒì„±ê¸°: ê·œì¹™/OpenAI API í† ê¸€
- ìƒë‹¨ ENVY ë¡œê³ 
"""
import os, json, re, time
from datetime import date, timedelta
from random import sample
import pandas as pd
import requests
import streamlit as st
from PIL import Image as _PILImage

st.set_page_config(page_title="ENVY v9", layout="wide")

# --- Header (ENVY logo) ---
try:
    _lg = _PILImage.open("/mnt/data/envy_logo.png")
    h1_l, h1_r = st.columns([1,4])
    with h1_l:
        st.image(_lg, width=120)
    with h1_r:
        st.markdown(" ")
except Exception:
    st.markdown("### ENVY")

# --- Dark mode ---
st.sidebar.checkbox("ğŸŒ™ ë‹¤í¬ ëª¨ë“œ", key="dark_mode")
if st.session_state.get("dark_mode", False):
    st.markdown("""
    <style>
      body, .stApp { background:#121212; color:#EDEDED; }
      .stMarkdown h1, h2, h3, h4, h5 { color:#FFF !important; }
      .stDataFrame, .stTable { color:#EDEDED !important; }
    </style>
    """, unsafe_allow_html=True)

st.markdown("### ğŸ’± ì‹¤ì‹œê°„ í™˜ìœ¨ + ğŸ“Š ë§ˆì§„ + ğŸ“ˆ ë°ì´í„°ë©(API) + ğŸ›’ 11ë²ˆê°€ + âœï¸ ìƒí’ˆëª…(API)")

# --- FX (30m cache, dual fallback) ---
@st.cache_data(ttl=1800)
def fx_rate(base="USD", target="KRW"):
    try:
        r = requests.get(f"https://api.exchangerate.host/latest?base={base}&symbols={target}", timeout=8).json()
        if "rates" in r and target in r["rates"]:
            return float(r["rates"][target])
    except Exception:
        pass
    try:
        r = requests.get(f"https://open.er-api.com/v6/latest/{base}", timeout=8).json()
        if "rates" in r and target in r["rates"]:
            return float(r["rates"][target])
    except Exception:
        pass
    return None

# --- Sidebar: FX calc ---
st.sidebar.subheader("ğŸ’± í™˜ìœ¨ ê³„ì‚°ê¸°")
fx_amount = st.sidebar.number_input("ìƒí’ˆ ì›ê°€", min_value=0.0, value=1.0, step=1.0)
fx_currency = st.sidebar.selectbox("í†µí™”", ["USD ($)", "EUR (â‚¬)", "JPY (Â¥)", "CNY (Â¥)"])
fx_map = {"USD ($)":"USD","EUR (â‚¬)":"EUR","JPY (Â¥)":"JPY","CNY (Â¥)":"CNY"}
rate = fx_rate(fx_map[fx_currency])
if rate:
    st.sidebar.markdown(f"### {fx_amount:.2f} {fx_map[fx_currency]} â†’ **{fx_amount*rate:,.0f} ì›**")
    st.sidebar.caption(f"1 {fx_map[fx_currency]} = â‚©{rate:,.2f} (30ë¶„ ìºì‹œ)")
else:
    st.sidebar.error("í™˜ìœ¨ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- Sidebar: Margin calc ---
st.sidebar.subheader("ğŸ§® ê°„ì´ ë§ˆì§„ ê³„ì‚°")
loc_price = st.sidebar.number_input("í˜„ì§€ ê¸ˆì•¡", min_value=0.0, value=0.0, step=1.0)
loc_cur   = st.sidebar.selectbox("í˜„ì§€ í†µí™”", ["USD","EUR","JPY","CNY"])
shipping  = st.sidebar.number_input("ë°°ì†¡ë¹„ (KRW)", min_value=0.0, value=0.0, step=100.0)
card_fee  = st.sidebar.number_input("ì¹´ë“œ ìˆ˜ìˆ˜ë£Œ (%)", min_value=0.0, value=4.0, step=0.5)
market_fee= st.sidebar.number_input("ë§ˆì¼“ ìˆ˜ìˆ˜ë£Œ (%)", min_value=0.0, value=15.0, step=0.5)
target_margin = st.sidebar.number_input("ëª©í‘œ ë§ˆì§„ (%)", min_value=0.0, value=40.0, step=1.0)
rate2 = fx_rate(loc_cur)
if rate2 and loc_price > 0:
    conv_cost = loc_price * rate2
    cost_krw = conv_cost + shipping
    sell_base = cost_krw * (1 + target_margin/100.0)
    final_price = sell_base / (1 - (card_fee + market_fee)/100.0) if (card_fee + market_fee) < 100 else sell_base
    profit = final_price - cost_krw
    margin_pct = (profit / final_price * 100) if final_price>0 else 0.0
    st.sidebar.info(f"í™˜ìœ¨({loc_cur}â†’KRW): â‚©{rate2:,.2f}")
    st.sidebar.caption(f"í™˜ì‚° ì›ê°€: â‚©{conv_cost:,.0f}  â€¢  ë°°ì†¡ë¹„: â‚©{shipping:,.0f}")
    st.sidebar.success(f"ğŸ”¥ ì˜ˆìƒ íŒë§¤ê°€: **{final_price:,.0f} ì›**")
    st.sidebar.write(f"ìˆœì´ìµ: **{profit:,.0f} ì›**  (ì‹¤ë§ˆì§„ {margin_pct:.1f}%)")

# --- Layout ---
col1, col2 = st.columns(2, gap="large")

# --- DataLab API-only (category -> keywords -> 1/7/30 ratio) ---
with col1:
    st.subheader("ğŸ“ˆ ë„¤ì´ë²„ ë°ì´í„°ë© (API ì „ìš©: 1/7/30ì¼ í‰ê· )")
    NAVER_ID = "h4mkIM2hNLct04BD7sC0"
    NAVER_SECRET = "ltoxUNyKxi"

    # Category seeds (fallback guaranteed)
    CATS = ["íŒ¨ì…˜ì˜ë¥˜","í™”ì¥í’ˆ/ë¯¸ìš©","ì‹í’ˆ","ë””ì§€í„¸/ê°€ì „","ìƒí™œ/ê±´ê°•","ìŠ¤í¬ì¸ /ë ˆì €","ì¶œì‚°/ìœ¡ì•„","ê°€êµ¬/ì¸í…Œë¦¬ì–´","ë„ì„œ/ì·¨ë¯¸/ìŒë°˜","ìë™ì°¨/ê³µêµ¬"]
    cat = st.selectbox("ì¹´í…Œê³ ë¦¬ ì„ íƒ", CATS)

    _DEF_SEEDS = {
        "íŒ¨ì…˜ì˜ë¥˜": ["ë§¨íˆ¬ë§¨","ìŠ¬ë™ìŠ¤","ì²­ë°”ì§€","ì¹´ë¼í‹°","ë°”ëŒë§‰ì´","ë‹ˆíŠ¸","ê°€ë””ê±´","ë¡±ìŠ¤ì»¤íŠ¸","ë¶€ì¸ ì»·","ì™€ì´ë“œíŒ¬ì¸ ","ì¡°ê±°íŒ¬ì¸ ","ë¹…ì‚¬ì´ì¦ˆ","íŒ¨ë”©","ë°”ì§€","ì •ì¥","ì…”ì¸ ","ì½”íŠ¸","ë¸”ë¼ìš°ìŠ¤","ì›í”¼ìŠ¤","í›„ë“œí‹°"],
        "í™”ì¥í’ˆ/ë¯¸ìš©": ["í´ë Œì§•","ì„ í¬ë¦¼","ì•°í”Œ","ì„¸ëŸ¼","í† ë„ˆ","ë¡œì…˜","í¬ë¦¼","íŒ©","ë§ˆìŠ¤í¬íŒ©","ë¦½ë°¤","ë¦½ìŠ¤í‹±","ì¿ ì…˜","íŒŒìš´ë°ì´ì…˜","ì•„ì´ì„€ë„ìš°","ì•„ì´ë¼ì´ë„ˆ","ì»¨ì‹¤ëŸ¬","ë¸ŒëŸ¬ì‹œ","í–¥ìˆ˜","ìƒ´í‘¸","íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸"],
        "ì‹í’ˆ": ["ë¼ë©´","ì»¤í”¼","ì°¸ì¹˜","ê¹€ì¹˜","ìŠ¤íŒ¸","ì ¤ë¦¬","ê°„ì‹","ìœ ê¸°ë†","ê³¼ì","ì•„ëª¬ë“œ","ìº”ë””","ìš°ìœ ","ì¹˜ì¦ˆ","ìš”ê±°íŠ¸","ìŒ€","ê²¬ê³¼","ì˜¬ë¦¬ë¸Œìœ ","ì†ŒìŠ¤","ì‹œë¦¬ì–¼","ê¿€"],
        "ë””ì§€í„¸/ê°€ì „": ["ë…¸íŠ¸ë¶","ëª¨ë‹ˆí„°","í‚¤ë³´ë“œ","ë§ˆìš°ìŠ¤","SSD","HDD","ìŠ¤ë§ˆíŠ¸ì›Œì¹˜","íƒœë¸”ë¦¿","ìŠ¤ë§ˆíŠ¸í°","ì¶©ì „ê¸°","ì¼€ì´ë¸”","ì´ì–´í°","í—¤ë“œì…‹","ê³µì²­ê¸°","ì—ì–´ì»¨","ì²­ì†Œê¸°","TV","ìº ","í”„ë¦°í„°","NAS"],
        "ìƒí™œ/ê±´ê°•": ["ë¬¼í‹°ìŠˆ","íœ´ì§€","ì„¸ì œ","ì„¬ìœ ìœ ì—°ì œ","ì£¼ë°©ì„¸ì œ","ì¹«ì†”","ì¹˜ì•½","ë°”ë””ì›Œì‹œ","ìƒ´í‘¸","ë§ˆìŠ¤í¬","ë¹„íƒ€ë¯¼","ì˜ì–‘ì œ","êµ¬ê°•ì²­ê²°ì œ","ì²´ì˜¨ê³„","í•«íŒ©","ìˆ˜ì„¸ë¯¸","ê³ ë¬´ì¥ê°‘","ì œìŠµì œ","ë²Œë ˆí‡´ì¹˜","ìƒë¹„ì•½"],
        "ìŠ¤í¬ì¸ /ë ˆì €": ["ë¤ë²¨","ìš”ê°€ë§¤íŠ¸","ëŸ¬ë‹í™”","ë“±ì‚°í™”","ìì „ê±°","í…íŠ¸","ìº í•‘ì˜ì","ë²„ë„ˆ","ì½”í ","í—¬ë©§","ë°°ë‚­","ì•„ì´ìŠ¤ë°•ìŠ¤","ìŠ¤í‹±","ìˆ˜ì˜ë³µ","ê³ ê¸€","ëª¨ì","ì¥ê°‘","ë³´ë“œë³µ","ìŠ¤ë…¸ìš°ë³´ë“œ","ë³¼"],
        "ì¶œì‚°/ìœ¡ì•„": ["ê¸°ì €ê·€","ë¬¼í‹°ìŠˆ","ë¶„ìœ ","ì –ë³‘","ì´ìœ ì‹","ìœ ì•„ì˜ì","ìœ ëª¨ì°¨","ì¹´ì‹œíŠ¸","í„±ë°›ì´","ì¹˜ë°œê¸°","ì –ë³‘ì†Œë…ê¸°","ì•„ê¸°ì²´ì˜¨ê³„","ê°€ì œì†ìˆ˜ê±´","ì•„ê¸°ì¹¨ëŒ€","ë²”í¼ì¹¨ëŒ€","ìˆ˜ìœ ì¿ ì…˜","í¡ì…ê¸°","í¬ë¦¼","ì›Œì‹œ","íŒŒìš°ë”"],
        "ê°€êµ¬/ì¸í…Œë¦¬ì–´": ["ì†ŒíŒŒ","ì±…ìƒ","ì˜ì","ì„œëì¥","í–‰ê±°","ì˜·ì¥","ì‹íƒ","í˜‘íƒ","ëŸ¬ê·¸","ê±°ì‹¤ì¥","TVì¥","ì±…ì¥","ì„ ë°˜","ë²½ë“±","ìŠ¤íƒ ë“œ","ì»¤íŠ¼","ë¸”ë¼ì¸ë“œ","ì´ë¶ˆ","ë² ê°œ","ë§¤íŠ¸ë¦¬ìŠ¤"],
        "ë„ì„œ/ì·¨ë¯¸/ìŒë°˜": ["ì†Œì„¤","ì—ì„¸ì´","ì‹œì§‘","ë§Œí™”","ìˆ˜í—˜ì„œ","ì°¸ê³ ì„œ","ì¡ì§€","ì•¨ë²”","LP","CD","ë³´ë“œê²Œì„","í¼ì¦","í”„ë¼ëª¨ë¸","í•„ê¸°êµ¬","ìˆ˜ì±„í™”","ìœ í™”","ìŠ¤ì¼€ì¹˜ë¶","ì»¬ëŸ¬ë§ë¶","ìº˜ë¦¬ê·¸ë¼í”¼","ì·¨ë¯¸í‚¤íŠ¸"],
        "ìë™ì°¨/ê³µêµ¬": ["ë¸”ë™ë°•ìŠ¤","í•˜ì´íŒ¨ìŠ¤","ëŒ€ì‰¬ìº ","íƒ€ì´ì–´","ì—”ì§„ì˜¤ì¼","ì™€ì´í¼","ì½”íŒ…ì œ","ê´‘íƒì œ","ì¶©ì „ê¸°","ì í”„ìŠ¤íƒ€í„°","ê³µêµ¬ì„¸íŠ¸","ë“œë¦´","ê¸€ë£¨ê±´","ì¸¡ì •ê¸°","ë©€í‹°íƒ­","ì „ë“±","í›„í¬","ìš©ì ‘ê¸°","ë Œì¹˜","í•´ë¨¸"]
    }

    # Normalizer + CSV dictionary
    from unicodedata import normalize as _norm
    _norm_map = {"í›„ë””":"í›„ë“œí‹°","ë¡±íŒ¨ë”©":"íŒ¨ë”©","ìˆíŒ¨ë”©":"íŒ¨ë”©","ì²­ë°”ì§€":"ë°ë‹˜ë°”ì§€"}
    def _norm_k(s): return _norm("NFKC", s).replace(" ","").lower()
    def normalize_keywords(lst):
        out = []
        for k in lst:
            key = _norm_k(k)
            mapped = None
            for raw, canonical in _norm_map.items():
                if _norm_k(raw) == key:
                    mapped = canonical; break
            out.append(mapped if mapped else k)
        # dedup
        seen=set(); uniq=[]
        for x in out:
            if x not in seen:
                seen.add(x); uniq.append(x)
        return uniq[:20]

    seeds = normalize_keywords(_DEF_SEEDS.get(cat, _DEF_SEEDS["íŒ¨ì…˜ì˜ë¥˜"]))

    with st.expander("ğŸ“š í‚¤ì›Œë“œ ì‚¬ì „ ì—…ë¡œë“œ (CSV, ì„ íƒ)", expanded=False):
        st.caption("í˜•ì‹: raw,canonical (í—¤ë” í¬í•¨) / ì˜ˆ: í›„ë””,í›„ë“œí‹°")
        sample = "raw,canonical\ní›„ë””,í›„ë“œí‹°\në¡±íŒ¨ë”©,íŒ¨ë”©\nìˆíŒ¨ë”©,íŒ¨ë”©\n"
        st.download_button("ì˜ˆì œ CSV ë°›ê¸°", data=sample, file_name="envy_keyword_map.csv", mime="text/csv")
        up = st.file_uploader("ì‚¬ì „ CSV ì—…ë¡œë“œ", type=["csv"], key="dl_csv_v9")
        if up is not None:
            try:
                _df_map = pd.read_csv(up)
                add_map = {str(r["raw"]).strip(): str(r["canonical"]).strip() for _, r in _df_map.iterrows() if str(r.get("raw","")).strip()}
                _norm_map.update(add_map)
                st.success(f"ì‚¬ì „ {len(add_map)}ê°œ ì ìš© ì™„ë£Œ")
                seeds = normalize_keywords(seeds)  # ì¬ì •ê·œí™”
            except Exception as e:
                st.error("CSV íŒŒì‹± ì‹¤íŒ¨: " + str(e))

    # Query Naver DataLab API for 1/7/30 day windows (avg ratio)
    def datalab_avg(keyword:str, days:int):
        end = date.today() - timedelta(days=1)
        start = end - timedelta(days=days-1)
        url = "https://openapi.naver.com/v1/datalab/search"
        headers={"X-Naver-Client-Id":NAVER_ID, "X-Naver-Client-Secret":NAVER_SECRET}
        body={
            "startDate": start.strftime("%Y-%m-%d"),
            "endDate": end.strftime("%Y-%m-%d"),
            "timeUnit":"date",
            "keywordGroups":[{"groupName":keyword, "keywords":[keyword]}],
            "device":"pc","ages":[],"gender":""
        }
        try:
            r = requests.post(url, headers=headers, json=body, timeout=10).json()
            data = r["results"][0]["data"]
            vals = [d["ratio"] for d in data if "ratio" in d]
            return sum(vals)/len(vals) if vals else 0.0
        except Exception:
            return 0.0

    rows = []
    for kw in seeds:
        d1  = datalab_avg(kw, 1)
        d7  = datalab_avg(kw, 7)
        d30 = datalab_avg(kw, 30)
        rows.append({"keyword": kw, "day1": round(d1,2), "day7": round(d7,2), "day30": round(d30,2)})
    df = pd.DataFrame(rows)
    st.dataframe(df, use_container_width=True, height=480)

# --- 11st ---
with col2:
    st.subheader("ğŸ›’ 11ë²ˆê°€ AmazonBest")
with col2:
    st.subheader("ğŸ›’ 11ë²ˆê°€ AmazonBest")

    # ì‚¬ì´ë“œë°” ì˜µì…˜ (í”„ë¡ì‹œ/UA/í‘œì‹œ ëª¨ë“œ ìœ ì§€í•˜ë˜, ë³¸ë¬¸ì€ 'ë‘˜ë‹¤' ì œê³µ)
    with st.sidebar.expander("ğŸ›’ 11ë²ˆê°€ ì˜µì…˜", expanded=False):
        st.caption("í”„ë¡ì‹œ ì˜ˆì‹œ: https://your-proxy.example/fetch?url=")
        proxy_base = st.text_input("í”„ë¡ì‹œ ë² ì´ìŠ¤ URL", value=st.session_state.get("e11_proxy", ""))
        ua = st.text_input("User-Agent (ì„ íƒ)", value=st.session_state.get("e11_ua", "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"))
        st.session_state["e11_proxy"] = proxy_base
        st.session_state["e11_ua"] = ua

    # 1) í•­ì‹œ ì œê³µ: ìƒˆì°½ ì—´ê¸° ë²„íŠ¼ (100% ë³´ì¥)
    st.link_button("ğŸ”— ìƒˆì°½ì—ì„œ 11ë²ˆê°€ AmazonBest ì—´ê¸°", "https://m.11st.co.kr/browsing/AmazonBest")

    # 2) ê¸°ë³¸ ì œê³µ: ìš°íšŒ(í”„ë¡ì‹œ/ì§ê²°) í…Œì´ë¸”
    def fetch_e11_list(proxy_base:str, ua:str):
        import json, re, requests
        headers = {"User-Agent": ua} if ua else {}
        target = "https://m.11st.co.kr/browsing/AmazonBest"
        text = ""
        try:
            if proxy_base:
                url = proxy_base + target
                text = requests.get(url, headers=headers, timeout=8).text
            else:
                text = requests.get(target, headers=headers, timeout=8).text
        except Exception:
            text = ""

        rows = []
        # naive JSON block
        try:
            m = re.search(r'(\{.*\"AmazonBest\".*\})', text, re.DOTALL)
            if m:
                blob = m.group(1).replace("\n","")
                js = json.loads(blob)
                items = []
                try:
                    items = js["state"]["bests"]["items"]
                except Exception:
                    items = []
                if items:
                    for i, it in enumerate(items[:20]):
                        rows.append({
                            "rank": i+1,
                            "product": it.get("productName") or it.get("name") or "",
                            "price": str(it.get("finalPrice") or it.get("price") or ""),
                            "link": it.get("detailUrl") or ""
                        })
                    return rows
        except Exception:
            pass

        # regex fallback
        try:
            names = re.findall(r'\"productName\"\s*:\s*\"([^\"]{3,120})\"', text)
            prices = re.findall(r'\"finalPrice\"\s*:\s*\"?(\d[\d,]{2,})\"?', text)
            links  = re.findall(r'\"detailUrl\"\s*:\s*\"([^\"]+)\"', text)
            for i, n in enumerate(names[:20]):
                price = prices[i] if i < len(prices) else ""
                link  = links[i]  if i < len(links)  else ""
                rows.append({"rank": i+1, "product": n, "price": price.replace(",", ""), "link": link})
        except Exception:
            rows = []

        if not rows:
            rows = [
                {"rank":1,"product":"ì• í”Œ ì—ì–´íŒŸ Pro (2ì„¸ëŒ€)","price":"329000","link":""},
                {"rank":2,"product":"ì‚¼ì„± ê°¤ëŸ­ì‹œ S23 256GB","price":"998000","link":""},
                {"rank":3,"product":"ë‚˜ì´í‚¤ ìš´ë™í™” ë ˆë³¼ë£¨ì…˜","price":"89000","link":""},
                {"rank":4,"product":"LG ë…¸íŠ¸ë¶ 16í˜• ì´ˆê²½ëŸ‰","price":"1399000","link":""},
                {"rank":5,"product":"ìŠ¤íƒ€ë²…ìŠ¤ í…€ë¸”ëŸ¬ 473ml","price":"23000","link":""},
            ]
        return rows

    rows = fetch_e11_list(st.session_state.get("e11_proxy",""), st.session_state.get("e11_ua",""))
    st.caption("ìš°íšŒ(í”„ë¡ì‹œ/ì§ê²°) í…Œì´ë¸” â€“ ì°¨ë‹¨ ì‹œ ìƒ˜í”Œ ë°ì´í„°ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
    st.dataframe(pd.DataFrame(rows), use_container_width=True, height=420)

    # 3) ì„ íƒ ì œê³µ: iframe (ì°¨ë‹¨ë  ìˆ˜ ìˆìŒ)
    with st.expander("ğŸ§ª iframeìœ¼ë¡œ ì§ì ‘ ë³´ê¸° (í™˜ê²½ì— ë”°ë¼ ì°¨ë‹¨ë¨)", expanded=False):
        html = """
        <iframe src='https://m.11st.co.kr/browsing/AmazonBest'
                width='100%' height='780' frameborder='0'
                referrerpolicy='no-referrer'
                sandbox='allow-same-origin allow-scripts allow-popups allow-forms'>
        </iframe>"""
        st.components.v1.html(html, height=800)
# --- Title generator ---
st.subheader("âœï¸ ìƒí’ˆëª… ìƒì„±ê¸°")
_left, _right = st.columns([3,2])
with _left:
    mode = st.radio("ëª¨ë“œ ì„ íƒ", ["ê·œì¹™ ê¸°ë°˜(ë¬´ë£Œ)", "OpenAI API ì‚¬ìš©"], horizontal=True)
with _right:
    with st.expander("ğŸ” OpenAI API ì„¤ì • (ì„ íƒ)", expanded=False):
        st.text_input("API í‚¤ ì…ë ¥ (ì„¸ì…˜ ì €ì¥)", type="password", key="OPENAI_API_KEY")
        st.caption("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ì‚¬ìš©ë„ ê°€ëŠ¥. ë¯¸ì…ë ¥ ì‹œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í´ë°±.")

btn_col1, btn_col2 = st.columns([1,5])
with btn_col1:
    gen_now = st.button("ì œëª© ìƒì„±", use_container_width=True)
with btn_col2:
    st.caption("ìƒë‹¨ì—ì„œ ë°”ë¡œ ìƒì„±")

brand   = st.text_input("ë¸Œëœë“œ")
base_kw = st.text_input("ê¸°ë³¸ ë¬¸ì¥")
extra_kw= st.text_input("í‚¤ì›Œë“œ (ì‰¼í‘œ , ë¡œ êµ¬ë¶„)")
count   = st.slider("ìƒì„± ê°œìˆ˜", 3, 10, 5)

def gen_rule_titles(brand, base_kw, extra_kw, count):
    extras = [x.strip() for x in extra_kw.split(",") if x.strip()]
    if not extras:
        return [f"{brand} {base_kw}".strip()]
    picks = (extras * ((count // len(extras)) + 1))[:count]
    return [f"{brand} {base_kw} {p}".strip() for p in picks[:count]]

def gen_openai_titles(brand, base_kw, keywords, n=5):
    key = st.session_state.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not key:
        raise RuntimeError("NO_API_KEY")
    try:
        from openai import OpenAI
        client = OpenAI(api_key=key)
        prompt = f"""
ì—­í• : ì´ì»¤ë¨¸ìŠ¤ ìƒí’ˆëª… ì¹´í”¼ë¼ì´í„°
ì¡°ê±´:
- í•œêµ­ì–´, ê³µë°± í¬í•¨ 28~36ì
- {brand}ì„(ë¥¼) ë§¨ ì•ì—, í•µì‹¬ í‚¤ì›Œë“œ 1~2ê°œ í¬í•¨
- ê³¼ì¥/ê´‘ê³ ì„± ê¸ˆì§€ì–´(ìµœê°•, ì—­ëŒ€ê¸‰, ì™„íŒ ë“±) ê¸ˆì§€
- í”Œë«í¼ ê²€ìƒ‰ìµœì í™”(ì¤‘ë³µì–´ ì œê±°, ë¶ˆí•„ìš” ê¸°í˜¸ ì œê±°)
- {n}ê°œ ìƒì„±
ì…ë ¥:
ë¸Œëœë“œ: {brand}
ê¸°ë³¸ ë¬¸ì¥: {base_kw}
í‚¤ì›Œë“œ í›„ë³´: {", ".join(keywords)}
ì¶œë ¥í˜•ì‹: JSON ë°°ì—´(ë¬¸ìì—´ë“¤ë§Œ)
"""
        resp = client.responses.create(model="gpt-4o-mini", input=prompt)
        titles = json.loads(resp.output_text)
        return titles[:n]
    except Exception as e:
        raise RuntimeError(f"API_FAIL:{e}")

if gen_now:
    if mode.startswith("ê·œì¹™"):
        titles = gen_rule_titles(brand, base_kw, extra_kw, count)
        st.success("ê·œì¹™ ê¸°ë°˜ ê²°ê³¼")
        st.write(pd.DataFrame({"#": range(1, len(titles)+1), "title": titles}))
    else:
        kw_list = [x.strip() for x in extra_kw.split(",") if x.strip()]
        try:
            titles = gen_openai_titles(brand, base_kw, kw_list, n=count)
            st.success("OpenAI API ê²°ê³¼")
            st.write(pd.DataFrame({"#": range(1, len(titles)+1), "title": titles}))
        except RuntimeError as err:
            titles = gen_rule_titles(brand, base_kw, extra_kw, count)
            st.warning(f"API ëª¨ë“œ ì‹¤íŒ¨ â†’ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´ ({err})")
            st.write(pd.DataFrame({"#": range(1, len(titles)+1), "title": titles}))
