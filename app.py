import streamlit as st
import requests
import pandas as pd
import datetime

# ============ ê¸°ë³¸ í˜ì´ì§€ ì„¤ì • ============
st.set_page_config(
    page_title="ENVY v27.7 Full",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸš€ ENVY v27.7 Full (Rakuten API + DataLab)")

# ============ ì‚¬ì´ë“œë°” : í™˜ìœ¨/ë§ˆì§„ ê³„ì‚°ê¸° ============
st.sidebar.header("í™˜ìœ¨ ì„¤ì •")
base_currency = st.sidebar.selectbox("ê¸°ì¤€ í†µí™”", ["USD", "EUR", "JPY", "CNY"], index=0)

# (ì„ì‹œ í™˜ìœ¨ - ë‚˜ì¤‘ì— API ì—°ë™)
exchange_rates = {"USD": 1400, "EUR": 1500, "JPY": 9, "CNY": 190}
rate = exchange_rates.get(base_currency, 1400)

# í™˜ìœ¨ ê³„ì‚°ê¸°
st.sidebar.subheader("â‘  í™˜ìœ¨ ê³„ì‚°ê¸°")
foreign_price = st.sidebar.number_input(f"íŒë§¤ê¸ˆì•¡ ({base_currency})", 0.0, 1000000.0, 100.0)
converted_price = foreign_price * rate
st.sidebar.text_input("í™˜ì‚° ê¸ˆì•¡(ì½ê¸°ì „ìš©)", f"{converted_price:,.0f} KRW", disabled=True)

# ë§ˆì§„ ê³„ì‚°ê¸°
st.sidebar.subheader("â‘¡ ë§ˆì§„ ê³„ì‚°ê¸° (v23)")
m_sale_foreign = st.sidebar.number_input(f"íŒë§¤ê¸ˆì•¡ ({base_currency})", 0.0, 1000000.0, 100.0)
m_converted = m_sale_foreign * rate
st.sidebar.text_input("í™˜ì‚° ê¸ˆì•¡(ì½ê¸°ì „ìš©)", f"{m_converted:,.0f} KRW", disabled=True)

card_fee = st.sidebar.number_input("ì¹´ë“œìˆ˜ìˆ˜ë£Œ (%)", 0.0, 100.0, 4.0)
market_fee = st.sidebar.number_input("ë§ˆì¼“ìˆ˜ìˆ˜ë£Œ (%)", 0.0, 100.0, 14.0)
shipping_fee = st.sidebar.number_input("ë°°ì†¡ë¹„ (â‚©)", 0.0, 1000000.0, 0.0)

margin_mode = st.sidebar.radio("ë§ˆì§„ ë°©ì‹", ["í¼ì„¼íŠ¸ ë§ˆì§„(%)", "ë”í•˜ê¸° ë§ˆì§„(â‚©)"])
margin_value = st.sidebar.number_input("ë§ˆì§„ìœ¨ / ì¶”ê°€ê¸ˆ", 0.0, 1000000.0, 10.0)

# ê³„ì‚°ì‹ (v23 ë¡œì§)
if margin_mode == "í¼ì„¼íŠ¸ ë§ˆì§„(%)":
    final_price = m_converted * (1 + card_fee/100 + market_fee/100) * (1 + margin_value/100) + shipping_fee
else:
    final_price = m_converted * (1 + card_fee/100 + market_fee/100) + shipping_fee + margin_value

profit = final_price - m_converted

st.sidebar.markdown(f"ğŸ’° **íŒë§¤ê°€ê²© (ê³„ì‚° ê²°ê³¼):** {final_price:,.0f} KRW")
st.sidebar.markdown(f"ğŸ“ˆ **ìˆœì´ìµ (ë§ˆì§„):** {profit:,.0f} KRW")
# ============ Part 2: DataLab + Itemscout + SellerLife ============

import altair as alt
import time as _t

# ë„¤ì´ë²„ ì‡¼í•‘ ì¹´í…Œê³ ë¦¬ CID ë§¤í•‘
NAVER_CATEGORIES = {
    "íŒ¨ì…˜ì˜ë¥˜": "50000000", "íŒ¨ì…˜ì¡í™”": "50000001", "í™”ì¥í’ˆ/ë¯¸ìš©": "50000002",
    "ë””ì§€í„¸/ê°€ì „": "50000003", "ê°€êµ¬/ì¸í…Œë¦¬ì–´": "50000004", "ì‹í’ˆ": "50000005",
    "ìƒí™œ/ê±´ê°•": "50000006", "ì¶œì‚°/ìœ¡ì•„": "50000007", "ìŠ¤í¬ì¸ /ë ˆì €": "50000008",
    "ë„ì„œ/ì·¨ë¯¸/ì• ì™„": "50000009",
}
COMMON_HEADERS = {
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
    "Accept-Language":"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}

# ì¬ì‹œë„ ìœ í‹¸
def _retry_post(url, headers=None, data=None, timeout=12, tries=4):
    last = None
    for i in range(tries):
        try:
            r = requests.post(url, headers=headers, data=data, timeout=timeout)
            if r.status_code in (200, 201):
                return r
            if r.status_code in (403, 429):
                _t.sleep(1.2 * (2**i))
                continue
            last = r
        except Exception as e:
            last = e
    raise RuntimeError(f"POST ì‹¤íŒ¨: {last}")

# DataLab ìˆ˜ì§‘
def fetch_datalab_top20(cid: str, start_date: str, end_date: str, proxy: str|None=None) -> pd.DataFrame:
    # 1) ì„¸ì…˜ ì˜ˆì—´
    s = requests.Session()
    cat_url = "https://datalab.naver.com/shoppingInsight/sCategory.naver"
    s.get(cat_url, headers={**COMMON_HEADERS, "Accept":"text/html,*/*"}, timeout=10)

    # 2) API í˜¸ì¶œ (í”„ë¡ì‹œ ì„ íƒ)
    api_url = "https://datalab.naver.com/shoppingInsight/getCategoryKeywordRank.naver"
    if proxy:
        api_url = f"{proxy}?target=" + requests.utils.quote(api_url, safe="")

    headers = {
        **COMMON_HEADERS,
        "Accept":"application/json, text/javascript, */*; q=0.01",
        "Content-Type":"application/x-www-form-urlencoded; charset=UTF-8",
        "Origin":"https://datalab.naver.com",
        "Referer": cat_url,
        "X-Requested-With":"XMLHttpRequest",
    }
    payload = {
        "cid": cid, "timeUnit": "date",
        "startDate": start_date, "endDate": end_date,
        "device": "pc", "gender": "", "ages": ""
    }

    r = _retry_post(api_url, headers=headers, data=payload, timeout=12, tries=4)
    txt = r.text.strip()
    if not txt or not (txt.startswith("{") or txt.startswith("[")):
        raise RuntimeError("DataLab JSON ì•„ë‹˜(ì°¨ë‹¨/êµ¬ì¡°ë³€ê²½ ê°€ëŠ¥ì„±)")

    data = r.json()
    if "keywordList" not in data or not isinstance(data["keywordList"], list):
        raise RuntimeError("DataLab êµ¬ì¡° ë³€ê²½ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")

    rows = []
    for it in data["keywordList"][:20]:
        rows.append({
            "rank": it.get("rank") or len(rows)+1,
            "keyword": it.get("keyword", ""),
            "search": it.get("ratio") or 0,
        })
    df = pd.DataFrame(rows).sort_values("rank").reset_index(drop=True)
    return df


# ===== ìƒë‹¨ 3ì—´: ë°ì´í„°ë© / ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ / ì…€ëŸ¬ë¼ì´í”„ (ìˆœì„œ ê³ ì •) =====
c1, c2, c3 = st.columns(3)

with c1:
    st.subheader("ë°ì´í„°ë©")
    sel_cat = st.selectbox("ì¹´í…Œê³ ë¦¬", list(NAVER_CATEGORIES.keys()), index=0, key="dl_cat")
    cid = NAVER_CATEGORIES[sel_cat]
    proxy_hint = "https://your-proxy/app?target=<url>  (ì„ íƒ)"
    dl_proxy = st.text_input("í”„ë¡ì‹œ", "", key="dl_proxy", placeholder=proxy_hint)

    today = datetime.date.today()
    start = (today - datetime.timedelta(days=30)).strftime("%Y-%m-%d")
    end = today.strftime("%Y-%m-%d")

    try:
        df_dl = fetch_datalab_top20(cid, start, end, proxy=(dl_proxy or None))
        st.dataframe(df_dl, use_container_width=True, height=280)
        # ì‹¤ì„  ê·¸ë˜í”„
        chart = alt.Chart(df_dl).mark_line(point=True).encode(
            x=alt.X("rank:Q", title="ë­í¬(1=ìƒìœ„)"),
            y=alt.Y("search:Q", title="ê²€ìƒ‰ëŸ‰(ì§€ìˆ˜)"),
            tooltip=["rank","keyword","search"]
        ).properties(height=180)
        st.altair_chart(chart, use_container_width=True)
        st.download_button("Top20 CSV ë‹¤ìš´ë¡œë“œ", df_dl.to_csv(index=False).encode("utf-8-sig"),
                           "datalab_top20.csv", mime="text/csv", key="dl_csv")
        # êµ­ë‚´ ë ˆì´ë”ì—ì„œ ì“°ë„ë¡ ê³µìœ 
        st.session_state["datalab_df"] = df_dl.copy()
    except Exception as e:
        st.error(f"ë°ì´í„°ë© ì˜¤ë¥˜: {e}")

with c2:
    st.subheader("ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸")
    st.info("ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ì—°ë™ ëŒ€ê¸°(ë³„ë„ API/í”„ë¡ì‹œ ì—°ê²° ì˜ˆì •)")

with c3:
    st.subheader("ì…€ëŸ¬ë¼ì´í”„")
    st.info("ì…€ëŸ¬ë¼ì´í”„ ì—°ë™ ëŒ€ê¸°(ë³„ë„ API/í”„ë¡ì‹œ ì—°ê²° ì˜ˆì •)")
# ============ Part 3: AI í‚¤ì›Œë“œ ë ˆì´ë” (êµ­ë‚´/ê¸€ë¡œë²Œ) ============

import re
from bs4 import BeautifulSoup

# Rakuten AppID (ìƒìš©)
RAKUTEN_APP_ID = "1043271015809337425"  # â† ë„¤ê°€ ë°œê¸‰ë°›ì€ AppID ê·¸ëŒ€ë¡œ ì‚¬ìš©

def _retry_get(url, headers=None, timeout=12, tries=4):
    last = None
    for i in range(tries):
        try:
            r = requests.get(url, headers=headers, timeout=timeout)
            if r.status_code in (200, 201):
                return r
            if r.status_code in (403, 429):
                import time as _t; _t.sleep(1.2 * (2**i))
                continue
            last = r
        except Exception as e:
            last = e
    raise RuntimeError(f"GET ì‹¤íŒ¨: {last}")

# ---- Amazon ë² ìŠ¤íŠ¸ì…€ëŸ¬ (HTML íŒŒì‹± + í”„ë¡ì‹œ ì˜µì…˜) ----
def fetch_amazon_bestsellers(limit:int=15, proxy:str|None=None) -> pd.DataFrame:
    url = "https://www.amazon.com/Best-Sellers/zgbs"
    if proxy:
        url = f"{proxy}?target=" + requests.utils.quote(url, safe="")
    headers = {**COMMON_HEADERS, "Referer": "https://www.amazon.com/"}
    r = _retry_get(url, headers=headers, timeout=12, tries=4)

    soup = BeautifulSoup(r.text, "html.parser")
    titles=[]
    selectors = [
        "div.p13n-sc-truncate",
        "div._cDEzb_p13n-sc-css-line-clamp-3_g3dy1",
        "div._cDEzb_p13n-sc-css-line-clamp-2_EWgCb",
        "div.a-section.a-spacing-small > h3, div.a-section.a-spacing-small > a > span",
        "span.zg-text-center-align > div > a > div",
    ]
    for sel in selectors:
        for el in soup.select(sel):
            t = re.sub(r"\s+"," ", el.get_text(strip=True))
            if t and t not in titles:
                titles.append(t)
            if len(titles) >= limit:
                break
        if len(titles) >= limit:
            break

    if not titles:
        raise RuntimeError("Amazon íŒŒì‹± ì‹¤íŒ¨(êµ¬ì¡°ë³€ê²½/ì°¨ë‹¨ ê°€ëŠ¥)")
    df = pd.DataFrame({"rank": range(1, len(titles)+1), "keyword": titles[:limit]})
    df["score"] = [300 - i for i in range(1, len(df)+1)]
    df["source"] = "Amazon US"
    return df[["source","rank","keyword","score"]]

# ---- Rakuten ê³µì‹ Ranking API ----
def fetch_rakuten_ranking_api(app_id: str, genre_id: str|None=None,
                              period: str="day", limit:int=15) -> pd.DataFrame:
    """
    Rakuten Ichiba Item Ranking API (ì •ì‹)
    https://app.rakuten.co.jp/services/api/IchibaItem/Ranking/20170628
    """
    url = "https://app.rakuten.co.jp/services/api/IchibaItem/Ranking/20170628"
    params = {"applicationId": app_id, "format": "json", "periodType": period}
    if genre_id:
        params["genreId"] = genre_id
    r = requests.get(url, params=params, timeout=12)
    if r.status_code != 200:
        raise RuntimeError(f"Rakuten API ì˜¤ë¥˜: {r.status_code} / {r.text[:120]}")
    js = r.json()
    items = js.get("Items", [])
    rows=[]
    for it in items[:limit]:
        I = it.get("Item", {})
        rows.append({
            "rank": I.get("rank"),
            "keyword": I.get("itemName"),
            "score": 220 - (I.get("rank") or len(rows)+1),
        })
    if not rows:
        raise RuntimeError("Rakuten API ì‘ë‹µì— í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    df = pd.DataFrame(rows)
    df["source"] = "Rakuten JP"
    return df[["source","rank","keyword","score"]]

# ---- í•˜ë‹¨ 3ì—´ ì¤‘: ì¢Œì¸¡(ë ˆì´ë”) / ì¤‘ê°„(11ë²ˆê°€) / ìš°ì¸¡(ìƒí’ˆëª… ìƒì„±ê¸°) ----
d1, d2, d3 = st.columns(3)

with d1:
    st.subheader("AI í‚¤ì›Œë“œ ë ˆì´ë” (êµ­ë‚´/ê¸€ë¡œë²Œ)")

    mode = st.radio("ëª¨ë“œ", ["êµ­ë‚´","ê¸€ë¡œë²Œ"], horizontal=True, key="air_mode")
    if mode == "êµ­ë‚´":
        src = st.session_state.get("datalab_df")
        if src is not None and len(src):
            radar = (src.assign(source="DataLab",
                                score=lambda x: 1000 - x["rank"]*10)
                       [["source","keyword","score","rank"]]
                       .sort_values(["score","rank"], ascending=[False, True]))
            st.dataframe(radar, use_container_width=True, height=420)
            st.download_button("êµ­ë‚´ í‚¤ì›Œë“œ CSV",
                               radar.to_csv(index=False).encode("utf-8-sig"),
                               "radar_domestic.csv", mime="text/csv",
                               key="air_csv_dom")
        else:
            st.info("ë°ì´í„°ë© ê²°ê³¼ê°€ ì—†ì–´ í‘œì‹œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ê¸€ë¡œë²Œ: Amazon + Rakuten (ê³µì‹ API)
        amz_proxy = st.text_input("Amazon í”„ë¡ì‹œ(ì„ íƒ)",
                                  "", key="amz_proxy",
                                  placeholder="https://your-proxy/app?target=<url>")
        rak_genre = st.text_input("Rakuten genreId (ì„ íƒ, ë¹„ìš°ë©´ ì¢…í•©)",
                                  "", key="rak_genre")

        # ìˆ˜ì§‘
        try:
            df_amz = fetch_amazon_bestsellers(15, proxy=(amz_proxy or None))
        except Exception as e:
            st.error(f"Amazon ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            df_amz = pd.DataFrame(columns=["source","rank","keyword","score"])

        try:
            df_rak = fetch_rakuten_ranking_api(RAKUTEN_APP_ID,
                                               genre_id=(rak_genre or None),
                                               period="day", limit=15)
        except Exception as e:
            st.error(f"Rakuten API ì‹¤íŒ¨: {e}")
            df_rak = pd.DataFrame(columns=["source","rank","keyword","score"])

        df_glb = pd.concat([df_amz, df_rak], ignore_index=True)
        if len(df_glb):
            df_glb = df_glb.sort_values(["score","rank"], ascending=[False, True])
            st.dataframe(df_glb, use_container_width=True, height=420)
            st.download_button("ê¸€ë¡œë²Œ í‚¤ì›Œë“œ CSV",
                               df_glb.to_csv(index=False).encode("utf-8-sig"),
                               "radar_global.csv", mime="text/csv",
                               key="air_csv_glb")
        else:
            st.info("ê¸€ë¡œë²Œ ì†ŒìŠ¤ ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
# ============ Part 4: 11ë²ˆê°€ (ëª¨ë°”ì¼ í”„ë¡ì‹œ + ìš”ì•½í‘œ) & ìƒí’ˆëª… ìƒì„±ê¸° ============

# ---- ì¤‘ê°„ ì»¬ëŸ¼ (11ë²ˆê°€) ----
with d2:
    st.subheader("11ë²ˆê°€ (ëª¨ë°”ì¼ í”„ë¡ì‹œ + ìš”ì•½í‘œ)")

    url_11 = st.text_input("ëŒ€ìƒ URL", "https://www.11st.co.kr/", key="url_11")
    proxy_11 = st.text_input("í”„ë¡ì‹œ ì—”ë“œí¬ì¸íŠ¸(ì„ íƒ)", "", key="proxy_11")

    html11 = ""
    try:
        if proxy_11:
            tgt = f"{proxy_11}?target=" + requests.utils.quote(url_11, safe="")
            r = requests.get(tgt, headers=COMMON_HEADERS, timeout=10)
        else:
            r = requests.get(url_11, headers=COMMON_HEADERS, timeout=10)
        if r.status_code == 200:
            html11 = r.text
        else:
            st.error(f"11ë²ˆê°€ ì‘ë‹µ ì˜¤ë¥˜: {r.status_code}")
    except Exception as e:
        st.error(f"11ë²ˆê°€ ìš”ì²­ ì‹¤íŒ¨: {e}")

    if html11:
        st.components.v1.html(
            f"<iframe srcdoc='{html11}' width='100%' height='400'></iframe>",
            height=420, scrolling=True
        )

    st.button("ì„ë² ë“œ ì‹¤íŒ¨ ëŒ€ë¹„ ìš”ì•½í‘œ ë³´ê¸°")


# ---- ìš°ì¸¡ ì»¬ëŸ¼ (ìƒí’ˆëª… ìƒì„±ê¸°) ----
with d3:
    st.subheader("ìƒí’ˆëª… ìƒì„±ê¸° (ê·œì¹™ + HuggingFace KoGPT2)")

    brand = st.text_input("ë¸Œëœë“œ", "envy")
    base_kw = st.text_input("ë² ì´ìŠ¤ í‚¤ì›Œë“œ", "K-coffee mix")
    rel_kw = st.text_input("ì—°ê´€í‚¤ì›Œë“œ", "Maxim, Kanu, Korea")
    ban_kw = st.text_input("ê¸ˆì¹™ì–´", "copy, fake, replica")
    limit_len = st.slider("ê¸€ììˆ˜ ì œí•œ", 10, 120, 80)

    mode = st.radio("ëª¨ë“œ", ["ê·œì¹™ ê¸°ë°˜", "HuggingFace AI"], horizontal=True, key="gen_mode")

    if st.button("ìƒì„±"):
        if mode == "ê·œì¹™ ê¸°ë°˜":
            # ê°„ë‹¨ ê·œì¹™ ìƒì„±ê¸°
            out = f"{brand} {base_kw} {rel_kw}".replace(",", " ")
            for w in ban_kw.split(","):
                out = out.replace(w.strip(), "")
            st.success(out[:limit_len])
        else:
            try:
                HF_KEY = "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"  # ë„¤ê°€ ë°œê¸‰ë°›ì€ HuggingFace API í‚¤
                headers = {
                    "Authorization": f"Bearer {HF_KEY}",
                    "Content-Type": "application/json",
                }
                payload = {
                    "inputs": f"{brand} {base_kw} {rel_kw}",
                    "parameters": {"max_new_tokens": 32, "return_full_text": False},
                }
                r = requests.post(
                    "https://api-inference.huggingface.co/models/skt/kogpt2-base-v2",
                    headers=headers, json=payload, timeout=15
                )
                if r.status_code == 200:
                    js = r.json()
                    text = js[0]["generated_text"]
                    for w in ban_kw.split(","):
                        text = text.replace(w.strip(), "")
                    st.success(text[:limit_len])
                else:
                    st.error(f"HuggingFace API ì˜¤ë¥˜: {r.status_code} {r.text}")
            except Exception as e:
                st.error(f"HuggingFace í˜¸ì¶œ ì‹¤íŒ¨: {e}")
