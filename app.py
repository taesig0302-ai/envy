# -*- coding: utf-8 -*-
# ENVY â€” Season 1 (Dual Proxy Edition, Radar tabs=êµ­ë‚´/í•´ì™¸, Rakuten scope radio removed, row1 ratio 8:5:3)
# ì´ë²ˆ ë²„ì „:
# - ìƒí’ˆëª… ìƒì„±ê¸° ì¹´ë“œ ë‚´ë¶€ íƒ­: [ìƒì„±ê¸° | ê¸ˆì¹™ì–´ ê´€ë¦¬]
# - ì™¸ë¶€ ê¸ˆì¹™ì–´ ì„¹ì…˜ì€ ìœ ì§€(ì„ íƒ). ë™ì¼ ì„¸ì…˜í‚¤ ê³µìœ ë¡œ ë™ê¸°í™”ë¨.
# - ì‚¬ì´ë“œë°”: ë‹¤í¬+ë²ˆì—­ê¸° í† ê¸€ / ë²ˆì—­ê¸° ON: ë²ˆì—­ê¸° í¼ì¹¨Â·ê³„ì‚°ê¸° ì ‘í˜, OFF: ê³„ì‚°ê¸° í¼ì¹¨Â·ë²ˆì—­ê¸° ì ‘í˜

import base64, time, re, math, json, io, datetime as dt
from pathlib import Path
from urllib.parse import quote

import pandas as pd
import streamlit as st

# -------- Optional imports --------
try:
    import requests
except Exception:
    requests = None

try:
    from deep_translator import GoogleTranslator
except Exception:
    GoogleTranslator = None

st.set_page_config(page_title="ENVY â€” Season 1 (Dual Proxy Edition)", layout="wide")

# =========================
# 0) GLOBALS & DEFAULT KEYS
# =========================
SHOW_ADMIN_BOX = False

# Proxies (Cloudflare Worker)
NAVER_PROXY      = "https://envy-proxy.taesig0302.workers.dev"
ELEVENST_PROXY   = "https://worker-11stjs.taesig0302.workers.dev"
ITEMSCOUT_PROXY  = "https://worker-itemscoutjs.taesig0302.workers.dev"
SELLERLIFE_PROXY = "https://worker-sellerlifejs.taesig0302.workers.dev"

# ---- Default credentials (secrets ê°€ ìˆìœ¼ë©´ secrets ìš°ì„ ) ----
DEFAULT_KEYS = {
    # Rakuten
    "RAKUTEN_APP_ID":       "1043271015809337425",
    "RAKUTEN_AFFILIATE_ID": "4c723498.cbfeca46.4c723499.1deb6f77",
    # NAVER Searchad(ê²€ìƒ‰ê´‘ê³  API / í‚¤ì›Œë“œë„êµ¬)
    "NAVER_API_KEY":        "0100000000785cf1d8f039b13a5d3c3d1262b84e9ad4a046637e8887bbd003051b0d2a5cdf",
    "NAVER_SECRET_KEY":     "AQAAAAB4XPHY8DmxOl08PRJiuE6ao1LN3lh0kF9rOJ4m5b8O5g==",
    "NAVER_CUSTOMER_ID":    "2274338",
    # NAVER Developers (DataLab Open API)
    "NAVER_CLIENT_ID":      "nBay2VW6uz7E4bZnZ2y9",
    "NAVER_CLIENT_SECRET":  "LNuLh1E3e1",
}
def _get_key(name: str) -> str:
    return (st.secrets.get(name, "") or DEFAULT_KEYS.get(name, "")).strip()

# Simple FX
CURRENCIES = {
    "USD":{"kr":"ë¯¸êµ­ ë‹¬ëŸ¬","symbol":"$","unit":"USD"},
    "EUR":{"kr":"ìœ ë¡œ","symbol":"â‚¬","unit":"EUR"},
    "JPY":{"kr":"ì¼ë³¸ ì—”","symbol":"Â¥","unit":"JPY"},
    "CNY":{"kr":"ì¤‘êµ­ ìœ„ì•ˆ","symbol":"å…ƒ","unit":"CNY"},
}
FX_DEFAULT = {"USD":1400.0,"EUR":1500.0,"JPY":10.0,"CNY":200.0}

# =========================
# Stopwords â€” ì „ì—­/ì¹´í…Œê³ ë¦¬ + í”„ë¦¬ì…‹
# =========================
STOPWORDS_GLOBAL = [
    # ê´‘ê³ /í–‰ì‚¬/ê°€ê²© ê³¼ì¥
    "ë¬´ë£Œë°°ì†¡","ë¬´ë°°","ì´ˆíŠ¹ê°€","íŠ¹ê°€","í•«ë”œ","ìµœì €ê°€","ì„¸ì¼","sale","ì´ë²¤íŠ¸","ì‚¬ì€í’ˆ","ì¦ì •",
    "ì¿ í°","ì—­ëŒ€ê¸‰","ì—­ëŒ€ê°€","í­íƒ„ì„¸ì¼","ì›ê°€","ì •ê°€","íŒŒê²©","ì´ˆëŒ€ë°•","í• ì¸í­","í˜œíƒê°€",
    # ìš´ì˜/AS ë¦¬ìŠ¤í¬
    "íŒŒì†","í™˜ë¶ˆ","êµí™˜","ì¬ê³ ","í’ˆì ˆ","í•œì •ìˆ˜ëŸ‰","ê¸´ê¸‰","ê¸‰ì²˜","íŠ¹íŒ",
    # ê³¼ë„í•œ ë§ˆì¼€íŒ… í‘œí˜„/ì´ëª¨ì§€
    "mustbuy","ê°•ì¶”","ì¶”ì²œ","ì¶”ì²œí…œ","ğŸ”¥","ğŸ’¥","â­","best","ë² ìŠ¤íŠ¸"
]
STOPWORDS_BY_CAT = {
    "íŒ¨ì…˜ì˜ë¥˜":   ["ë£¨ì¦ˆí•","ë¹…ì‚¬ì´ì¦ˆ","ì´ˆìŠ¬ë¦¼","ê·¹ì„¸ì‚¬","ì´ˆê²½ëŸ‰","ì™•ì˜¤ë²„","ëª¸ë§¤ë³´ì •"],
    "íŒ¨ì…˜ì¡í™”":   ["ë¬´ë£Œê°ì¸","ì‚¬ì€í’ˆì§€ê¸‰","ì„¸íŠ¸ì¦ì •"],
    "ë·°í‹°/ë¯¸ìš©":  ["ì •í’ˆë³´ì¥","ë³‘í–‰ìˆ˜ì…","ë²Œí¬","ë¦¬í•„ë§Œ","ìƒ˜í”Œ","í…ŒìŠ¤í„°"],
    "ìƒí™œ/ê±´ê°•":  ["ê³µìš©","ë¹„ë§¤í’ˆ","ë¦¬í¼","ë¦¬í¼ë¹„ì‹œ"],
    "ë””ì§€í„¸/ê°€ì „": ["ê´€ë¶€ê°€ì„¸","ë¶€ê°€ì„¸","í•´ì™¸ì§êµ¬","ë¦¬í¼","ë¦¬í¼ë¹„ì‹œ","ë²Œí¬"],
    "ìŠ¤í¬ì¸ /ë ˆì €": ["ë¬´ë£Œì¡°ë¦½","ê°€ì„±ë¹„ê°‘"],
}
STOP_PRESETS = {
    "ë„¤ì´ë²„_ì•ˆì „ê¸°ë³¸": {
        "global": STOPWORDS_GLOBAL, "by_cat": STOPWORDS_BY_CAT, "whitelist": [],
        "replace": ["ë¬´ë°°=> ", "ë¬´ë£Œë°°ì†¡=> ", "ì •í’ˆ=> "], "aggressive": False
    },
    "ê´‘ê³ í‘œí˜„_ê°•ë ¥ì°¨ë‹¨": {
        "global": STOPWORDS_GLOBAL + ["ì´ˆê°•ë ¥","ì´ˆì €ê°€","ê·¹ê°•","í˜œì","ëŒ€ë€","í’ˆì ˆì„ë°•","ì™„íŒì„ë°•","ë§ˆê°ì„ë°•"],
        "by_cat": STOPWORDS_BY_CAT, "whitelist": [],
        "replace": ["ë¬´ë°°=> ", "ë¬´ë£Œë°°ì†¡=> ", "ì •í’ˆ=> ", "í• ì¸=> "], "aggressive": True
    }
}

# =========================
# 1) UI defaults & CSS
# =========================
def _ensure_session_defaults():
    ss = st.session_state
    ss.setdefault("theme","light")
    ss.setdefault("fx_base","USD")
    ss.setdefault("sale_foreign",1.00)
    ss.setdefault("m_base","USD")
    ss.setdefault("purchase_foreign",0.00)
    ss.setdefault("card_fee_pct",4.00)
    ss.setdefault("market_fee_pct",14.00)
    ss.setdefault("shipping_won",0.0)
    ss.setdefault("margin_mode","í¼ì„¼íŠ¸")
    ss.setdefault("margin_pct",10.00)
    ss.setdefault("margin_won",10000.0)
    # Stopwords manager ìƒíƒœ
    ss.setdefault("STOP_GLOBAL", list(STOPWORDS_GLOBAL))
    ss.setdefault("STOP_BY_CAT", dict(STOPWORDS_BY_CAT))
    ss.setdefault("STOP_WHITELIST", [])
    ss.setdefault("STOP_REPLACE", ["ë¬´ë°°=> ", "ë¬´ë£Œë°°ì†¡=> ", "ì •í’ˆ=> "])
    ss.setdefault("STOP_AGGR", False)
    # Rakuten genre map
    ss.setdefault("rk_genre_map", {
        "ì „ì²´(ìƒ˜í”Œ)": "100283","ë·°í‹°/ì½”ìŠ¤ë©”í‹±": "100283","ì˜ë¥˜/íŒ¨ì…˜": "100283","ê°€ì „/ë””ì§€í„¸": "100283",
        "ê°€êµ¬/ì¸í…Œë¦¬ì–´": "100283","ì‹í’ˆ": "100283","ìƒí™œ/ê±´ê°•": "100283","ìŠ¤í¬ì¸ /ë ˆì €": "100283","ë¬¸êµ¬/ì·¨ë¯¸": "100283",
    })

def _toggle_theme():
    st.session_state["theme"]="dark" if st.session_state.get("theme","light")=="light" else "light"

def _inject_css():
    theme = st.session_state.get("theme","light")
    bg, fg = ("#0e1117","#e6edf3") if theme=="dark" else ("#ffffff","#111111")
    st.markdown(f"""
    <style>
      .block-container{{max-width:3800px!important;padding-top:.55rem!important;padding-bottom:1rem!important}}
      html,body,[data-testid="stAppViewContainer"]{{background:{bg}!important;color:{fg}!important}}
      h2,h3{{margin-top:.3rem!important}}
      [data-testid="stSidebar"],[data-testid="stSidebar"]>div:first-child,[data-testid="stSidebar"] section{{height:100vh!important;overflow:hidden!important;padding:.15rem .25rem!important}}
      [data-testid="stSidebar"] section{{overflow-y:auto!important}}
      [data-testid="stSidebar"] ::-webkit-scrollbar{{display:none!important}}
      [data-testid="stSidebar"] .stSelectbox,.stNumberInput,.stRadio,.stMarkdown,.stTextInput,.stButton{{margin:.06rem 0!important}}
      [data-baseweb="input"] input,.stNumberInput input,[data-baseweb="select"] div[role="combobox"]{{height:1.55rem!important;padding:.12rem .6rem!important;font-size:.96rem!important;border-radius:12px!important}}
      .pill{{border-radius:9999px;padding:.40rem .9rem;font-weight:800;display:inline-block;margin:.10rem 0!important}}
      .pill-green{{background:#b8f06c;border:1px solid #76c02a;color:#083500}}
      .pill-blue{{background:#dbe6ff;border:1px solid #88a8ff;color:#09245e}}
      .pill-yellow{{background:#ffe29b;border:1px solid #d2a12c;color:#3e2a00}}
      .card{{border:1px solid rgba(0,0,0,.06);border-radius:14px;padding:.85rem;background:#fff;box-shadow:0 1px 6px rgba(0,0,0,.05)}}
      .card-title{{font-size:1.18rem;font-weight:900;margin:.1rem 0 .55rem 0}}
      .card iframe{{border:0;width:100%;border-radius:10px}}
      .row-gap{{height:16px}}
      .logo-circle{{width:72px;height:72px;border-radius:50%;overflow:hidden;margin:.2rem auto .4rem auto;box-shadow:0 2px 8px rgba(0,0,0,.12);border:1px solid rgba(0,0,0,.06)}}
      .logo-circle img{{width:100%;height:100%;object-fit:cover}}
      #rk-card [data-testid="stDataFrame"] * {{ font-size: 0.92rem !important; }}
      #rk-card [data-testid="stDataFrame"] div[role='grid']{{ overflow-x: hidden !important; }}
      #rk-card [data-testid="stDataFrame"] div[role='gridcell']{{ white-space: normal !important; word-break: break-word !important; overflow-wrap: anywhere !important; }}
    </style>
    """, unsafe_allow_html=True)

def _inject_alert_center():
    st.markdown("""
    <div id="envy-alert-root" style="position:fixed;top:16px;right:16px;z-index:999999;pointer-events:none;"></div>
    <style>
      .envy-toast{min-width:220px;max-width:420px;margin:8px 0;padding:.7rem 1rem;border-radius:12px;color:#fff;font-weight:700;box-shadow:0 8px 24px rgba(0,0,0,.25);opacity:0;transform:translateY(-6px);transition:opacity .2s ease, transform .2s ease;}
      .envy-toast.show{opacity:1;transform:translateY(0)}
      .envy-info{background:#2563eb}.envy-warn{background:#d97706}.envy-error{background:#dc2626}
    </style>
    <script>
      (function(){
        const root = document.getElementById('envy-alert-root');
        function toast(level, text){
          const el = document.createElement('div');
          el.className='envy-toast envy-'+(level||'info'); el.textContent=text||'ì•Œë¦¼';
          el.style.pointerEvents='auto'; root.appendChild(el);
          requestAnimationFrame(()=>el.classList.add('show'));
          setTimeout(()=>{el.classList.remove('show'); setTimeout(()=>el.remove(), 300);}, 5000);
        }
        window.addEventListener('message',(e)=>{ const d=e.data||{}; if(d.__envy && d.kind==='alert'){toast(d.level,d.msg);} },false);
        let heard=false; window.addEventListener('message',(e)=>{ const d=e.data||{}; if(d.__envy && d.kind==='title'){heard=true;}},false);
        setTimeout(()=>{ if(!heard){ toast('warn','ë°ì´í„°ë© ì—°ê²°ì´ ì§€ì—°ë˜ê³  ìˆì–´ìš”.'); } },8000);
      })();
    </script>
    """, unsafe_allow_html=True)

# =========================
# 2) Responsive
# =========================
def _responsive_probe():
    html = """
    <script>
    (function(){
      const bps=[900,1280,1600];
      const w=Math.max(document.documentElement.clientWidth||0, window.innerWidth||0);
      let bin=0; for(let i=0;i<bps.length;i++) if(w>=bps[i]) bin=i+1;
      const url=new URL(window.location); const curr=url.searchParams.get('vwbin');
      if(curr!==String(bin)){ url.searchParams.set('vwbin', String(bin)); window.location.replace(url.toString()); }
    })();
    </script>
    """
    st.components.v1.html(html, height=0, scrolling=False)

def _get_view_bin():
    try:
        raw = st.query_params.get("vwbin", "3")
    except Exception:
        raw = (st.experimental_get_query_params().get("vwbin", ["3"])[0])
    try:
        return max(0, min(3, int(raw)))
    except:
        return 3

# =========================
# 3) Generic proxy iframe
# =========================
def _proxy_iframe(proxy_base: str, target_url: str, height: int = 860, scroll=True, key=None):
    proxy = (proxy_base or "").strip().rstrip("/")
    url   = f"{proxy}/?url={quote(target_url, safe=':/?&=%')}"
    h     = int(height)
    try:
        st.iframe(url, height=h); return
    except Exception:
        pass
    try:
        st.components.v1.iframe(url, height=h, scrolling=bool(scroll)); return
    except Exception:
        pass
    st.markdown(f'<iframe src="{url}" style="width:100%;height:{h}px;border:0;border-radius:10px;"></iframe>',
                unsafe_allow_html=True)

def _proxy_iframe_with_title(proxy_base: str, target_url: str, height: int = 860, key: str = "naver_home"):
    proxy = (proxy_base or "").strip().rstrip("/")
    url   = f"{proxy}/?url={quote(target_url, safe=':/?&=%')}"
    h     = int(height)
    html  = f'''
<div id="{key}-wrap" style="width:100%;overflow:hidden;">
  <div id="{key}-title"
       style="display:inline-block;border-radius:9999px;padding:.40rem .9rem;
              font-weight:800;background:#dbe6ff;border:1px solid #88a8ff;color:#09245e;margin:0 0 .5rem 0;">
    DataLab
  </div>
  <iframe src="{url}" style="width:100%;height:{h}px;border:0;border-radius:10px;"></iframe>
</div>
<script>
(function(){{
  var titleEl=document.getElementById("{key}-title");
  window.addEventListener("message",function(e){{
    try{{var d=e.data||{{}}; if(d.__envy && d.kind==="title" && d.title) titleEl.textContent=d.title;}}catch(_){{
    }}
  }},false);
}})();
</script>
'''
    st.components.v1.html(html, height=h+56, scrolling=False)

# =========================
# 4) Sidebar (theme + translator toggle + calculators)
# =========================
def _sidebar():
    _ensure_session_defaults(); _inject_css(); _inject_alert_center()
    with st.sidebar:
        lp = Path(__file__).parent / "logo.png"
        if lp.exists():
            b64 = base64.b64encode(lp.read_bytes()).decode("ascii")
            st.markdown(f'<div class="logo-circle"><img src="data:image/png;base64,{b64}"></div>', unsafe_allow_html=True)

        c1, c2 = st.columns(2)
        with c1:
            st.toggle("ğŸŒ“ ë‹¤í¬", value=(st.session_state.get("theme","light")=="dark"),
                      on_change=_toggle_theme, key="__theme_toggle")
        with c2:
            st.toggle("ğŸŒ ë²ˆì—­ê¸°", value=False, key="__show_translator")
        show_tr = st.session_state.get("__show_translator", False)

        def translator_block(expanded=True):
            with st.expander("ğŸŒ êµ¬ê¸€ ë²ˆì—­ê¸°", expanded=expanded):
                LANG_LABELS_SB = {
                    "auto":"ìë™ ê°ì§€","ko":"í•œêµ­ì–´","en":"ì˜ì–´","ja":"ì¼ë³¸ì–´","zh-CN":"ì¤‘êµ­ì–´(ê°„ì²´)",
                    "zh-TW":"ì¤‘êµ­ì–´(ë²ˆì²´)","vi":"ë² íŠ¸ë‚¨ì–´","th":"íƒœêµ­ì–´","id":"ì¸ë„ë„¤ì‹œì•„ì–´",
                    "de":"ë…ì¼ì–´","fr":"í”„ë‘ìŠ¤ì–´","es":"ìŠ¤í˜ì¸ì–´","it":"ì´íƒˆë¦¬ì•„ì–´","pt":"í¬ë¥´íˆ¬ê°ˆì–´"
                }
                def _code_sb(x): return {v:k for k,v in LANG_LABELS_SB.items()}.get(x, x)
                src_label = st.selectbox("ì›ë¬¸ ì–¸ì–´", list(LANG_LABELS_SB.values()),
                                         index=list(LANG_LABELS_SB.keys()).index("auto"), key="sb_tr_src")
                tgt_label = st.selectbox("ë²ˆì—­ ì–¸ì–´", list(LANG_LABELS_SB.values()),
                                         index=list(LANG_LABELS_SB.keys()).index("ko"), key="sb_tr_tgt")
                text_in = st.text_area("í…ìŠ¤íŠ¸", height=120, key="sb_tr_in")
                if st.button("ë²ˆì—­ ì‹¤í–‰", key="sb_tr_btn"):
                    try:
                        from deep_translator import GoogleTranslator as _GT
                    except Exception:
                        _GT = None
                    if not _GT:
                        st.error("deep-translator ì„¤ì¹˜ í•„ìš” ë˜ëŠ” ëŸ°íƒ€ì„ ë¬¸ì œ")
                    else:
                        try:
                            src_code = _code_sb(src_label); tgt_code = _code_sb(tgt_label)
                            out_main = _GT(source=src_code, target=tgt_code).translate(text_in or "")
                            st.text_area(f"ê²°ê³¼ ({tgt_label})", value=out_main, height=120, key="sb_tr_out_main")
                            if tgt_code != "ko":
                                out_ko = _GT(source=tgt_code, target="ko").translate(out_main or "")
                                st.text_area("ê²°ê³¼ (í•œêµ­ì–´)", value=out_ko, height=120, key="sb_tr_out_ko")
                        except Exception as e:
                            st.error(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {e}")

        def fx_block(expanded=True):
            with st.expander("ğŸ’± í™˜ìœ¨ ê³„ì‚°ê¸°", expanded=expanded):
                fx_base = st.selectbox("ê¸°ì¤€ í†µí™”", list(CURRENCIES.keys()),
                                       index=list(CURRENCIES.keys()).index(st.session_state.get("fx_base","USD")),
                                       key="fx_base")
                sale_foreign = st.number_input("íŒë§¤ê¸ˆì•¡ (ì™¸í™”)", value=float(st.session_state.get("sale_foreign",1.0)),
                                               step=0.01, format="%.2f", key="sale_foreign")
                won = FX_DEFAULT[fx_base]*sale_foreign
                st.markdown(
                    f'<div class="pill pill-green">í™˜ì‚° ê¸ˆì•¡: <b>{won:,.2f} ì›</b>'
                    f'<span style="opacity:.75;font-weight:700"> ({CURRENCIES[fx_base]["symbol"]})</span></div>',
                    unsafe_allow_html=True
                )
                st.caption(f"í™˜ìœ¨ ê¸°ì¤€: {FX_DEFAULT[fx_base]:,.2f} â‚©/{CURRENCIES[fx_base]['unit']}")

        def margin_block(expanded=True):
            with st.expander("ğŸ“ˆ ë§ˆì§„ ê³„ì‚°ê¸°", expanded=expanded):
                m_base = st.selectbox("ë§¤ì… í†µí™”", list(CURRENCIES.keys()),
                                      index=list(CURRENCIES.keys()).index(st.session_state.get("m_base","USD")),
                                      key="m_base")
                purchase_foreign = st.number_input("ë§¤ì…ê¸ˆì•¡ (ì™¸í™”)",
                                                   value=float(st.session_state.get("purchase_foreign",0.0)),
                                                   step=0.01, format="%.2f", key="purchase_foreign")
                base_cost_won = FX_DEFAULT[m_base]*purchase_foreign if purchase_foreign>0 \
                                else FX_DEFAULT[st.session_state.get("fx_base","USD")]*st.session_state.get("sale_foreign",1.0)
                st.markdown(f'<div class="pill pill-green">ì›ê°€(â‚©): <b>{base_cost_won:,.2f} ì›</b></div>', unsafe_allow_html=True)
                c1, c2 = st.columns(2)
                with c1:
                    card_fee = st.number_input("ì¹´ë“œìˆ˜ìˆ˜ë£Œ(%)", value=float(st.session_state.get("card_fee_pct",4.0)),
                                               step=0.01, format="%.2f", key="card_fee_pct")
                with c2:
                    market_fee = st.number_input("ë§ˆì¼“ìˆ˜ìˆ˜ë£Œ(%)", value=float(st.session_state.get("market_fee_pct",14.0)),
                                                 step=0.01, format="%.2f", key="market_fee_pct")
                shipping_won = st.number_input("ë°°ì†¡ë¹„(â‚©)", value=float(st.session_state.get("shipping_won",0.0)),
                                               step=100.0, format="%.0f", key="shipping_won")
                mode = st.radio("ë§ˆì§„ ë°©ì‹", ["í¼ì„¼íŠ¸","í”ŒëŸ¬ìŠ¤"], horizontal=True, key="margin_mode")
                if mode=="í¼ì„¼íŠ¸":
                    margin_pct = st.number_input("ë§ˆì§„ìœ¨ (%)", value=float(st.session_state.get("margin_pct",10.0)),
                                                 step=0.01, format="%.2f", key="margin_pct")
                    target_price = base_cost_won*(1+card_fee/100)*(1+market_fee/100)*(1+margin_pct/100)+shipping_won
                    margin_value = target_price - base_cost_won; desc=f"{margin_pct:.2f}%"
                else:
                    margin_won = st.number_input("ë§ˆì§„ì•¡ (â‚©)", value=float(st.session_state.get("margin_won",10000.0)),
                                                 step=100.0, format="%.0f", key="margin_won")
                    target_price = base_cost_won*(1+card_fee/100)*(1+market_fee/100)+margin_won+shipping_won
                    margin_value = margin_won; desc=f"+{margin_won:,.0f}"
                st.markdown(f'<div class="pill pill-blue">íŒë§¤ê°€: <b>{target_price:,.2f} ì›</b></div>', unsafe_allow_html=True)
                st.markdown(f'<div class="pill pill-yellow">ìˆœì´ìµ(ë§ˆì§„): <b>{margin_value:,.2f} ì›</b> â€” {desc}</div>', unsafe_allow_html=True)

        if show_tr:
            translator_block(expanded=True); fx_block(expanded=False); margin_block(expanded=False)
        else:
            fx_block(expanded=True); margin_block(expanded=True); translator_block(expanded=False)

        if SHOW_ADMIN_BOX:
            st.divider()
            st.text_input("PROXY_URL(ë””ë²„ê·¸)", key="PROXY_URL", help="Cloudflare Worker ì£¼ì†Œ (ì˜µì…˜)")

# =========================
# 5) Rakuten Ranking
# =========================
def _rakuten_keys():
    app_id = _get_key("RAKUTEN_APP_ID")
    affiliate = _get_key("RAKUTEN_AFFILIATE_ID")
    return app_id, affiliate

@st.cache_data(ttl=900, show_spinner=False)
def _rk_fetch_rank_cached(genre_id: str, topn: int = 20, strip_emoji: bool=True) -> pd.DataFrame:
    app_id, affiliate = _rakuten_keys()
    def _clean(s: str) -> str:
        if not strip_emoji: return s
        return re.sub(r"[\U00010000-\U0010ffff]", "", s or "")
    if not (requests and app_id):
        rows=[{"rank":i+1,"keyword":_clean(f"[ìƒ˜í”Œ] í‚¤ì›Œë“œ {i+1} ãƒãƒ­ã‚¦ã‚£ãƒ³ ç§‹ ğŸ‚"),
               "shop":"ìƒ˜í”Œ","url":"https://example.com"} for i in range(topn)]
        return pd.DataFrame(rows)
    def _do():
        api = "https://app.rakuten.co.jp/services/api/IchibaItem/Ranking/20170628"
        params = {"applicationId": app_id, "genreId": str(genre_id).strip(), "hits": topn}
        if affiliate: params["affiliateId"] = affiliate
        r = requests.get(api, params=params, timeout=12)
        r.raise_for_status()
        items = r.json().get("Items", [])[:topn]
        rows=[]
        for it in items:
            node = it.get("Item", {})
            rows.append({
                "rank": node.get("rank"),
                "keyword": _clean(node.get("itemName","")),
                "shop": node.get("shopName",""),
                "url": node.get("itemUrl",""),
            })
        return pd.DataFrame(rows)
    try:
        return _do()
    except Exception:
        rows=[{"rank":i+1,"keyword":_clean(f"[ìƒ˜í”Œ] í‚¤ì›Œë“œ {i+1} ãƒãƒ­ã‚¦ã‚£ãƒ³ ç§‹ ğŸ‚"),
               "shop":"ìƒ˜í”Œ","url":"https://example.com"} for i in range(topn)]
        return pd.DataFrame(rows)

def section_rakuten_ui():
    st.markdown('<div id="rk-card">', unsafe_allow_html=True)
    colB, colC = st.columns([2,1])
    with colB:
        cat = st.selectbox("ë¼ì¿ í… ì¹´í…Œê³ ë¦¬",
                           ["ì „ì²´(ìƒ˜í”Œ)","ë·°í‹°/ì½”ìŠ¤ë©”í‹±","ì˜ë¥˜/íŒ¨ì…˜","ê°€ì „/ë””ì§€í„¸","ê°€êµ¬/ì¸í…Œë¦¬ì–´","ì‹í’ˆ","ìƒí™œ/ê±´ê°•","ìŠ¤í¬ì¸ /ë ˆì €","ë¬¸êµ¬/ì·¨ë¯¸"], key="rk_cat")
    with colC:
        sample_only = st.checkbox("ìƒ˜í”Œ ë³´ê¸°", value=False, key="rk_sample")
    strip_emoji = st.toggle("ì´ëª¨ì§€ ì œê±°", value=True, key="rk_strip_emoji")
    genre_map = st.session_state.get("rk_genre_map", {})
    genre_id = (genre_map.get(cat) or "").strip() or "100283"
    with st.spinner("ë¼ì¿ í… ë­í‚¹ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦"):
        df = (pd.DataFrame([{"rank":i+1,"keyword":f"[ìƒ˜í”Œ] í‚¤ì›Œë“œ {i+1}","shop":"ìƒ˜í”Œ","url":"https://example.com"} for i in range(20)])
              if sample_only else _rk_fetch_rank_cached(genre_id, topn=20, strip_emoji=strip_emoji))
    colcfg = {
        "rank": st.column_config.NumberColumn("rank", width="small"),
        "keyword": st.column_config.TextColumn("keyword", width="medium"),
        "shop": st.column_config.TextColumn("shop", width="small"),
        "url": st.column_config.LinkColumn("url", display_text="ì—´ê¸°", width="small"),
    }
    st.dataframe(df[["rank","keyword","shop","url"]], hide_index=True, use_container_width=True, height=430, column_config=colcfg)
    st.download_button("í‘œ CSV ë‹¤ìš´ë¡œë“œ", data=df.to_csv(index=False).encode("utf-8-sig"),
                       file_name="rakuten_ranking.csv", mime="text/csv")
    with st.expander("ğŸ”§ ì¥ë¥´ ë§¤í•‘ í¸ì§‘ (í™”ë©´ì—ëŠ” ìˆ¨ê¹€)", expanded=False):
        st.caption("ì¹´í…Œê³ ë¦¬ â†’ genreId ë§¤í•‘ì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ genreIdë¡œ ë°”ê¾¸ê³  ì €ì¥í•˜ì„¸ìš”.")
        g1, g2 = st.columns(2)
        with g1:
            for k in ["ë·°í‹°/ì½”ìŠ¤ë©”í‹±","ì˜ë¥˜/íŒ¨ì…˜","ê°€êµ¬/ì¸í…Œë¦¬ì–´","ìŠ¤í¬ì¸ /ë ˆì €","ë¬¸êµ¬/ì·¨ë¯¸"]:
                st.session_state["rk_genre_map"][k] = st.text_input(k, st.session_state["rk_genre_map"].get(k,"100283"), key=f"rk_{k}")
        with g2:
            for k in ["ê°€ì „/ë””ì§€í„¸","ì‹í’ˆ","ìƒí™œ/ê±´ê°•","ì „ì²´(ìƒ˜í”Œ)"]:
                st.session_state["rk_genre_map"][k] = st.text_input(k, st.session_state["rk_genre_map"].get(k,"100283"), key=f"rk_{k}")
        st.info("ì„¸ì…˜ì— ì €ì¥ë©ë‹ˆë‹¤. ì•± ì¬ì‹¤í–‰ ì‹œ ì´ˆê¸°ê°’ìœ¼ë¡œ ëŒì•„ì˜¬ ìˆ˜ ìˆì–´ìš”.")
    st.markdown('</div>', unsafe_allow_html=True)

# =========================
# 6) Korea Radar (Naver Searchad API)
# =========================
import hashlib, hmac, base64 as b64
def _naver_signature(timestamp: str, method: str, uri: str, secret: str) -> str:
    msg = f"{timestamp}.{method}.{uri}"
    digest = hmac.new(bytes(secret, "utf-8"), bytes(msg, "utf-8"), hashlib.sha256).digest()
    return b64.b64encode(digest).decode("utf-8")
def _naver_keys_from_secrets():
    ak = _get_key("NAVER_API_KEY"); sk = _get_key("NAVER_SECRET_KEY"); cid= _get_key("NAVER_CUSTOMER_ID")
    return ak.strip(), sk.strip(), str(cid).strip()
def _naver_keywordstool(hint_keywords: list[str]) -> pd.DataFrame:
    api_key, sec_key, customer_id = _naver_keys_from_secrets()
    if not (requests and api_key and sec_key and customer_id and hint_keywords):
        return pd.DataFrame()
    base_url="https://api.naver.com"; uri="/keywordstool"; ts = str(round(time.time()*1000))
    headers = {"X-API-KEY": api_key, "X-Signature": _naver_signature(ts, "GET", uri, sec_key),
               "X-Timestamp": ts, "X-Customer": customer_id}
    params={ "hintKeywords": ",".join(hint_keywords), "includeHintKeywords": "0", "showDetail": "1" }
    r = requests.get(base_url+uri, headers=headers, params=params, timeout=12)
    try:
        r.raise_for_status()
        data = r.json().get("keywordList", [])[:200]
        if not data: return pd.DataFrame()
        df = pd.DataFrame(data).rename(columns={
            "relKeyword":"í‚¤ì›Œë“œ","monthlyPcQcCnt":"PCì›”ê°„ê²€ìƒ‰ìˆ˜","monthlyMobileQcCnt":"Mobileì›”ê°„ê²€ìƒ‰ìˆ˜",
            "monthlyAvePcClkCnt":"PCì›”í‰ê· í´ë¦­ìˆ˜","monthlyAveMobileClkCnt":"Mobileì›”í‰ê· í´ë¦­ìˆ˜",
            "monthlyAvePcCtr":"PCì›”í‰ê· í´ë¦­ë¥ ","monthlyAveMobileCtr":"Mobileì›”í‰ê· í´ë¦­ë¥ ",
            "plAvgDepth":"ì›”í‰ê· ë…¸ì¶œê´‘ê³ ìˆ˜","compIdx":"ê´‘ê³ ê²½ìŸì •ë„",
        }).drop_duplicates(["í‚¤ì›Œë“œ"]).set_index("í‚¤ì›Œë“œ").reset_index()
        num_cols=["PCì›”ê°„ê²€ìƒ‰ìˆ˜","Mobileì›”ê°„ê²€ìƒ‰ìˆ˜","PCì›”í‰ê· í´ë¦­ìˆ˜","Mobileì›”í‰ê· í´ë¦­ìˆ˜","PCì›”í‰ê· í´ë¦­ë¥ ","Mobileì›”í‰ê· í´ë¦­ë¥ ","ì›”í‰ê· ë…¸ì¶œê´‘ê³ ìˆ˜"]
        for c in num_cols: df[c]=pd.to_numeric(df[c], errors="coerce")
        return df
    except Exception:
        return pd.DataFrame()

def _count_product_from_shopping(keyword: str) -> int|None:
    if not requests: return None
    try:
        url=f"https://search.shopping.naver.com/search/all?where=all&frm=NVSCTAB&query={quote(keyword)}"
        r=requests.get(url, timeout=10); r.raise_for_status()
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(r.text, "html.parser")
        a_tags = soup.select("a.subFilter_filter__3Y-uy")
        for a in a_tags:
            if "ì „ì²´" in a.text:
                span = a.find("span")
                if span:
                    txt = span.get_text().replace(",","").strip()
                    return int(re.sub(r"[^0-9]", "", txt) or "0")
        return None
    except Exception:
        return None

def section_korea_ui():
    st.caption("â€» ê²€ìƒ‰ì§€í‘œëŠ” ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  API(í‚¤ì›Œë“œë„êµ¬) ê¸°ì¤€, ìƒí’ˆìˆ˜ëŠ” ë„¤ì´ë²„ì‡¼í•‘ â€˜ì „ì²´â€™ íƒ­ í¬ë¡¤ë§ ê¸°ì¤€ì…ë‹ˆë‹¤.")
    c1, c2, c3 = st.columns([1,1,1])
    with c1:
        months = st.slider("ë¶„ì„ê¸°ê°„(ê°œì›”, í‘œì‹œìš©)", 1, 6, 3)
    with c2:
        device = st.selectbox("ë””ë°”ì´ìŠ¤", ["all","pc","mo"], index=0)
    with c3:
        src = st.selectbox("í‚¤ì›Œë“œ ì†ŒìŠ¤", ["ì§ì ‘ ì…ë ¥"], index=0)
    keywords_txt = st.text_area("í‚¤ì›Œë“œ(ì½¤ë§ˆë¡œ êµ¬ë¶„)", "í•¸ë“œë©”ì´ë“œì½”íŠ¸, ë‚¨ìì½”íŠ¸, ì—¬ìì½”íŠ¸", height=96)
    kw_list = [k.strip() for k in (keywords_txt or "").split(",") if k.strip()]
    opt1, opt2 = st.columns([1,1])
    with opt1:
        add_product = st.toggle("ë„¤ì´ë²„ì‡¼í•‘ â€˜ì „ì²´â€™ ìƒí’ˆìˆ˜ ìˆ˜ì§‘(ëŠë¦¼)", value=False)
    with opt2:
        table_mode = st.radio("í‘œ ëª¨ë“œ", ["A(ê²€ìƒ‰ì§€í‘œ)","B(ê²€ìƒ‰+ìˆœìœ„)","C(ê²€ìƒ‰+ìƒí’ˆìˆ˜+ìŠ¤ì½”ì–´)"], horizontal=True, index=2)
    if st.button("ë ˆì´ë” ì—…ë°ì´íŠ¸", use_container_width=False):
        with st.spinner("ë„¤ì´ë²„ í‚¤ì›Œë“œë„êµ¬ ì¡°íšŒ ì¤‘â€¦"):
            df = _naver_keywordstool(kw_list)
        if df.empty:
            st.error("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (API/ê¶Œí•œ/ì¿¼í„° ë˜ëŠ” í‚¤ì›Œë“œ í™•ì¸)")
            return
        if table_mode.startswith("A"):
            st.dataframe(df, use_container_width=True, height=430)
            st.download_button("CSV ë‹¤ìš´ë¡œë“œ", df.to_csv(index=False).encode("utf-8-sig"),
                               file_name="korea_keyword_A.csv", mime="text/csv"); return
        df2 = df.copy()
        df2["ê²€ìƒ‰í•©ê³„"] = (pd.to_numeric(df2["PCì›”ê°„ê²€ìƒ‰ìˆ˜"], errors="coerce").fillna(0) +
                           pd.to_numeric(df2["Mobileì›”ê°„ê²€ìƒ‰ìˆ˜"], errors="coerce").fillna(0))
        df2["ê²€ìƒ‰ìˆœìœ„"] = df2["ê²€ìƒ‰í•©ê³„"].rank(ascending=False, method="min")
        if table_mode.startswith("B"):
            out = df2.sort_values("ê²€ìƒ‰ìˆœìœ„")
            st.dataframe(out, use_container_width=True, height=430)
            st.download_button("CSV ë‹¤ìš´ë¡œë“œ", out.to_csv(index=False).encode("utf-8-sig"),
                               file_name="korea_keyword_B.csv", mime="text/csv"); return
        product_counts = []
        if add_product:
            with st.spinner("ë„¤ì´ë²„ì‡¼í•‘ ìƒí’ˆìˆ˜ ìˆ˜ì§‘ ì¤‘â€¦(í‚¤ì›Œë“œ ìˆ˜ì— ë”°ë¼ ìˆ˜ ë¶„ ì†Œìš”)"):
                for k in df2["í‚¤ì›Œë“œ"]:
                    cnt = _count_product_from_shopping(k)
                    product_counts.append(cnt if cnt is not None else math.nan)
        else:
            product_counts = [math.nan]*len(df2)
        df2["íŒë§¤ìƒí’ˆìˆ˜"] = product_counts
        df2["ìƒí’ˆìˆ˜ìˆœìœ„"] = df2["íŒë§¤ìƒí’ˆìˆ˜"].rank(na_option="bottom", method="min")
        df2["ìƒí’ˆë°œêµ´ëŒ€ìƒ"] = (df2["ê²€ìƒ‰ìˆœìœ„"] + df2["ìƒí’ˆìˆ˜ìˆœìœ„"]).rank(na_option="bottom", method="min")
        cols = ["í‚¤ì›Œë“œ","PCì›”ê°„ê²€ìƒ‰ìˆ˜","Mobileì›”ê°„ê²€ìƒ‰ìˆ˜","íŒë§¤ìƒí’ˆìˆ˜",
                "PCì›”í‰ê· í´ë¦­ìˆ˜","Mobileì›”í‰ê· í´ë¦­ìˆ˜","PCì›”í‰ê· í´ë¦­ë¥ ","Mobileì›”í‰ê· í´ë¦­ë¥ ",
                "ì›”í‰ê· ë…¸ì¶œê´‘ê³ ìˆ˜","ê´‘ê³ ê²½ìŸì •ë„","ê²€ìƒ‰ìˆœìœ„","ìƒí’ˆìˆ˜ìˆœìœ„","ìƒí’ˆë°œêµ´ëŒ€ìƒ"]
        out = df2[cols].sort_values("ìƒí’ˆë°œêµ´ëŒ€ìƒ")
        st.dataframe(out, use_container_width=True, height=430)
        st.download_button("CSV ë‹¤ìš´ë¡œë“œ", out.to_csv(index=False).encode("utf-8-sig"),
                           file_name="korea_keyword_C.csv", mime="text/csv")

# =========================
# 7) DataLab Trend (Open API) + Category â†’ Top20 UI (+ Direct Trend)
# =========================
@st.cache_data(ttl=1800, show_spinner=False)
def _datalab_trend(groups: list, start_date: str, end_date: str,
                   time_unit: str = "week", device: str = "", gender: str = "", ages: list | None = None) -> pd.DataFrame:
    if not requests: return pd.DataFrame()
    cid  = _get_key("NAVER_CLIENT_ID"); csec = _get_key("NAVER_CLIENT_SECRET")
    if not (cid and csec): return pd.DataFrame()
    ref = _get_key("NAVER_WEB_REFERER").strip() or "https://2vrc9owdssnberky8hssf7.streamlit.app"
    groups = (groups or [])[:5]
    url = "https://openapi.naver.com/v1/datalab/search"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec, "Content-Type": "application/json; charset=utf-8", "Referer": ref}
    payload = {"startDate": start_date, "endDate": end_date, "timeUnit": time_unit, "keywordGroups": groups}
    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=12)
    try:
        r.raise_for_status()
        js = r.json(); out=[]
        for gr in js.get("results", []):
            name = gr.get("title") or (gr.get("keywords") or [""])[0]
            tmp = pd.DataFrame(gr.get("data", []))
            if tmp.empty: continue
            tmp["keyword"] = name; out.append(tmp)
        if not out: return pd.DataFrame()
        big = pd.concat(out, ignore_index=True)
        big.rename(columns={"period": "ë‚ ì§œ", "ratio": "ê²€ìƒ‰ì§€ìˆ˜"}, inplace=True)
        pivot = big.pivot_table(index="ë‚ ì§œ", columns="keyword", values="ê²€ìƒ‰ì§€ìˆ˜", aggfunc="mean")
        pivot = pivot.reset_index().sort_values("ë‚ ì§œ")
        return pivot
    except Exception:
        return pd.DataFrame()

SEED_MAP = {
    "íŒ¨ì…˜ì˜ë¥˜":   ["ì›í”¼ìŠ¤","ì½”íŠ¸","ë‹ˆíŠ¸","ì…”ì¸ ","ë¸”ë¼ìš°ìŠ¤"],
    "íŒ¨ì…˜ì¡í™”":   ["ê°€ë°©","ì§€ê°‘","ëª¨ì","ìŠ¤ì¹´í”„","ë²¨íŠ¸"],
    "ë·°í‹°/ë¯¸ìš©":  ["ì¿ ì…˜","ë¦½ìŠ¤í‹±","ì„ í¬ë¦¼","ë§ˆìŠ¤ì¹´ë¼","í† ë„ˆ"],
    "ìƒí™œ/ê±´ê°•":  ["ì¹«ì†”","ì¹˜ì•½","ìƒ´í‘¸","ì„¸ì œ","ë¬¼í‹°ìŠˆ"],
    "ë””ì§€í„¸/ê°€ì „": ["ë¸”ë£¨íˆ¬ìŠ¤ì´ì–´í°","ìŠ¤í”¼ì»¤","ëª¨ë‹ˆí„°","ë…¸íŠ¸ë¶","ë¡œë´‡ì²­ì†Œê¸°"],
    "ìŠ¤í¬ì¸ /ë ˆì €": ["ëŸ¬ë‹í™”","ìš”ê°€ë³µ","ìº í•‘ì˜ì","í…íŠ¸","ìì „ê±°"],
}

def section_category_keyword_lab():
    st.markdown('<div class="card"><div class="card-title">ì¹´í…Œê³ ë¦¬ â†’ í‚¤ì›Œë“œ Top20 & íŠ¸ë Œë“œ</div>', unsafe_allow_html=True)
    cA, cB, cC = st.columns([1,1,1])
    with cA:
        cat = st.selectbox("ì¹´í…Œê³ ë¦¬", list(SEED_MAP.keys()))
    with cB:
        time_unit = st.selectbox("ë‹¨ìœ„", ["week", "month"], index=0)
    with cC:
        months = st.slider("ì¡°íšŒê¸°ê°„(ê°œì›”)", 1, 12, 3)
    start = (dt.date.today() - dt.timedelta(days=30 * months)).strftime("%Y-%m-%d")
    end   = (dt.date.today() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
    seeds = SEED_MAP.get(cat, [])
    df = _naver_keywordstool(seeds)
    if df.empty:
        st.warning("í‚¤ì›Œë“œë„êµ¬ ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. (API/ê¶Œí•œ/ì¿¼í„° í™•ì¸)")
        st.markdown('</div>', unsafe_allow_html=True); return
    df["ê²€ìƒ‰í•©ê³„"] = pd.to_numeric(df["PCì›”ê°„ê²€ìƒ‰ìˆ˜"], errors="coerce").fillna(0) + pd.to_numeric(df["Mobileì›”ê°„ê²€ìƒ‰ìˆ˜"], errors="coerce").fillna(0)
    top20 = df.sort_values("ê²€ìƒ‰í•©ê³„", ascending=False).head(20).reset_index(drop=True)
    st.dataframe(top20[["í‚¤ì›Œë“œ","ê²€ìƒ‰í•©ê³„","PCì›”ê°„ê²€ìƒ‰ìˆ˜","Mobileì›”ê°„ê²€ìƒ‰ìˆ˜","ì›”í‰ê· ë…¸ì¶œê´‘ê³ ìˆ˜","ê´‘ê³ ê²½ìŸì •ë„"]],
                 use_container_width=True, height=340)
    st.download_button("CSV ë‹¤ìš´ë¡œë“œ", top20.to_csv(index=False).encode("utf-8-sig"),
                       file_name=f"category_{cat}_top20.csv", mime="text/csv")
    topk = st.slider("ë¼ì¸ì°¨íŠ¸ í‚¤ì›Œë“œ ìˆ˜", 3, 10, 5, help="ìƒìœ„ Nê°œ í‚¤ì›Œë“œë§Œ íŠ¸ë Œë“œë¥¼ ê·¸ë¦½ë‹ˆë‹¤.")
    kws = top20["í‚¤ì›Œë“œ"].head(topk).tolist()
    groups = [{"groupName": k, "keywords": [k]} for k in kws]
    ts = _datalab_trend(groups, start, end, time_unit=time_unit)
    if ts.empty:
        st.info("DataLab íŠ¸ë Œë“œ ì‘ë‹µì´ ë¹„ì–´ ìˆì–´ìš”. (Client ID/Secret, Referer/í™˜ê²½, ë‚ ì§œ/ë‹¨ìœ„ í™•ì¸)")
    else:
        try:
            st.line_chart(ts.set_index("ë‚ ì§œ"))
        except Exception:
            st.dataframe(ts, use_container_width=True, height=260)
    st.markdown('</div>', unsafe_allow_html=True)

def section_keyword_trend_widget():
    st.markdown('<div class="card"><div class="card-title">í‚¤ì›Œë“œ íŠ¸ë Œë“œ (ì§ì ‘ ì…ë ¥)</div>', unsafe_allow_html=True)
    kwtxt  = st.text_input("í‚¤ì›Œë“œ(ì½¤ë§ˆ)", "ê°€ë°©, ì›í”¼ìŠ¤", key="kw_txt")
    unit   = st.selectbox("ë‹¨ìœ„", ["week", "month"], index=0, key="kw_unit")
    months = st.slider("ì¡°íšŒê¸°ê°„(ê°œì›”)", 1, 12, 3, key="kw_months")
    if st.button("íŠ¸ë Œë“œ ì¡°íšŒ", key="kw_run"):
        start = (dt.date.today() - dt.timedelta(days=30 * months)).strftime("%Y-%m-%d")
        end   = (dt.date.today() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
        kws = [k.strip() for k in (kwtxt or "").split(",") if k.strip()]
        groups = [{"groupName": k, "keywords": [k]} for k in kws][:5]
        df = _datalab_trend(groups, start, end, time_unit=unit)
        if df.empty:
            st.error("DataLab íŠ¸ë Œë“œ ì‘ë‹µì´ ë¹„ì–´ ìˆì–´ìš”. (Client ID/Secret, Referer/í™˜ê²½, ê¶Œí•œ/ì¿¼í„°/ë‚ ì§œ/ë‹¨ìœ„ í™•ì¸)")
        else:
            st.dataframe(df, use_container_width=True, height=260)
            st.line_chart(df.set_index("ë‚ ì§œ"))
    st.markdown('</div>', unsafe_allow_html=True)

# =========================
# 8) Radar Card (tabs: êµ­ë‚´ -> í•´ì™¸)
# =========================
def section_radar():
    st.markdown('<div class="card"><div class="card-title">AI í‚¤ì›Œë“œ ë ˆì´ë”</div>', unsafe_allow_html=True)
    tab_domestic, tab_overseas = st.tabs(["êµ­ë‚´", "í•´ì™¸"])
    with tab_domestic:
        section_korea_ui()
    with tab_overseas:
        section_rakuten_ui()
    st.markdown('</div>', unsafe_allow_html=True)

# =========================
# Stopwords Manager UI (ê³µìš©) â€” ìƒì„±ê¸° íƒ­/ì™¸ë¶€ ì„¹ì…˜ì—ì„œ ì¬ì‚¬ìš©
# =========================
def _stopwords_manager_ui(compact: bool = False):
    ss = st.session_state
    ss.setdefault("STOP_GLOBAL", list(STOPWORDS_GLOBAL))
    ss.setdefault("STOP_BY_CAT", dict(STOPWORDS_BY_CAT))
    ss.setdefault("STOP_WHITELIST", [])
    ss.setdefault("STOP_REPLACE", ["ë¬´ë°°=> ", "ë¬´ë£Œë°°ì†¡=> ", "ì •í’ˆ=> "])
    ss.setdefault("STOP_AGGR", False)

    # í”„ë¦¬ì…‹(ì»´íŒ©íŠ¸ ëª¨ë“œì—ì„  ìˆ¨ê¹€)
    if not compact:
        with st.expander("ğŸ”§ í”„ë¦¬ì…‹", expanded=False):
            preset = st.selectbox("í”„ë¦¬ì…‹", list(STOP_PRESETS.keys()), key="stop_preset_sel")
            if st.button("í”„ë¦¬ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°", key="stop_preset_load"):
                obj = STOP_PRESETS[preset]
                ss["STOP_GLOBAL"]    = list(obj.get("global", []))
                ss["STOP_BY_CAT"]    = dict(obj.get("by_cat", {}))
                ss["STOP_WHITELIST"] = list(obj.get("whitelist", []))
                ss["STOP_REPLACE"]   = list(obj.get("replace", []))
                ss["STOP_AGGR"]      = bool(obj.get("aggressive", False))
                st.success(f"í”„ë¦¬ì…‹ â€˜{preset}â€™ ì ìš© ì™„ë£Œ")

    tab_global, tab_cat, tab_white, tab_replace, tab_io = st.tabs(
        ["ì „ì—­ ê¸ˆì¹™ì–´", "ì¹´í…Œê³ ë¦¬ ê¸ˆì¹™ì–´", "í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸", "ì¹˜í™˜ ê·œì¹™", "ê°€ì ¸ì˜¤ê¸°/ë‚´ë ¤ë°›ê¸°"]
    )

    with tab_global:
        txt = st.text_area("ì „ì—­ ê¸ˆì¹™ì–´ (ì½¤ë§ˆ)", value=",".join(ss["STOP_GLOBAL"]), height=120, key="stop_glob_txt")
        if st.button("ì €ì¥(ì „ì—­)", key="stop_glob_save"):
            ss["STOP_GLOBAL"] = [t.strip() for t in txt.split(",") if t.strip()]
            st.success("ì „ì—­ ê¸ˆì¹™ì–´ ì €ì¥ ì™„ë£Œ")

    with tab_cat:
        all_cats = sorted(set(list(ss["STOP_BY_CAT"].keys()) + list(STOPWORDS_BY_CAT.keys()))) or \
                   ["íŒ¨ì…˜ì˜ë¥˜","íŒ¨ì…˜ì¡í™”","ë·°í‹°/ë¯¸ìš©","ìƒí™œ/ê±´ê°•","ë””ì§€í„¸/ê°€ì „","ìŠ¤í¬ì¸ /ë ˆì €"]
        cat = st.selectbox("ì¹´í…Œê³ ë¦¬", all_cats, key="stop_cat_sel")
        curr = ",".join(ss["STOP_BY_CAT"].get(cat, []))
        new  = st.text_area("í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ê¸ˆì¹™ì–´ (ì½¤ë§ˆ)", value=curr, height=120, key=f"stop_cat_txt_{cat}")
        c1, c2 = st.columns([1,1])
        with c1:
            if st.button("ì €ì¥(ì¹´í…Œê³ ë¦¬)", key=f"stop_cat_save_{cat}"):
                ss["STOP_BY_CAT"][cat] = [t.strip() for t in new.split(",") if t.strip()]
                st.success(f"{cat} ì €ì¥ ì™„ë£Œ")
        with c2:
            ss["STOP_AGGR"] = st.toggle("ê³µê²©ì  ë¶€ë¶„ì¼ì¹˜ ì œê±°", value=bool(ss["STOP_AGGR"]), key="stop_aggr_ui")

    with tab_white:
        wt = st.text_area("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(í—ˆìš©, ì½¤ë§ˆ)", value=",".join(ss["STOP_WHITELIST"]), height=100, key="stop_white_txt")
        if st.button("ì €ì¥(í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸)", key="stop_white_save"):
            ss["STOP_WHITELIST"] = [t.strip() for t in wt.split(",") if t.strip()]
            st.success("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ")

    with tab_replace:
        rp = st.text_area("ì¹˜í™˜ ê·œì¹™ (í˜•ì‹: src=>dst, ì½¤ë§ˆ)", value=",".join(ss["STOP_REPLACE"]), height=100, key="stop_repl_txt")
        if st.button("ì €ì¥(ì¹˜í™˜)", key="stop_repl_save"):
            ss["STOP_REPLACE"] = [t.strip() for t in rp.split(",") if t.strip()]
            st.success("ì¹˜í™˜ ê·œì¹™ ì €ì¥ ì™„ë£Œ")

    with tab_io:
        payload = {
            "global": ss["STOP_GLOBAL"],
            "by_cat": ss["STOP_BY_CAT"],
            "whitelist": ss["STOP_WHITELIST"],
            "replace": ss["STOP_REPLACE"],
            "aggressive": bool(ss["STOP_AGGR"]),
        }
        st.download_button("ì„¤ì • ë‚´ë ¤ë°›ê¸°(JSON)",
                           data=json.dumps(payload, ensure_ascii=False, indent=2).encode("utf-8"),
                           file_name="stopwords_profile.json", mime="application/json", key="stop_dl")
        up = st.file_uploader("ì„¤ì • ê°€ì ¸ì˜¤ê¸°(JSON)", type=["json"], key="stop_ul")
        if up:
            try:
                obj = json.load(io.TextIOWrapper(up, encoding="utf-8"))
                ss["STOP_GLOBAL"]    = list(obj.get("global", ss["STOP_GLOBAL"]))
                ss["STOP_BY_CAT"]    = dict(obj.get("by_cat", ss["STOP_BY_CAT"]))
                ss["STOP_WHITELIST"] = list(obj.get("whitelist", ss["STOP_WHITELIST"]))
                ss["STOP_REPLACE"]   = list(obj.get("replace", ss["STOP_REPLACE"]))
                ss["STOP_AGGR"]      = bool(obj.get("aggressive", ss["STOP_AGGR"]))
                st.success("ì„¤ì • ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")
            except Exception as e:
                st.error(f"ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

# =========================
# 9) ìƒí’ˆëª… ìƒì„±ê¸° (ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ í’€ì˜µì…˜: ê¸ˆì¹™ì–´/ë¸Œëœë“œ ë³´í˜¸ + ê²€ìƒ‰ëŸ‰ ìë™í™•ì¥ + 30~50ìÂ·50ë°”ì´íŠ¸ + ë³µì‚¬/ì ìˆ˜ + ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°)
# =========================

import re, json, time
from pathlib import Path
import pandas as pd
import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸ˆì¹™ì–´/ë¸Œëœë“œ ë³´í˜¸ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€

# íŒ¨í„´ ê¸ˆì¹™ì–´: ì„±ì¸/ë¶ˆë²•/ì˜ì•½ì„±ë¶„/ì •ì¹˜ë¯¼ê° ë“±(ë¸Œëœë“œ ì—¬ë¶€ ë¬´ê´€)
PATTERN_STOPWORDS = [
    # ì„±ì¸/ìŒë€
    r"í¬ë¥´ë…¸", r"ì„±ì¸(ê²Œì„|ë¹„ë””ì˜¤)?", r"ì„¹ìŠ¤", r"ì„¹ë„êµ¬", r"ì½˜ë”", r"ì˜¤ë‚˜í™€",
    r"ì‚¬ì •ì§€ì—°", r"ì• ë„", r"ìŒë€", r"ìŒëª¨", r"ìŒë¶€", r"ì„±ê¸°", r"ì„±êµ", r"ìµœìŒ", r"í¥ë¶„ì ¤",
    r"ì•¼í•œ", r"(ìƒ‰|ì„¹)ìŠ¤|ì„¹ì“°|ì…ìŠ¤|ìŒ•ìŠ¤",
    # ë¶ˆë²•/ë²”ì£„/ë¬´ê¸°/ëª°ì¹´
    r"ë¶ˆë²•", r"ëª°ì¹´", r"ë„ì´¬", r"(ì´|ê¶Œì´|íˆ¬ì‹œê²½|ì¹¼|ìƒˆì´)", r"ë„ë‚œ",
    # ì˜ì•½/í–¥ì • ì˜ì‹¬êµ°
    r"(ì‹œë¶€íŠ¸ë¼ë¯¼|sibutramine)", r"(ì‹¤ë°ë‚˜í•„|sildenafil)", r"(íƒ€ë‹¤ë¼í•„|tadalafil)",
    r"(ë°”ë°ë‚˜í•„|vardenafil)", r"(ë°ë‚˜í•„|denafil)", r"(ìš”í˜ë¹ˆ|yohimbin?e?)",
    r"(ì—í˜ë“œë¦°|ephedrine)", r"(DMAA|DMBA|DNP)", r"(ë©œë¼í† ë‹Œ|melatonin)",
    r"(ë¹ˆí¬ì„¸í‹´|vinpocetine)",
    r"(í•˜ì´ë“œë¡ì‹œ|hydroxy)\w*denafil", r"(í”„ë¡œí­ì‹œ|propoxy)\w*denafil", r"dimethyl",
    # ì •ì¹˜/êµ­ê°€ ë¯¼ê°
    r"ë¶í•œ|ê³µí™”êµ­|ì¸ë¯¼ê³µí™”êµ­|DPRK|êµ­ê¸°",
    # ì•„ë™/ì„ì‚°ë¶€/ì‹ ìƒì•„
    r"ì•„ë™|ì„ì‚°ë¶€|ì‹ ìƒì•„",
    # ë…¸ê³¨ ë¹„ì†ì–´
    r"ë³´ì§€|ë¶ˆì•Œ|ê¼¬ì¶”|ì –íƒ±ì´|ì –ê¼­ì§€",
]

# ë¦¬í„°ëŸ´ ê¸ˆì¹™ì–´(ë¹„ë¸Œëœë“œ) ì‹œë“œ â€” ë„¤ê°€ ì¤€ ë°©ëŒ€í•œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëŒ€í‘œ ìœ„í—˜êµ°ë§Œ ë°œì·Œ
SEEDED_NONBRAND_LITERALS = [
    "ê°•ê°„","ì‚´ì¸","ë„ì´¬","ëª°ì¹´","êµ°ì‚¬","ì´","ê¶Œì´","ëˆ„ë“œ","ìŒê²½","í•­ë¬¸","ì„±êµ","ì„±ìƒí™œ",
    "ì„±ê°ëŒ€","ì„±ê¸°ëŠ¥ë¶€ì „ê°œì„ ","ì‚¬ì •ì§€ì—°","ì• ë„","ì„¹ë„êµ¬","ì„¹ìŠ¤","í¬ë¥´ë…¸","í¬ë¥´ë…¸ê±¸",
    "ìš”í˜ë¹ˆ","ìš”í˜ë² ","ì‹œë¶€íŠ¸ë¼ë¯¼","ì‹¤ë°ë‚˜í•„","íƒ€ë‹¤ë¼í•„","ë°”ë°ë‚˜í•„","ë°ë‚˜í•„","í•˜ì´ë“œë¡ì‹œí˜¸ëª¨ì‹¤ë°ë‚˜í•„",
    "í•˜ì´ë“œë¡ì‹œí™ë°ë‚˜í•„","í•˜ì´ë“œë¡ì‹œë°”ë°ë‚˜í•„","ë””ë©”í‹¸ì¹˜ì˜¤ì‹¤ë°ë‚˜í•„","ë””ë©”ì¹ ì‹¤ë°ë‚˜í•„",
    "ë””ì¹˜ì˜¤í”„ë¡œí•„ì¹´ë³´ë°ë‚˜í•„","ë””ë©”í‹¸ì‹œë¶€íŠ¸ë¼ë¯¼","ë””ë°ìŠ¤ë©”í‹¸ì‹œë¶€íŠ¸ë¼ë¯¼",
    "ë©œë¼í† ë‹Œ","ë¹ˆí¬ì„¸í‹´","ì—í˜ë“œë¦°","DMAA","DMBA","DNP","ìˆ˜ë©´ì œ","íˆë¡œë½•",
    "ì•„ë™","ì„ì‚°ë¶€","ì‹ ìƒì•„","ë¶í•œ","ê³µí™”êµ­","ì¸ë¯¼ê³µí™”êµ­",
    "ë³´ì§€","ë¶ˆì•Œ","ê¼¬ì¶”","ì –íƒ±ì´","ì –ê¼­ì§€","ìŒë€","ìŒëª¨",
]

# ì—¬ê¸°ì— â€œì¶”ê°€ ê¸ˆì¹™ì–´(ë„¤ê°€ ì „ë‹¬í•œ í’€ ë¦¬ìŠ¤íŠ¸)â€ë¥¼ ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ì–´ë„ ë¨.
# ì½”ë“œëŠ” ìë™ìœ¼ë¡œ â€˜ë¸Œëœë“œì„±â€™ ë‹¨ì–´ë¥¼ ì œì™¸í•˜ê³  â€˜ë¹„ë¸Œëœë“œâ€™ë§Œ ì ìš©í•œë‹¤.
USER_BLOB_EXTRA = r"""
# â¬‡ ì—¬ê¸°ì— ë„¤ê°€ ì¤€ ì¶”ê°€ ê¸ˆì¹™ì–´ ë¶™ì´ê¸°(ì¤„ë°”ê¿ˆ ë‹¨ìœ„)
""".strip()

# ë¸Œëœë“œ íœ´ë¦¬ìŠ¤í‹±
_BRAND_ASCII_RE = re.compile(r"^[A-Za-z0-9][A-Za-z0-9\-\& ]{1,24}$")
_BRAND_KO_SUFFIX = (
    "ìŠ¤","ì¦ˆ","ì½”","ë§ˆ","ë‹ˆ","ë¡œë Œ","ì½”ì–´ìŠ¤","ë¼ì½”ìŠ¤í…Œ","ë¡œì—ë² ","ë¡œì— ","ë¥´ë©”ë¥´","ë¡œë§¨í‹±í¬ë¼ìš´",
    "ì¹´ë¯¼ìŠ¤í‚¤","í”„ë ˆìŠ¤í† ","í”„ë¦¬ë¯¸ì—„","ìŠ¤í†¤","ì•„ì¼ëœë“œ","ë‚˜ì´í‚¤","ì•„ë””ë‹¤ìŠ¤","ë‰´ë°œë€ìŠ¤",
    "ìƒ¤ë„¬","ë£¨ì´ë¹„í†µ","êµ¬ì°Œ","í”„ë¼ë‹¤","ë””ì˜¬","ëª½í´ë ˆì–´","ìŠ¤íƒ€ë²…ìŠ¤","ë¼ì¸í”„ë Œì¦ˆ","í—¬ë¡œí‚¤í‹°","í¬ì¼“ëª¬",
)
HARD_NONBRAND = {
    "í¬ë¥´ë…¸","ì„¹ìŠ¤","ì„¹ë„êµ¬","ì˜¤ë‚˜í™€","ì‚¬ì •ì§€ì—°","ì• ë„","ìŒë€","ìŒëª¨","ìŒë¶€","ì„±ê¸°","ì„±êµ","ìµœìŒ",
    "íˆë¡œë½•","ëŒ€ë§ˆ","ìˆ˜ë©´ì œ","ì‹œë¶€íŠ¸ë¼ë¯¼","ì‹¤ë°ë‚˜í•„","íƒ€ë‹¤ë¼í•„","ë°”ë°ë‚˜í•„","ë°ë‚˜í•„","ëª°ì¹´","ë„ì´¬","ì´","ê¶Œì´",
    "ë¶í•œ","ê³µí™”êµ­","ì¸ë¯¼ê³µí™”êµ­","ê°•ê°„","ì‚´ì¸","ì•„ë™","ì„ì‚°ë¶€","ì‹ ìƒì•„",
}

def _is_brandish(term: str) -> bool:
    t = (term or "").strip()
    if not t: return False
    if _BRAND_ASCII_RE.match(t): return True
    if any(t.endswith(suf) for suf in _BRAND_KO_SUFFIX): return True
    return False

def _extract_nonbrand_from_blob(blob: str) -> list[str]:
    raw = [x.strip() for x in (blob or "").splitlines()]
    raw = [x for x in raw if x and not x.startswith("#")]
    uniq = list(dict.fromkeys(raw))
    nonbrands=[]
    for w in uniq:
        if w in HARD_NONBRAND: nonbrands.append(w); continue
        if not _is_brandish(w): nonbrands.append(w)
    return nonbrands

PATTERN_RE = re.compile("|".join(PATTERN_STOPWORDS), re.IGNORECASE)

_MIN_PART = 2
def _compile_literals(words: list[str]) -> re.Pattern:
    pats=[]
    for w in words:
        w=w.strip()
        if not w: continue
        if len(w) < _MIN_PART: continue
        pats.append(re.escape(w))
    if not pats:
        return re.compile(r"$^\b$")
    return re.compile("|".join(pats), re.IGNORECASE)

USER_EXTRA_NONBRAND = _extract_nonbrand_from_blob(USER_BLOB_EXTRA)
LITERAL_RE = _compile_literals(sorted(set(SEEDED_NONBRAND_LITERALS + USER_EXTRA_NONBRAND)))

def _apply_stopwords_nonbrand(text: str, brand_allow: set[str] | None = None) -> str:
    brand_allow = {*(brand_allow or set())}
    marker_l, marker_r = "Â«", "Â»"
    protected_map={}
    def _protect(m):
        tok = m.group(0)
        key = f"{marker_l}{len(protected_map)}{marker_r}"
        protected_map[key]=tok
        return key
    out = text
    if brand_allow:
        for b in sorted(brand_allow, key=len, reverse=True):
            if not b: continue
            out = re.sub(rf"(?i)\b{re.escape(b)}\b", _protect, out)
    out = PATTERN_RE.sub(" ", out)
    out = LITERAL_RE.sub(" ", out)
    out = re.sub(r"\s+", " ", out).strip()
    for key, val in protected_map.items():
        out = out.replace(key, val)
    return out

def _dedupe_double_brands(title: str) -> str:
    tokens = title.split()
    seen=set(); out=[]
    for t in tokens:
        low = t.lower()
        if low in seen: continue
        seen.add(low); out.append(t)
    return " ".join(out)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸¸ì´/ë°”ì´íŠ¸ ë³´ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _truncate_by_bytes(text: str, max_bytes: int = 50) -> str:
    raw = text.encode("utf-8")
    if len(raw) <= max_bytes: return text
    cut = raw[:max_bytes]
    while True:
        try:
            s = cut.decode("utf-8"); break
        except UnicodeDecodeError:
            cut = cut[:-1]
            if not cut: return ""
    m = re.match(r"^(.{1,})[\s\|\Â·\-]", s[::-1])
    if m:
        s2 = m.group(1)[::-1].rstrip()
        return s2 + "â€¦"
    return s.rstrip() + "â€¦"

def _smart_truncate(text: str, max_len: int, min_len: int) -> str:
    if len(text) > max_len:
        return text[: max_len - 1] + "â€¦"
    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²€ìƒ‰ëŸ‰ ê¸°ë°˜ ìë™ í™•ì¥(ë„¤ì´ë²„ í‚¤ì›Œë“œë„êµ¬ ì‘ë‹µ ì¬í™œìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â€» ì´ ì•± ìƒë‹¨ì— ì´ë¯¸ ì •ì˜ëœ _naver_keywordstool(df ë°˜í™˜)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

def _expand_title_with_searchvol(base_tokens: list[str], df_kstats: pd.DataFrame,
                                 target_min_chars: int, max_chars: int) -> list[str]:
    """
    base_tokens: í˜„ì¬ ì œëª© í† í°(ê³µë°± ê²°í•© ì˜ˆì •)
    df_kstats: _naver_keywordstool ê²°ê³¼ (í‚¤ì›Œë“œ/ê²€ìƒ‰í•©ê³„ ë“± í¬í•¨)
    ê¸¸ì´ê°€ target_min_chars ë¯¸ë‹¬ì´ë©´, df_kstatsì—ì„œ 'ê²€ìƒ‰í•©ê³„' ìƒìœ„ í‚¤ì›Œë“œë¥¼
    ì¤‘ë³µ/ê¸ˆì¹™ì–´/ì´ë¯¸í¬í•¨ ì œì™¸í•˜ê³  ìˆœì„œëŒ€ë¡œ ì¶”ê°€í•´ ìµœì†Œ ê¸¸ì´ì— ë„ë‹¬í•˜ë„ë¡ í™•ì¥.
    """
    exist_set = {t.strip().lower() for t in base_tokens if t.strip()}
    # ì •ë ¬: ê²€ìƒ‰í•©ê³„ ë‚´ë¦¼ì°¨ìˆœ
    cand = []
    if not df_kstats.empty and "í‚¤ì›Œë“œ" in df_kstats.columns:
        df2 = df_kstats.copy()
        if "ê²€ìƒ‰í•©ê³„" not in df2.columns:
            df2["ê²€ìƒ‰í•©ê³„"] = pd.to_numeric(df2.get("PCì›”ê°„ê²€ìƒ‰ìˆ˜",0), errors="coerce").fillna(0) + \
                              pd.to_numeric(df2.get("Mobileì›”ê°„ê²€ìƒ‰ìˆ˜",0), errors="coerce").fillna(0)
        df2 = df2.sort_values("ê²€ìƒ‰í•©ê³„", ascending=False)
        cand = [x.strip() for x in df2["í‚¤ì›Œë“œ"].tolist() if x and len(x.strip())>=2]

    out = base_tokens[:]
    for kw in cand:
        low = kw.lower()
        if low in exist_set:  # ì´ë¯¸ í¬í•¨
            continue
        # ê¸ˆì¹™ì–´ ì œê±°(ë¸Œëœë“œ í—ˆìš©ì€ ì—¬ê¸°ì„  ê³ ë ¤ ì•ˆ í•¨ â€” í™•ì¥ í‚¤ì›Œë“œë¼ì„œ)
        test = " ".join(out + [kw])
        test2 = _apply_stopwords_nonbrand(test, brand_allow=set())
        if test2 != test:
            continue  # ê¸ˆì¹™ì–´ì— ê±¸ë¦¼
        # ê¸¸ì´ ê²€ì‚¬(ë¬¸ì ê¸°ì¤€ ë¨¼ì €)
        test_join = " ".join(out + [kw])
        if len(test_join) > max_chars:
            continue
        out.append(kw); exist_set.add(low)
        if len(" ".join(out)) >= target_min_chars:
            break
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ SEO ì ìˆ˜(ê°„ë‹¨ ì§€ìˆ˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _seo_score(title: str, df_kstats: pd.DataFrame, target_min: int = 30, max_bytes: int = 50) -> dict:
    """
    0~100 ê°€ì¤‘ ì ìˆ˜:
      - ê¸¸ì´(ë¬¸ì/ë°”ì´íŠ¸) ì í•© 35
      - ìƒìœ„ í‚¤ì›Œë“œ í¬í•¨ë„ 45 (ìƒìœ„ 10ê°œ ì¤‘ í¬í•¨ ë¹„ìœ¨)
      - ê¸ˆì¹™ì–´/ì§€ì €ë¶„í•œ í† í° ê°ì  20
    """
    score = 0
    reasons = []

    # 1) ê¸¸ì´ ì í•©
    char_len = len(title)
    byte_len = len(title.encode("utf-8"))
    if 30 <= char_len <= 50 and byte_len <= max_bytes:
        score += 35
        reasons.append("ê¸¸ì´ ì í•©(+35)")
    else:
        # ê±°ë¦¬ ê¸°ë°˜ ì™„í™”
        penalty = min(abs(char_len-40), 20)  # ì¤‘ì‹¬ 40ì ê°€ì •
        gain = max(0, 35 - penalty)
        score += gain
        reasons.append(f"ê¸¸ì´ ë³´ì •(+{gain})")

    # 2) ìƒìœ„ í‚¤ì›Œë“œ í¬í•¨ë„
    cover_gain = 0
    if not df_kstats.empty and "í‚¤ì›Œë“œ" in df_kstats.columns:
        df2 = df_kstats.copy()
        if "ê²€ìƒ‰í•©ê³„" not in df2.columns:
            df2["ê²€ìƒ‰í•©ê³„"] = pd.to_numeric(df2.get("PCì›”ê°„ê²€ìƒ‰ìˆ˜",0), errors="coerce").fillna(0) + \
                              pd.to_numeric(df2.get("Mobileì›”ê°„ê²€ìƒ‰ìˆ˜",0), errors="coerce").fillna(0)
        top = df2.sort_values("ê²€ìƒ‰í•©ê³„", ascending=False).head(10)["í‚¤ì›Œë“œ"].tolist()
        hit = sum(1 for k in top if re.search(rf"(?i)\b{re.escape(k)}\b", title))
        ratio = hit / max(len(top),1)
        cover_gain = int(round(45 * ratio))
        score += cover_gain
        reasons.append(f"ìƒìœ„í‚¤ì›Œë“œ í¬í•¨ {hit}/{len(top)}(+{cover_gain})")
    else:
        reasons.append("ê²€ìƒ‰ì§€í‘œ ë¯¸ë°˜ì˜(+0)")

    # 3) ê¸ˆì¹™ì–´/ì§€ì €ë¶„ í† í° ê°ì 
    dirty = PATTERN_RE.search(title) or LITERAL_RE.search(title)
    if dirty:
        score -= 20
        reasons.append("ê¸ˆì¹™ì–´ ê°ì (-20)")

    score = max(0, min(100, score))
    return {"score": score, "reasons": reasons, "chars": char_len, "bytes": byte_len}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ AI ì†ì„± ì œì•ˆ(ë£° ê¸°ë°˜ ë¼ì´íŠ¸ë²„ì „) â”€â”€â”€â”€â”€â”€â”€â”€â”€
_ATTR_HINTS = {
    "ê°€ë°©|ë°±|ë°±íŒ©|í† íŠ¸": ["ì •í’ˆ", "ê²½ëŸ‰", "ìˆ˜ë‚©", "ë°©ìˆ˜"],
    "ë…¸íŠ¸ë¶|ë§¥ë¶|ë©íƒ‘|ê±°ì¹˜ëŒ€|ìŠ¤íƒ ë“œ": ["ì•Œë£¨ë¯¸ëŠ„", "ë†’ì´ì¡°ì ˆ", "ë¯¸ë„ëŸ¼ë°©ì§€", "íœ´ëŒ€ìš©"],
    "ìš´ë™í™”|ëŸ°ë‹í™”|ìŠ¤ë‹ˆì»¤ì¦ˆ": ["ê²½ëŸ‰", "í†µê¸°ì„±", "ì¿ ì…”ë‹"],
    "ì˜ì|ì²´ì–´": ["ì¸ì²´ê³µí•™", "í—ˆë¦¬ì§€ì§€", "ë°©ì„ í¬í•¨"],
    "ì„ í’ê¸°|íŒ¬|ì„œí˜ë ˆì´í„°": ["ì €ì†ŒìŒ", "BLDC", "ë¬´ì„ ", "1+1"],
    "ì´ì–´í°|í—¤ë“œí°|ì—ì–´íŒŸ|ë²„ì¦ˆ": ["ë…¸ì´ì¦ˆìº”ìŠ¬ë§", "ë¬´ì„ ì¶©ì „", "ê¸´ë°°í„°ë¦¬"],
}
def _suggest_attrs(text: str) -> list[str]:
    out=[]
    for pat, hints in _ATTR_HINTS.items():
        if re.search(pat, text):
            out.extend(hints)
    # ì¤‘ë³µ ì œê±°
    s=set(); r=[]
    for h in out:
        if h not in s:
            s.add(h); r.append(h)
    return r[:6]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° ìŠ¤í† ë¦¬ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€

_STORE_PATH = Path(__file__).parent / "titles_store.json"

def _read_store() -> dict:
    if _STORE_PATH.exists():
        try:
            return json.loads(_STORE_PATH.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def _write_store(obj: dict):
    tmp = _STORE_PATH.with_suffix(".tmp")
    tmp.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
    tmp.replace(_STORE_PATH)

def _ensure_store():
    st.session_state.setdefault("TITLE_COLLECTIONS", _read_store())

def _save_collection(name: str, titles: list[str], meta: dict | None = None):
    name = name.strip()
    if not name or not titles:
        st.warning("ì´ë¦„ê³¼ ì œëª© ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    coll = st.session_state["TITLE_COLLECTIONS"]
    coll[name] = {
        "titles": titles,
        "meta": meta or {},
        "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
    }
    _write_store(coll)
    st.success(f"ì €ì¥ ì™„ë£Œ: {name} ({len(titles)}ê±´)")

def _merge_titles(a: list[str], b: list[str]) -> list[str]:
    seen=set(); out=[]
    for t in a + b:
        key = t.strip().lower()
        if not key or key in seen: continue
        seen.add(key); out.append(t.strip())
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€

def section_title_generator():
    st.markdown('<div class="card"><div class="card-title">ìƒí’ˆëª… ìƒì„±ê¸° (ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ â€¢ í’€ì˜µì…˜)</div>', unsafe_allow_html=True)
    with st.container():
        cA, cB = st.columns([1, 2])
        with cA:
            brand = st.text_input("ë¸Œëœë“œ", placeholder="ì˜ˆ: Apple / ìƒ¤ì˜¤ë¯¸ / ë¬´ì§€")
            attrs = st.text_input("ì†ì„±(ì½¤ë§ˆ, ì„ íƒ)", placeholder="ì˜ˆ: ê³µì‹, ì •í’ˆ, í•œì •íŒ")
        with cB:
            kws = st.text_input("í‚¤ì›Œë“œ(ì½¤ë§ˆ)", placeholder="ì˜ˆ: ë…¸íŠ¸ë¶ ìŠ¤íƒ ë“œ, ì ‘ì´ì‹, ì•Œë£¨ë¯¸ëŠ„")

        a, b, c = st.columns([1, 1, 1])
        with a:
            max_len = st.slider("ìµœëŒ€ ê¸€ììˆ˜(ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê·œì¹™)", 30, 50, 50, 1, key="seo_maxlen")
        with b:
            target_min = st.slider("ìµœì†Œ ê¸€ììˆ˜(ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê·œì¹™)", 30, 50, 30, 1, key="seo_minlen")
        with c:
            order = st.selectbox("ìˆœì„œ", ["ë¸Œëœë“œ-í‚¤ì›Œë“œ-ì†ì„±", "í‚¤ì›Œë“œ-ë¸Œëœë“œ-ì†ì„±", "ë¸Œëœë“œ-ì†ì„±-í‚¤ì›Œë“œ"], index=0)

        # ë¼ì´íŠ¸í•œ AI ì†ì„± ì œì•ˆ
        if st.button("AI ì†ì„± ì œì•ˆ ë°›ê¸°", use_container_width=False):
            base_text = f"{brand} {' '.join([x.strip() for x in (kws or '').split(',') if x.strip()])}"
            hints = _suggest_attrs(base_text)
            if hints:
                st.info("ì¶”ì²œ ì†ì„±: " + ", ".join(hints))
            else:
                st.info("ì¶”ì²œ ì†ì„±ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ ë” êµ¬ì²´í™”í•´ ë³´ì„¸ìš”.")

        if st.button("ìƒí’ˆëª… ìƒì„±"):
            kw_list = [k.strip() for k in (kws or "").split(",") if k.strip()]
            at_list = [a.strip() for a in (attrs or "").split(",") if a.strip()]
            titles = []
            seo_rows = []  # ì ìˆ˜ í…Œì´ë¸”

            # ê²€ìƒ‰ëŸ‰ ë°ì´í„°(ìë™ í™•ì¥ ë° ì ìˆ˜ì— ì¬ì‚¬ìš©)
            # ì´ë¯¸ ìƒë‹¨ì— ì •ì˜ëœ ë„¤ì´ë²„ í‚¤ì›Œë“œë„êµ¬ í•¨ìˆ˜ ì‚¬ìš©: _naver_keywordstool
            df_stats = _naver_keywordstool(kw_list) if ' _naver_keywordstool' or '_naver_keywordstool' in globals() else pd.DataFrame()
            if not df_stats.empty:
                df_stats["ê²€ìƒ‰í•©ê³„"] = pd.to_numeric(df_stats.get("PCì›”ê°„ê²€ìƒ‰ìˆ˜",0), errors="coerce").fillna(0) + \
                                       pd.to_numeric(df_stats.get("Mobileì›”ê°„ê²€ìƒ‰ìˆ˜",0), errors="coerce").fillna(0)

            for k in (kw_list or [""]):
                # 1) ê¸°ë³¸ ì‹œí€€ìŠ¤
                if order == "ë¸Œëœë“œ-í‚¤ì›Œë“œ-ì†ì„±":
                    seq = [brand, k] + at_list
                elif order == "í‚¤ì›Œë“œ-ë¸Œëœë“œ-ì†ì„±":
                    seq = [k, brand] + at_list
                else:
                    seq = [brand] + at_list + [k]

                # 2) ê³µë°± ê³ ì •
                tokens = [p for p in seq if p]
                base_title = " ".join(tokens)

                # 3) ê²€ìƒ‰ëŸ‰ ê¸°ë°˜ ìë™ í™•ì¥
                if not df_stats.empty and target_min > len(base_title):
                    tokens = _expand_title_with_searchvol(tokens, df_stats, target_min_chars=target_min, max_chars=max_len)

                # 4) ê¸ˆì¹™ì–´ í•„í„°(ë¸Œëœë“œ ë³´í˜¸) + ì¤‘ë³µ ë¸Œëœë“œ ì •ë¦¬
                brand_allow = {brand.strip()} | {kk for kk in kw_list if _is_brandish(kk)}
                final = " ".join(tokens)
                final = _apply_stopwords_nonbrand(final, brand_allow=brand_allow)
                final = _dedupe_double_brands(final)

                # 5) ê¸¸ì´ ë³´ì •(ë¬¸ìâ†’ë°”ì´íŠ¸)
                final = _smart_truncate(final, max_len, target_min)
                if len(final.encode("utf-8")) > 50:
                    final = _truncate_by_bytes(final, 50)

                titles.append(final)

                # 6) SEO ì ìˆ˜ ì‚°ì¶œ
                sc = _seo_score(final, df_stats if not df_stats.empty else pd.DataFrame())
                seo_rows.append({"title": final, "SEOì ìˆ˜": sc["score"], "ì‚¬ìœ ": " / ".join(sc["reasons"]),
                                 "ë¬¸ììˆ˜": sc["chars"], "ë°”ì´íŠ¸": sc["bytes"]})

            if titles:
                st.success(f"ìƒì„± ì™„ë£Œ Â· {len(titles)}ê±´")

                # ê²°ê³¼ í‘œ + ë³µì‚¬ ë²„íŠ¼
                for i, t in enumerate(titles, 1):
                    char_len = len(t); byte_len = len(t.encode("utf-8"))
                    warn = []
                    if char_len < 30: warn.append("30ì ë¯¸ë§Œ")
                    if byte_len > 50: warn.append("50ë°”ì´íŠ¸ ì´ˆê³¼")
                    badge = "" if not warn else " â€” " + " / ".join([f":red[{w}]" for w in warn])
                    st.markdown(f"**{i}.** {t}  <span style='opacity:.7'>(ë¬¸ì {char_len}/50 Â· ë°”ì´íŠ¸ {byte_len}/50)</span>{badge}",
                                unsafe_allow_html=True)
                    # ë³µì‚¬ ë²„íŠ¼(ë¸Œë¼ìš°ì € ë³µì‚¬ ì•„ì´ì½˜ ì œê³µ)
                    st.code(t, language=None)

                # SEO ë¦¬í¬íŠ¸ í‘œ
                st.markdown("**SEO ë¦¬í¬íŠ¸**")
                df_seo = pd.DataFrame(seo_rows).sort_values("SEOì ìˆ˜", ascending=False)
                st.dataframe(df_seo, use_container_width=True, height=260)

                # CSV ë‹¤ìš´ë¡œë“œ
                out_df = pd.DataFrame({"title": titles})
                st.download_button(
                    "CSV ë‹¤ìš´ë¡œë“œ",
                    data=out_df.to_csv(index=False).encode("utf-8-sig"),
                    file_name="titles_smartstore.csv",
                    mime="text/csv",
                )

                # ìš”ì•½
                lens = [len(t) for t in titles]
                blens = [len(t.encode("utf-8")) for t in titles]
                st.caption(
                    f"ìš”ì•½ Â· ë¬¸ì(ìµœì†Œ/í‰ê· /ìµœëŒ€): {min(lens)}/{sum(lens)//len(lens)}/{max(lens)} Â· "
                    f"ë°”ì´íŠ¸(ìµœì†Œ/í‰ê· /ìµœëŒ€): {min(blens)}/{sum(blens)//len(blens)}/{max(blens)}"
                )
            else:
                st.warning("ìƒì„±ëœ ìƒí’ˆëª…ì´ ì—†ìŠµë‹ˆë‹¤. (ì…ë ¥ê°’/ê¸ˆì¹™ì–´ë¡œ ëª¨ë‘ ê±¸ëŸ¬ì¡Œì„ ìˆ˜ ìˆìŒ)")

        # â”€â”€ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° UI â”€â”€
        _ensure_store()
        with st.expander("ğŸ’¾ ìƒì„± ê²°ê³¼ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°", expanded=False):
            tab_save, tab_load, tab_io = st.tabs(["ì €ì¥", "ë¶ˆëŸ¬ì˜¤ê¸°/ê´€ë¦¬", "ê°€ì ¸ì˜¤ê¸°/ë‚´ë³´ë‚´ê¸°"])

            with tab_save:
                save_name = st.text_input("ì €ì¥í•  ì´ë¦„", placeholder="ì˜ˆ: ë§¥ë¶ê±°ì¹˜ëŒ€_240922_1")
                if st.button("ì´ ì œëª©ë“¤ ì»¬ë ‰ì…˜ìœ¼ë¡œ ì €ì¥", use_container_width=False):
                    if 'titles' in locals() and titles:
                        meta = {
                            "brand": brand,
                            "attrs": at_list if 'at_list' in locals() else [],
                            "kws": kw_list if 'kw_list' in locals() else [],
                            "max_len": max_len,
                            "min_len": target_min,
                            "order": order,
                        }
                        _save_collection(save_name, titles, meta)
                    else:
                        st.warning("ë¨¼ì € ìƒí’ˆëª…ì„ ìƒì„±í•˜ì„¸ìš”.")

            with tab_load:
                coll = st.session_state["TITLE_COLLECTIONS"]
                names = sorted(coll.keys())
                sel = st.selectbox("ì €ì¥ëœ ì»¬ë ‰ì…˜", names, index=0 if names else None)
                if names:
                    info = coll[sel]
                    st.caption(f"{sel} Â· {info.get('ts','')} Â· {len(info.get('titles',[]))}ê±´")
                    st.dataframe(pd.DataFrame({"title": info.get("titles", [])}),
                                 use_container_width=True, height=240)
                    c1, c2, c3 = st.columns(3)
                    with c1:
                        if st.button("í˜„ì¬ ìƒì„±ê²°ê³¼ì™€ ë³‘í•©(ì¤‘ë³µì œê±°)", use_container_width=True):
                            if 'titles' in locals() and titles:
                                merged = _merge_titles(titles, info["titles"])
                                st.session_state["__merged_titles__"] = merged
                                st.success(f"ë³‘í•© ì™„ë£Œ: {len(merged)}ê±´ (ì„¸ì…˜ì— ì„ì‹œ ì €ì¥)")
                            else:
                                st.warning("ë¨¼ì € ìƒí’ˆëª…ì„ ìƒì„±í•˜ì„¸ìš”.")
                    with c2:
                        if st.button("í˜„ì¬ ì„¸ì…˜ ê²°ê³¼ë¥¼ ì´ ì»¬ë ‰ì…˜ìœ¼ë¡œ êµì²´", use_container_width=True):
                            st.session_state["__merged_titles__"] = info["titles"]
                            st.success("ì„¸ì…˜ì— êµì²´ ì €ì¥ ì™„ë£Œ")
                    with c3:
                        if st.button("ì´ ì»¬ë ‰ì…˜ ì‚­ì œ", use_container_width=True):
                            del coll[sel]
                            _write_store(coll)
                            st.experimental_rerun()

                    if "__merged_titles__" in st.session_state:
                        st.divider()
                        st.caption("ì„ì‹œ ë³‘í•© ê²°ê³¼(ì„¸ì…˜)")
                        mt = st.session_state["__merged_titles__"]
                        st.dataframe(pd.DataFrame({"title": mt}), use_container_width=True, height=200)
                        st.download_button("ì„ì‹œ ë³‘í•© ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                                           data=pd.DataFrame({"title": mt}).to_csv(index=False).encode("utf-8-sig"),
                                           file_name="titles_merged.csv", mime="text/csv")

            with tab_io:
                c1, c2 = st.columns(2)
                with c1:
                    st.write("ğŸ“¤ ë‚´ë³´ë‚´ê¸°")
                    coll = st.session_state["TITLE_COLLECTIONS"]
                    if coll:
                        st.download_button("ì „ì²´ ì»¬ë ‰ì…˜( JSON ) ë‹¤ìš´ë¡œë“œ",
                                           data=json.dumps(coll, ensure_ascii=False, indent=2).encode("utf-8"),
                                           file_name="titles_store.json", mime="application/json")
                    else:
                        st.caption("ì €ì¥ëœ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
                with c2:
                    st.write("ğŸ“¥ ê°€ì ¸ì˜¤ê¸°")
                    up = st.file_uploader("JSON/CSV ì—…ë¡œë“œ(ì»¬ë ‰ì…˜ ì¶”ê°€/ë®ì–´ì“°ê¸°)", type=["json","csv"])
                    mode = st.radio("ê°€ì ¸ì˜¤ê¸° ëª¨ë“œ", ["ì¶”ê°€", "ë®ì–´ì“°ê¸°"], horizontal=True, index=0)
                    if up is not None:
                        try:
                            if up.type == "application/json" or up.name.lower().endswith(".json"):
                                data = json.loads(up.read().decode("utf-8"))
                                if not isinstance(data, dict):
                                    raise ValueError("JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                                if mode == "ë®ì–´ì“°ê¸°":
                                    st.session_state["TITLE_COLLECTIONS"] = data
                                else:
                                    merged = _read_store()
                                    merged.update(data)
                                    st.session_state["TITLE_COLLECTIONS"] = merged
                                _write_store(st.session_state["TITLE_COLLECTIONS"])
                                st.success("JSON ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")
                            else:
                                df = pd.read_csv(up)
                                if "title" not in df.columns:
                                    raise ValueError("CSVì— 'title' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                                name = f"import_{time.strftime('%Y%m%d_%H%M%S')}"
                                _save_collection(name, df["title"].astype(str).tolist(), meta={"import":"csv"})
                                st.success(f"CSV ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {name}")
                        except Exception as e:
                            st.error(f"ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    st.markdown("</div>", unsafe_allow_html=True)

# =========================
# 10) ê¸°íƒ€ ì¹´ë“œ
# =========================
def _11st_abest_url():
    return ("https://m.11st.co.kr/page/main/abest?tabId=ABEST&pageId=AMOBEST&ctgr1No=166160&_ts=%d" % int(time.time()))
def section_11st():
    st.markdown('<div class="card"><div class="card-title">11ë²ˆê°€ (ëª¨ë°”ì¼) â€” ì•„ë§ˆì¡´ ë² ìŠ¤íŠ¸</div>', unsafe_allow_html=True)
    _proxy_iframe(ELEVENST_PROXY, _11st_abest_url(), height=900, scroll=True, key="abest")
    st.markdown('</div>', unsafe_allow_html=True)
def section_itemscout_placeholder():
    st.markdown('<div class="card"><div class="card-title">ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸</div>', unsafe_allow_html=True)
    st.info("ì„ë² ë“œ ë³´ë¥˜ ì¤‘ì…ë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ì›ë³¸ í˜ì´ì§€ë¥¼ ìƒˆ íƒ­ì—ì„œ ì—¬ì„¸ìš”.")
    st.link_button("ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ì§ì ‘ ì—´ê¸°(ìƒˆ íƒ­)", "https://app.itemscout.io/market/keyword")
    st.markdown('</div>', unsafe_allow_html=True)
def section_sellerlife_placeholder():
    st.markdown('<div class="card"><div class="card-title">ì…€ëŸ¬ë¼ì´í”„</div>', unsafe_allow_html=True)
    st.info("ì„ë² ë“œ ë³´ë¥˜ ì¤‘ì…ë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ì›ë³¸ í˜ì´ì§€ë¥¼ ìƒˆ íƒ­ì—ì„œ ì—¬ì„¸ìš”.")
    st.link_button("ì§ì ‘ ì—´ê¸°(ìƒˆ íƒ­)", "https://sellochomes.co.kr/sellerlife/")
    st.markdown('</div>', unsafe_allow_html=True)

# =========================
# ì™¸ë¶€ Stopwords ì„¹ì…˜(ì„ íƒ) â€” ìœ ì§€í•´ë„ ë˜ê³ , ë ˆì´ì•„ì›ƒì—ì„œ í˜¸ì¶œ ì œê±°í•´ë„ ë¨
# =========================
def section_stopwords_manager():
    st.markdown('<div class="card"><div class="card-title">ê¸ˆì¹™ì–´ ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬ì (í˜„ì—…ìš©)</div>', unsafe_allow_html=True)
    _stopwords_manager_ui(compact=False)

# =========================
# 11) Layout â€” row1: Radar | (ì¹´í…Œê³ ë¦¬ or ì§ì ‘ ì…ë ¥) | ìƒí’ˆëª… ìƒì„±ê¸°
# =========================
_ = _sidebar()
_responsive_probe()
vwbin = _get_view_bin()

st.title("ENVY â€” Season 1 (Dual Proxy Edition)")

# 1í–‰
row1_a, row1_b, row1_c = st.columns([8, 5, 3], gap="medium")
with row1_a:
    section_radar()
with row1_b:
    tab_cat, tab_direct = st.tabs(["ì¹´í…Œê³ ë¦¬", "ì§ì ‘ ì…ë ¥"])
    with tab_cat:
        section_category_keyword_lab()
    with tab_direct:
        section_keyword_trend_widget()
with row1_c:
    section_title_generator()

st.markdown('<div class="row-gap"></div>', unsafe_allow_html=True)

# 2í–‰
c1, c2, c3 = st.columns([3, 3, 3], gap="medium")
with c1:
    section_11st()
with c2:
    section_itemscout_placeholder()
with c3:
    section_sellerlife_placeholder()

st.markdown('<div class="row-gap"></div>', unsafe_allow_html=True)
