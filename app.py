# ENVY v27.6 Full â€¢ Rakuten ê³µì‹ API ë‚´ì¥(AppID í•˜ë“œì½”ë”©)
# - ì‚¬ì´ë“œë°”: ì ˆëŒ€ ê³ ì • (í™˜ìœ¨/ë§ˆì§„ í†µí™” ë¶„ë¦¬)
# - ë³¸ë¬¸: ë°ì´í„°ë© â†’ ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ â†’ ì…€ëŸ¬ë¼ì´í”„ â†’ AI ë ˆì´ë”(êµ­ë‚´=DataLab, ê¸€ë¡œë²Œ=Amazon+Rakuten API) â†’ 11ë²ˆê°€ â†’ ìƒí’ˆëª… ìƒì„±ê¸°

import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import requests, datetime, random, textwrap, html, urllib.parse, re
from bs4 import BeautifulSoup

st.set_page_config(page_title="ENVY v27.6 Full", page_icon="ğŸš€", layout="wide")

# -------------------- Config --------------------
HF_API_KEY = "hf_xxxxxxxxxxxxxxxxxxxxxxxxx"   # ì„±ê³µ í›„ secretsë¡œ ì´ë™ ê¶Œì¥
RAKUTEN_APP_ID = "1043271015809337425"        # ğŸ‘‰ ì‚¬ìš©ì ìƒìš© AppID ì§ì ‘ ë°•ìŒ (ìš”ì²­ì— ë”°ë¼ í•˜ë“œì½”ë”©)
CURRENCY_SYMBOL = {"KRW":"â‚©","USD":"$","EUR":"â‚¬","JPY":"Â¥","CNY":"CNÂ¥"}
FX_ORDER = ["USD","EUR","JPY","CNY"]

# ë„¤ì´ë²„ ì‡¼í•‘ ì¹´í…Œê³ ë¦¬ CID ë§¤í•‘(í™”ë©´ ë¹„ë…¸ì¶œ)
NAVER_CATEGORIES = {
    "íŒ¨ì…˜ì˜ë¥˜": "50000000", "íŒ¨ì…˜ì¡í™”": "50000001", "í™”ì¥í’ˆ/ë¯¸ìš©": "50000002",
    "ë””ì§€í„¸/ê°€ì „": "50000003", "ê°€êµ¬/ì¸í…Œë¦¬ì–´": "50000004", "ì‹í’ˆ": "50000005",
    "ìƒí™œ/ê±´ê°•": "50000006", "ì¶œì‚°/ìœ¡ì•„": "50000007", "ìŠ¤í¬ì¸ /ë ˆì €": "50000008",
    "ë„ì„œ/ì·¨ë¯¸/ì• ì™„": "50000009"
}

def to_csv_bytes(df: pd.DataFrame) -> bytes:
    return df.to_csv(index=False).encode("utf-8-sig")

def copy_button(text: str, key: str):
    safe = html.escape(text).replace("\n","\\n").replace("'","\\'")
    st.components.v1.html(
        f"<div style='display:flex;gap:8px;align-items:center;margin:6px 0;'>"
        f"<input value='{html.escape(text)}' style='flex:1;padding:6px 8px;'/>"
        f"<button onclick=\"navigator.clipboard.writeText('{safe}')\">ë³µì‚¬</button>"
        f"</div>", height=46)

# v23 ë§ˆì§„ ê³µì‹
def margin_calc_percent(cost_krw, card_pct, market_pct, margin_pct, shipping_krw):
    cf, mf, t = card_pct/100.0, market_pct/100.0, margin_pct/100.0
    denom = max(1e-9, 1 - cf - mf)
    target_rev = (cost_krw + shipping_krw) * (1 + t)
    P = target_rev / denom
    revenue = P * (1 - cf - mf)
    profit = revenue - (cost_krw + shipping_krw)
    return P, profit, (profit/P*100 if P>0 else 0.0)

def margin_calc_add(cost_krw, card_pct, market_pct, add_margin_krw, shipping_krw):
    cf, mf = card_pct/100.0, market_pct/100.0
    denom = max(1e-9, 1 - cf - mf)
    target_rev = (cost_krw + shipping_krw) + add_margin_krw
    P = target_rev / denom
    revenue = P * (1 - cf - mf)
    profit = revenue - (cost_krw + shipping_krw)
    return P, profit, (profit/P*100 if P>0 else 0.0)

@st.cache_data(ttl=900, show_spinner=False)
def get_fx_rate(base_ccy: str) -> float:
    try:
        r = requests.get(f"https://api.exchangerate.host/latest?base={base_ccy}&symbols=KRW", timeout=8)
        if r.status_code == 200:
            return float(r.json()["rates"]["KRW"])
    except Exception:
        pass
    return {"USD":1400.0,"EUR":1500.0,"JPY":9.5,"CNY":190.0}.get(base_ccy,1400.0)

def readonly_money(label: str, value_krw: float, key: str):
    st.text_input(label, f"â‚©{value_krw:,.0f} KRW", disabled=True, key=key)

COMMON_HEADERS = {
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
    "Accept-Language":"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}
st.title("ğŸš€ ENVY v27.6 Full (Rakuten APIå…§)")

# -------------------- Sidebar (ì ˆëŒ€ ê³ ì •) --------------------
with st.sidebar:
    st.header("â‘  í™˜ìœ¨ ê³„ì‚°ê¸°")
    fx_ccy = st.selectbox("ê¸°ì¤€ í†µí™”", FX_ORDER, index=0, key="sb_fx_base")
    fx_rate = get_fx_rate(fx_ccy)
    st.caption(f"ìë™ í™˜ìœ¨: 1 {fx_ccy} = {fx_rate:,.2f} â‚©")
    fx_price = st.number_input(f"íŒë§¤ê¸ˆì•¡ ({fx_ccy})", 0.0, 1e12, 100.0, 1.0, key="sb_fx_price_foreign")
    readonly_money("í™˜ì‚° ê¸ˆì•¡(ì½ê¸°ì „ìš©)", fx_price*fx_rate, key="sb_fx_price_krw")

    st.markdown("---")
    st.header("â‘¡ ë§ˆì§„ ê³„ì‚°ê¸° (v23)")
    m_ccy = st.selectbox("ê¸°ì¤€ í†µí™”(íŒë§¤ê¸ˆì•¡)", FX_ORDER, index=0, key="sb_m_base")
    m_rate = get_fx_rate(m_ccy)
    st.caption(f"ìë™ í™˜ìœ¨: 1 {m_ccy} = {m_rate:,.2f} â‚©")
    sale_foreign = st.number_input(f"íŒë§¤ê¸ˆì•¡ ({m_ccy})", 0.0, 1e12, 100.0, 1.0, key="sb_m_sale_foreign")
    sale_krw = sale_foreign * m_rate
    readonly_money("í™˜ì‚° ê¸ˆì•¡(ì½ê¸°ì „ìš©)", sale_krw, key="sb_m_sale_krw")
    card = st.number_input("ì¹´ë“œìˆ˜ìˆ˜ë£Œ (%)", 0.0, 100.0, 4.0, 0.1, key="sb_card")
    market = st.number_input("ë§ˆì¼“ìˆ˜ìˆ˜ë£Œ (%)", 0.0, 100.0, 14.0, 0.1, key="sb_market")
    ship = st.number_input("ë°°ì†¡ë¹„ (â‚©)", 0.0, 1e9, 0.0, 100.0, key="sb_ship")
    mode = st.radio("ë§ˆì§„ ë°©ì‹", ["í¼ì„¼íŠ¸ ë§ˆì§„(%)","ë”í•˜ê¸° ë§ˆì§„(â‚©)"], horizontal=True, key="sb_mode")
    if mode=="í¼ì„¼íŠ¸ ë§ˆì§„(%)":
        margin_pct = st.number_input("ë§ˆì§„ìœ¨ (%)", 0.0, 500.0, 10.0, 0.1, key="sb_margin_pct")
        P, profit, on_sale = margin_calc_percent(sale_krw, card, market, margin_pct, ship)
    else:
        add_margin = st.number_input("ë”í•˜ê¸° ë§ˆì§„ (â‚©)", 0.0, 1e12, 10000.0, 100.0, key="sb_add_margin")
        P, profit, on_sale = margin_calc_add(sale_krw, card, market, add_margin, ship)
    st.metric("íŒë§¤ê°€ê²© (ê³„ì‚° ê²°ê³¼)", f"â‚©{P:,.0f}")
    st.metric("ìˆœì´ìµ(ë§ˆì§„)", f"â‚©{profit:,.0f}")
    st.caption(f"ë§ˆì§„ìœ¨(íŒë§¤ê°€ ê¸°ì¤€): {on_sale:.2f}%")

# -------------------- DataLab (ì„¸ì…˜/ì¿ í‚¤ + ê°•í—¤ë”) --------------------
def fetch_datalab_top20(cid: str, start_date: str, end_date: str, time_unit: str="date") -> pd.DataFrame:
    s = requests.Session()
    cat_url = "https://datalab.naver.com/shoppingInsight/sCategory.naver"
    s.get(cat_url, headers={**COMMON_HEADERS, "Accept":"text/html,*/*"}, timeout=10)
    api_url = "https://datalab.naver.com/shoppingInsight/getCategoryKeywordRank.naver"
    headers = {
        **COMMON_HEADERS,
        "Accept":"application/json, text/javascript, */*; q=0.01",
        "Content-Type":"application/x-www-form-urlencoded; charset=UTF-8",
        "Origin":"https://datalab.naver.com",
        "Referer":cat_url,
        "X-Requested-With":"XMLHttpRequest",
    }
    payload = {"cid":cid,"timeUnit":time_unit,"startDate":start_date,"endDate":end_date,
               "device":"pc","gender":"","ages":""}
    r = s.post(api_url, headers=headers, data=payload, timeout=12)
    if r.status_code != 200:
        raise RuntimeError(f"DataLab ì‘ë‹µ ì˜¤ë¥˜: {r.status_code}")
    txt = r.text.strip()
    if not txt or not (txt.startswith("{") or txt.startswith("[")):
        raise RuntimeError("DataLab JSON ì•„ë‹˜(ì°¨ë‹¨/êµ¬ì¡°ë³€ê²½ ê°€ëŠ¥ì„±)")
    data = r.json()
    if "keywordList" not in data or not isinstance(data["keywordList"], list):
        raise RuntimeError("DataLab êµ¬ì¡° ë³€ê²½ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
    rows=[]
    for it in data["keywordList"][:20]:
        rows.append({"rank": it.get("rank") or len(rows)+1,
                     "keyword": it.get("keyword",""),
                     "search": it.get("ratio") or 0})
    return pd.DataFrame(rows).sort_values("rank").reset_index(drop=True)

# ë³¸ë¬¸ ìƒë‹¨ 3ê°œ: ë°ì´í„°ë© â†’ ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ â†’ ì…€ëŸ¬ë¼ì´í”„ (ì¢Œâ†’ìš°)
c1, c2, c3 = st.columns(3)

with c1:
    st.subheader("ë°ì´í„°ë©")
    category = st.selectbox("ì¹´í…Œê³ ë¦¬", list(NAVER_CATEGORIES.keys()), index=0, key="dl_cat")
    cid = NAVER_CATEGORIES[category]
    today = datetime.date.today()
    start = (today - datetime.timedelta(days=30)).strftime("%Y-%m-%d")
    end = today.strftime("%Y-%m-%d")
    try:
        df_dl = fetch_datalab_top20(cid, start, end)
        st.dataframe(df_dl, use_container_width=True)
        st.session_state["datalab_df"] = df_dl.copy()
        chart = alt.Chart(df_dl).mark_line().encode(
            x=alt.X("rank:Q", title="ë­í¬(1=ìƒìœ„)"),
            y=alt.Y("search:Q", title="ê²€ìƒ‰ëŸ‰(ì§€ìˆ˜)"),
            tooltip=["rank","keyword","search"]
        ).properties(height=220)
        st.altair_chart(chart, use_container_width=True)
        st.download_button("Top20 CSV", to_csv_bytes(df_dl), "datalab_top20.csv", mime="text/csv", key="dl_csv")
    except Exception as e:
        st.error(f"ë°ì´í„°ë© ì˜¤ë¥˜: {e}")

with c2:
    st.subheader("ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸")
    st.info("ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ì—°ë™ ëŒ€ê¸°(ë³„ë„ API/í”„ë¡ì‹œ ì—°ê²° ì˜ˆì •)")

with c3:
    st.subheader("ì…€ëŸ¬ë¼ì´í”„")
    st.info("ì…€ëŸ¬ë¼ì´í”„ ì—°ë™ ëŒ€ê¸°(ë³„ë„ API/í”„ë¡ì‹œ ì—°ê²° ì˜ˆì •)")
# -------------------- Amazon (HTML íŒŒì‹± ê·¸ëŒ€ë¡œ) --------------------
@st.cache_data(ttl=600, show_spinner=False)
def fetch_amazon_bestsellers(limit:int=15) -> pd.DataFrame:
    url = "https://www.amazon.com/Best-Sellers/zgbs"
    headers = {**COMMON_HEADERS, "Referer":"https://www.amazon.com/"}
    r = requests.get(url, headers=headers, timeout=12)
    if r.status_code != 200:
        raise RuntimeError(f"Amazon ì‘ë‹µ ì˜¤ë¥˜: {r.status_code}")
    soup = BeautifulSoup(r.text, "html.parser")
    titles=[]
    for sel in [
        "div.p13n-sc-truncate",
        "div._cDEzb_p13n-sc-css-line-clamp-3_g3dy1",
        "div._cDEzb_p13n-sc-css-line-clamp-2_EWgCb",
        "div.a-section.a-spacing-small > h3, div.a-section.a-spacing-small > a > span",
        "span.zg-text-center-align > div > a > div",
    ]:
        for el in soup.select(sel):
            t = re.sub(r"\s+"," ", el.get_text(strip=True))
            if t and t not in titles:
                titles.append(t)
            if len(titles) >= limit:
                break
        if len(titles) >= limit:
            break
    if not titles:
        raise RuntimeError("Amazon íŒŒì‹± ì‹¤íŒ¨(êµ¬ì¡°ë³€ê²½/ì°¨ë‹¨ ê°€ëŠ¥)")
    df = pd.DataFrame({"rank":range(1,len(titles)+1), "keyword":titles[:limit]})
    df["score"] = [300-i for i in range(1,len(df)+1)]
    df["source"] = "Amazon US"
    return df[["source","rank","keyword","score"]]

# -------------------- Rakuten ê³µì‹ Ranking API --------------------
@st.cache_data(ttl=600, show_spinner=False)
def fetch_rakuten_ranking_api(app_id: str, genre_id: str|None=None, period: str="day", limit:int=15) -> pd.DataFrame:
    """
    Rakuten Ichiba Item Ranking API (ì •ì‹)
    https://app.rakuten.co.jp/services/api/IchibaItem/Ranking/20170628
    - applicationId í•„ìˆ˜
    - genreId ì„ íƒ(ì—†ìœ¼ë©´ ì¢…í•© ë­í‚¹)
    - periodType: 'day' or 'week'
    """
    url = "https://app.rakuten.co.jp/services/api/IchibaItem/Ranking/20170628"
    params = {"applicationId": app_id, "format":"json", "periodType": period}
    if genre_id:
        params["genreId"] = genre_id
    r = requests.get(url, params=params, timeout=12)
    if r.status_code != 200:
        raise RuntimeError(f"Rakuten API ì˜¤ë¥˜: {r.status_code} / {r.text[:120]}")
    js = r.json()
    items = js.get("Items", [])
    rows=[]
    for it in items[:limit]:
        I = it.get("Item", {})
        rows.append({"rank": I.get("rank"),
                     "keyword": I.get("itemName"),
                     "score": 220 - (I.get("rank") or len(rows)+1)})
    df = pd.DataFrame(rows)
    if len(df)==0:
        raise RuntimeError("Rakuten API ì‘ë‹µì— í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    df["source"] = "Rakuten JP"
    return df[["source","rank","keyword","score"]]

# -------------------- ë³¸ë¬¸ í•˜ë‹¨ ì¢Œ: AI ë ˆì´ë” --------------------
d1, d2, d3 = st.columns(3)

with d1:
    st.subheader("AI í‚¤ì›Œë“œ ë ˆì´ë” (êµ­ë‚´/ê¸€ë¡œë²Œ)")
    mode = st.radio("ëª¨ë“œ", ["êµ­ë‚´","ê¸€ë¡œë²Œ"], horizontal=True, key="air_mode")
    if mode=="êµ­ë‚´":
        src = st.session_state.get("datalab_df")
        if src is not None:
            radar = (src.assign(source="DataLab", score=lambda x: 1000 - x["rank"]*10)
                      [["source","keyword","score","rank"]].sort_values(["score","rank"], ascending=[False,True]))
            st.dataframe(radar, use_container_width=True)
            st.download_button("êµ­ë‚´ í‚¤ì›Œë“œ CSV", to_csv_bytes(radar), "radar_domestic.csv",
                               mime="text/csv", key="air_csv_dom")
        else:
            st.info("ë°ì´í„°ë© ê²°ê³¼ê°€ ì—†ì–´ í‘œì‹œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ì„ íƒ: ë¼ì¿ í… ì¥ë¥´ID (ì—†ìœ¼ë©´ ì¢…í•© ë­í‚¹)
        rak_genre = st.text_input("Rakuten genreId (ì„ íƒ, ë¹„ìš°ë©´ ì¢…í•©)", "", key="rak_genre")
        try:
            df_amz = fetch_amazon_bestsellers(15)
        except Exception as e:
            st.error(f"Amazon ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            df_amz = pd.DataFrame(columns=["source","rank","keyword","score"])
        try:
            df_rak = fetch_rakuten_ranking_api(RAKUTEN_APP_ID, genre_id=(rak_genre or None), period="day", limit=15)
        except Exception as e:
            st.error(f"Rakuten API ì‹¤íŒ¨: {e}")
            df_rak = pd.DataFrame(columns=["source","rank","keyword","score"])

        df_glb = pd.concat([df_amz, df_rak], ignore_index=True)
        if len(df_glb):
            df_glb = df_glb.sort_values(["score","rank"], ascending=[False, True])
            st.dataframe(df_glb, use_container_width=True)
            st.download_button("ê¸€ë¡œë²Œ í‚¤ì›Œë“œ CSV", to_csv_bytes(df_glb), "radar_global.csv",
                               mime="text/csv", key="air_csv_glb")
        else:
            st.info("ê¸€ë¡œë²Œ ì†ŒìŠ¤ ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
# -------------------- ë³¸ë¬¸ í•˜ë‹¨ ì¤‘: 11ë²ˆê°€ --------------------
with d2:
    st.subheader("11ë²ˆê°€ (ëª¨ë°”ì¼ í”„ë¡ì‹œ + ìš”ì•½í‘œ)")
    url = st.text_input("ëŒ€ìƒ URL", "https://www.11st.co.kr/", key="m11_url")
    proxy = st.text_input("í”„ë¡ì‹œ ì—”ë“œí¬ì¸íŠ¸(ì„ íƒ)", "", key="m11_proxy",
                          help="ì˜ˆ) https://your-proxy/app?target=<m.11st url>")
    src_url = (f"{proxy}?target={urllib.parse.quote(url.replace('www.11st.co.kr','m.11st.co.kr'), safe='')}"
               if proxy else url.replace("www.11st.co.kr","m.11st.co.kr"))
    st.components.v1.html(
        f"<div style='width:100%;height:500px;border:1px solid #eee;border-radius:10px;overflow:hidden'>"
        f"<iframe src='{src_url}' width='100%' height='100%' frameborder='0' "
        f"sandbox='allow-same-origin allow-scripts allow-popups allow-forms'></iframe></div>", height=520)

    df_11 = pd.DataFrame({
        "title":[f"ìƒí’ˆ{i}" for i in range(1,6)],
        "price":[i*1000 for i in range(1,6)],
        "sales":[i*7 for i in range(1,6)],
        "link":[url]*5
    })
    with st.expander("ì„ë² ë“œ ì‹¤íŒ¨ ëŒ€ë¹„ ìš”ì•½í‘œ ë³´ê¸°"):
        st.dataframe(df_11, use_container_width=True)
        st.download_button("CSV ë‹¤ìš´ë¡œë“œ", to_csv_bytes(df_11), "11st_list.csv", mime="text/csv", key="m11_csv")

# -------------------- ë³¸ë¬¸ í•˜ë‹¨ ìš°: ìƒí’ˆëª… ìƒì„±ê¸° --------------------
with d3:
    st.subheader("ìƒí’ˆëª… ìƒì„±ê¸°")
    brand = st.text_input("ë¸Œëœë“œ", "envy", key="ng_brand")
    base = st.text_input("ë² ì´ìŠ¤ í‚¤ì›Œë“œ", "K-coffee mix", key="ng_base")
    keywords = st.text_input("ì—°ê´€í‚¤ì›Œë“œ", "Maxim, Kanu, Korea", key="ng_kws")
    badwords = st.text_input("ê¸ˆì¹™ì–´", "copy, fake, replica", key="ng_bans")
    limit = st.slider("ê¸€ììˆ˜ ì œí•œ", 20, 120, 80, key="ng_limit")
    gen_mode = st.radio("ëª¨ë“œ", ["ê·œì¹™ ê¸°ë°˜","HuggingFace AI"], horizontal=True, key="ng_mode")

    def filter_and_trim(cands):
        bans = {w.strip().lower() for w in badwords.split(",") if w.strip()}
        out=[]
        for t in cands:
            t2 = " ".join(t.split())
            if any(b in t2.lower() for b in bans): continue
            if len(t2)>limit: t2=t2[:limit]
            out.append(t2)
        return out

    cands=[]
    if st.button("ìƒì„±", key="ng_go"):
        kws=[k.strip() for k in keywords.split(",") if k.strip()]
        if gen_mode=="ê·œì¹™ ê¸°ë°˜":
            for _ in range(5):
                pref=random.choice(["[New]","[Hot]","[Korea]"])
                suf=random.choice(["2025","FastShip","HotDeal"])
                join=random.choice([" | "," Â· "," - "])
                cands.append(f"{pref} {brand}{join}{base} {', '.join(kws[:2])} {suf}")
        else:
            if not HF_API_KEY:
                st.error("HuggingFace í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                API_URL = "https://api-inference.huggingface.co/models/skt/kogpt2-base-v2"
                headers = {"Authorization": f"Bearer {HF_API_KEY}", "X-Wait-For-Model": "true"}
                prompt = f"ìƒí’ˆëª… ì¶”ì²œ 5ê°œ: ë¸Œëœë“œ={brand}, ë² ì´ìŠ¤={base}, í‚¤ì›Œë“œ={keywords}. í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ."
                try:
                    resp = requests.post(API_URL, headers=headers,
                        json={"inputs": prompt, "parameters": {"max_new_tokens": 64, "return_full_text": False}},
                        timeout=30)
                    if resp.status_code==200:
                        data = resp.json()
                        text = data[0].get("generated_text","") if isinstance(data,list) and data else str(data)
                        lines = [ln.strip("-â€¢ ").strip() for ln in text.split("\n") if ln.strip()]
                        if len(lines)<5:
                            lines = [s.strip() for s in textwrap.fill(text, 120).split(".") if s.strip()]
                        cands = lines[:5]
                    else:
                        st.error(f"HuggingFace API ì˜¤ë¥˜: {resp.status_code} / {resp.text[:160]}")
                except Exception as e:
                    st.error(f"HuggingFace í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        st.session_state["name_cands"]=filter_and_trim(cands)

    for i,t in enumerate(st.session_state.get("name_cands", []), start=1):
        st.write(f"{i}. {t}")
        copy_button(t, key=f"name_{i}")
