# part_crawler.py
# ENVY â€” Season 1 add-on section (ë³¸ ì„¹ì…˜ë§Œ ì¶”ê°€/êµì²´, ê¸°ì¡´ ì‚¬ì´ë“œë°”Â·ë ˆì´ì•„ì›ƒÂ·í™˜ìœ¨/ë§ˆì§„ ê³„ì‚°ê¸°Â·PROXY_URL ë¸”ë¡ì€ ì ˆëŒ€ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ)
import os, time, json, random, traceback
from typing import List
import streamlit as st

# â”€â”€ ENVY ê³ ì • ê³µì§€(í”„ë¡ì‹œ ê´€ë ¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ í¬ë¡¤ëŸ¬ íŒŒíŠ¸ëŠ” í”„ë¡ì‹œ ë¶ˆí•„ìš”. ë‹¤ë§Œ ENVY ì „ë°˜ ìš´ì˜ ê·œì¹™ìƒ 11ë²ˆê°€(ëª¨ë°”ì¼) ì„ë² ë“œëŠ” í•­ìƒ í”„ë¡ì‹œ ê²½ìœ ê°€ í•„ìˆ˜.
# ê¸°ë³¸ í”„ë¡ì‹œ ì£¼ì†Œ: https://envy-proxy.taesig0302.workers.dev/  (ì•±ì—ì„œëŠ” PROXY_URLë§Œ ì‚¬ìš©)
PROXY_INFO = "âš ï¸ 11ë²ˆê°€(ëª¨ë°”ì¼) ì„ë² ë“œëŠ” í•­ìƒ í”„ë¡ì‹œ ê²½ìœ : https://envy-proxy.taesig0302.workers.dev/ (ì•±ì€ PROXY_URL í™˜ê²½ë§Œ ì°¸ì¡°)"

# â”€â”€ í¬ë¡¤ëŸ¬ ì½”ì–´ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import crawler_core as core
except Exception as e:
    core = None

def _ensure_core_ready():
    if core is None:
        st.error("crawler_core.py ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë‹¨ë… ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê°™ì€ í´ë”ì— `crawler_core.py`ë¡œ ì €ì¥í•˜ì„¸ìš”.")
        st.stop()

# â”€â”€ UI ì„¹ì…˜ ë Œë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_crawler_section():
    st.markdown("## ğŸ§² ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ í¬ë¡¤ëŸ¬")
    st.caption("URL ì…ë ¥ â†’ ìƒí’ˆëª…/ê°€ê²©/ì´ë¯¸ì§€ ìˆ˜ì§‘ â†’ ì—‘ì…€/ì´ë¯¸ì§€ í´ë” ì €ì¥. ë™ì‘ ì¤‘ í˜ì´ì§€ ì „í™˜/ìƒˆë¡œê³ ì¹¨ ê¸ˆì§€.")
    st.info(PROXY_INFO, icon="ğŸ”")

    _ensure_core_ready()

    with st.expander("ì˜µì…˜", expanded=True):
        urls_text = st.text_area(
            "í¬ë¡¤ë§í•  ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì¹´í…Œê³ ë¦¬/ê²€ìƒ‰ URL (ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—¬ëŸ¬ ê°œ)",
            value="\n".join(core.MARKETS) if hasattr(core, "MARKETS") else "",
            height=120,
            help="ì˜ˆ) https://smartstore.naver.com/ìŠ¤í† ì–´ID/category/...&page=1&size=60"
        )
        download_images = st.checkbox("ì´ë¯¸ì§€ ì €ì¥(DOWNLOAD_IMAGES)", value=getattr(core, "DOWNLOAD_IMAGES", True))
        headless = st.checkbox("í—¤ë“œë¦¬ìŠ¤(ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€)", value=True)
        autosave_every = st.number_input("ìë™ ì €ì¥ ê°„ê²©(AUTOSAVE_EVERY)", min_value=1, max_value=100, value=getattr(core, "AUTOSAVE_EVERY", 10), step=1)
        wait_sec = st.number_input("ëŒ€ê¸°ì‹œê°„(ì´ˆ)", min_value=3, max_value=60, value=getattr(core, "WAIT_SEC", 12), step=1)
        pageload_timeout = st.number_input("í˜ì´ì§€ë¡œë“œ íƒ€ì„ì•„ì›ƒ(ì´ˆ)", min_value=10, max_value=120, value=getattr(core, "PAGELOAD_TIMEOUT", 25), step=1)

        base_dir = st.text_input("BASE_DIR", value=getattr(core, "BASE_DIR", r"C:\singlefiless"))
        excel_path = st.text_input("ì—‘ì…€ ê²½ë¡œ", value=getattr(core, "EXCEL_PATH", os.path.join(base_dir, "ë„¤ì´ë²„_ìƒí’ˆì •ë³´_ìˆ˜ì§‘_í™•ì¥.xlsx")))

    urls: List[str] = [u.strip() for u in urls_text.splitlines() if u.strip()]
    log_box = st.empty()
    prog = st.progress(0, text="ëŒ€ê¸° ì¤‘â€¦")
    run = st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘", type="primary", use_container_width=True)

    if run:
        core.BASE_DIR = base_dir
        core.EXCEL_PATH = excel_path
        core.DOWNLOAD_IMAGES = bool(download_images)
        core.AUTOSAVE_EVERY = int(autosave_every)
        core.WAIT_SEC = int(wait_sec)
        core.PAGELOAD_TIMEOUT = int(pageload_timeout)

        from selenium.webdriver.chrome.options import Options
        def _make_driver_with_headless():
            d, w = core.make_driver()
            if headless:
                try:
                    d.quit()
                except: pass
                opts = Options()
                opts.add_argument("--disable-blink-features=AutomationControlled")
                opts.add_argument("--lang=ko-KR")
                opts.add_argument("start-maximized")
                opts.add_argument(f"user-agent={core.UA}")
                opts.add_argument("--headless=new")
                try:
                    d = core.webdriver.Chrome(options=opts)
                except:
                    from selenium.webdriver.chrome.service import Service
                    d = core.webdriver.Chrome(service=Service(core.CHROMEDRIVER_PATH), options=opts)
                d.set_page_load_timeout(core.PAGELOAD_TIMEOUT)
                d.execute_cdp_cmd(
                    "Page.addScriptToEvaluateOnNewDocument",
                    {"source": "Object.defineProperty(navigator,'webdriver',{get:()=>undefined});"}
                )
                w = core.WebDriverWait(d, core.WAIT_SEC)
            return d, w

        try:
            driver, wait = _make_driver_with_headless()
        except Exception as e:
            st.error(f"ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
            st.stop()

        done = 0
        total = max(len(urls), 1)
        start_time = time.time()

        try:
            for u in urls:
                prog.progress(done / total, text=f"ë¡œë”© ì¤‘â€¦ ({done}/{total})")
                log_box.write(f"### â–¶ {u}")
                try:
                    core.collect_one_market(driver, wait, u)
                except Exception as e:
                    log_box.error(f"[ì—ëŸ¬] {u}\n{e}\n{traceback.format_exc()}")
                done += 1
                prog.progress(done / total, text=f"ì§„í–‰ {done}/{total}")
                time.sleep(random.uniform(0.3, 0.8))

            try:
                core.safe_save_workbook(core.wb, core.EXCEL_PATH)
            except Exception as e:
                log_box.warning(f"[ì €ì¥ ê²½ê³ ] ìµœì¢… ì €ì¥ ì¬ì‹œë„ ì¤‘: {e}")

            dur = time.time() - start_time
            st.success(f"ì™„ë£Œ. {done}/{total} URL ì²˜ë¦¬, ì†Œìš” {dur:0.1f}s")
            if os.path.exists(core.EXCEL_PATH):
                st.link_button("ì—‘ì…€ ì—´ê¸°", url=f"file:///{core.EXCEL_PATH}".replace("\\", "/"), use_container_width=True)
            st.caption(f"ì—‘ì…€: {core.EXCEL_PATH}")
            img_root = getattr(core, "IMG_ROOT", os.path.join(core.BASE_DIR, "images"))
            st.caption(f"ì´ë¯¸ì§€ ì €ì¥ ë£¨íŠ¸: {img_root}")
        finally:
            try:
                driver.quit()
            except:
                pass

if __name__ == "__main__":
    st.set_page_config(page_title="ENVY â€” SmartStore Crawler", layout="wide")
    render_crawler_section()
