# -*- coding: utf-8 -*-
import os, re, json, time, random, html, hashlib
from typing import List, Dict, Tuple
from urllib.parse import urlparse
import openpyxl, requests
from requests.adapters import HTTPAdapter, Retry
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = r'C:\singlefiless'
IMG_ROOT = os.path.join(BASE_DIR, 'images')
EXCEL_PATH = os.path.join(BASE_DIR, 'ë„¤ì´ë²„_ìƒí’ˆì •ë³´_ìˆ˜ì§‘_í™•ì¥.xlsx')
os.makedirs(IMG_ROOT, exist_ok=True)
os.makedirs(os.path.dirname(EXCEL_PATH), exist_ok=True)

DOWNLOAD_IMAGES = True
WAIT_SEC = 12
PAGELOAD_TIMEOUT = 25
LOAD_RETRIES = 2
UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/121.0.0.0 Safari/537.36"

CHROMEDRIVER_PATH = r"C:/PATH/to/chromedriver.exe"
MARKETS = [
    "https://smartstore.naver.com/nutri_health/category/a932ac29907c43a9bda9c1e60ab0e244?st=TOTALSALE&dt=IMAGE&page=1&size=60",
]
AUTOSAVE_EVERY = 10

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¸Œë¼ìš°ì €
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_driver() -> Tuple[webdriver.Chrome, WebDriverWait]:
    opts = Options()
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_argument("--lang=ko-KR")
    opts.add_argument("start-maximized")
    opts.add_argument(f"user-agent={UA}")
    service = Service(CHROMEDRIVER_PATH)
    d = webdriver.Chrome(service=service, options=opts)
    d.set_page_load_timeout(PAGELOAD_TIMEOUT)
    d.execute_cdp_cmd(
        "Page.addScriptToEvaluateOnNewDocument",
        {"source": "Object.defineProperty(navigator,'webdriver',{get:()=>undefined});"}
    )
    return d, WebDriverWait(d, WAIT_SEC)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—‘ì…€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
wb = openpyxl.Workbook()
PLACEHOLDER_NAME = "_init"
wb.active.title = PLACEHOLDER_NAME

def ensure_ws(name: str):
    safe = re.sub(r'[\/:*?"<>|]', "_", name)[:31]
    ws = wb.create_sheet(safe)
    ws.append([
        "ë²ˆí˜¸","ìƒí’ˆë²ˆí˜¸","ìƒí’ˆëª…","ì •ê°€","í• ì¸ê°€","ë°°ì†¡ë¹„","ë¦¬ë·°ìˆ˜","í‰ì ",
        "ìƒì„¸í˜ì´ì§€ URL","ì›ë³¸ìƒí’ˆID","ì´ë¯¸ì§€í´ë”","ì²«ì´ë¯¸ì§€","ì €ì¥íŒŒì¼ëª©ë¡","ì´ë¯¸ì§€ìˆ˜"
    ])
    if PLACEHOLDER_NAME in wb.sheetnames and len(wb.sheetnames) > 1:
        del wb[PLACEHOLDER_NAME]
    return ws

def safe_save_workbook(wb, path: str):
    tmp = path + ".tmp"
    wb.save(tmp)
    os.replace(tmp, path)
    if os.path.exists(tmp):
        try: os.remove(tmp)
        except: pass

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
IMG_EXT_RE = re.compile(r'\.(?:jpg|jpeg|png|gif|bmp)(?:\?|$)', re.IGNORECASE)
PROD_ID_RE = re.compile(r"/products/(\d+)")

def hash_md5(s: str): return hashlib.md5(s.encode("utf-8")).hexdigest()

def extract_product_id(href: str, kv: Dict[str, str]) -> str:
    for k in ("chnl_prod_no","chnl_prod_id","productNo","chnlPordNo"):
        v = kv.get(k)
        if v: return str(v).strip()
    m = PROD_ID_RE.search(href or "")
    return m.group(1) if m else hash_md5(href)[:10]

def market_id_from_url(url: str) -> str:
    path = urlparse(url).path.strip('/')
    return re.sub(r'[^0-9a-zA-Z_ã„±-í£-]', '_', path.split('/')[0]) or "market"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒí’ˆ ëª©ë¡ ë¡œë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_products_loaded(driver: webdriver.Chrome, wait: WebDriverWait, retries: int = LOAD_RETRIES) -> bool:
    sel = (By.CSS_SELECTOR, 'a.linkAnchor[data-shp-contents-dtl]')
    for _ in range(retries):
        try:
            wait.until(EC.presence_of_all_elements_located(sel))
            return True
        except TimeoutException:
            try: driver.refresh()
            except: pass
            time.sleep(1)
    return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ë¯¸ì§€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SESSION = requests.Session()
SESSION.mount("https://", HTTPAdapter(max_retries=Retry(total=3, backoff_factor=0.3)))
SESSION.headers.update({"User-Agent": UA})

def save_image_as_seq(url: str, folder: str, prod_no: str, seq: int, referer: str) -> str:
    try:
        os.makedirs(folder, exist_ok=True)
        r = SESSION.get(url, headers={"Referer": referer}, timeout=10)
        if r.status_code != 200 or len(r.content) < 1024:
            return ""
        ext = re.search(r'\.(jpg|jpeg|png|gif|bmp)', url.split("?")[0], re.I)
        ext = "." + (ext.group(1).lower() if ext else "jpg")
        path = os.path.join(folder, f"{prod_no}_{seq:02d}{ext}")
        with open(path, "wb") as f:
            f.write(r.content)
        return path
    except Exception:
        return ""

def _push_candidates_from_img(el, seen: set, out: list):
    cand = [
        el.get_attribute("src"),
        el.get_attribute("data-src"),
        el.get_attribute("data-lazy-src"),
        el.get_attribute("currentSrc"),
    ]
    ss = el.get_attribute("srcset") or ""
    if ss:
        parts = [p.strip().split(" ")[0] for p in ss.split(",") if p.strip()]
        cand.extend(parts)
    for u in cand:
        if not u or not u.startswith("http"): 
            continue
        # í™•ì¥ì ì²´í¬ (ì¿¼ë¦¬ ì´ì „ ê¸°ì¤€)
        if not IMG_EXT_RE.search(u.split("?")[0]): 
            continue
        key = u.lower()  # â—ì¤‘ë³µì€ 'ì™„ì „ ë™ì¼ URL' ê¸°ì¤€ (ì‚¬ì´ì¦ˆ/í”„ë ˆì„ ë¶„ë¦¬)
        if key in seen: 
            continue
        seen.add(key)
        out.append(u)

def _push_background_images(scope_el, seen: set, out: list):
    # style="background-image:url(...)" í˜•íƒœë„ ìˆ˜ì§‘
    els = scope_el.find_elements(By.CSS_SELECTOR, "[style*='background-image']")
    for e in els:
        style = e.get_attribute("style") or ""
        # url("...") ë˜ëŠ” url('...') ë˜ëŠ” url(...)
        m = re.search(r"url\((['\"]?)(.+?)\1\)", style)
        if not m: 
            continue
        u = m.group(2)
        if not u.startswith("http"): 
            continue
        if not IMG_EXT_RE.search(u.split("?")[0]): 
            continue
        key = u.lower()
        if key in seen: 
            continue
        seen.add(key)
        out.append(u)

def collect_card_img_urls(card_el) -> List[str]:
    """
    ìƒí’ˆ ì¹´ë“œ ë‚´ ëª¨ë“  ì´ë¯¸ì§€ URL ìˆ˜ì§‘:
      1) ì¹´ë“œ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ <img>
      2) ê°™ì€ ì¹´ë“œì˜ <li> ë‚´ swiper-wrapper ì „ì²´ <img>
      3) background-image:url(...) í˜•íƒœ
      4) src/data-src/data-lazy-src/currentSrc/srcset ëª¨ë‘
    """
    urls, seen = [], set()

    # 1) ì¹´ë“œ ì»¨í…Œì´ë„ˆ
    try:
        container = card_el.find_element(By.CSS_SELECTOR, '.I3B6dXSHqa, .zslOZxOl9K')
    except Exception:
        container = card_el

    try:
        driver.execute_script("arguments[0].scrollIntoView({block:'center'});", container)
        time.sleep(random.uniform(0.15, 0.45))
    except Exception:
        pass

    for img in container.find_elements(By.CSS_SELECTOR, "img"):
        _push_candidates_from_img(img, seen, urls)
    _push_background_images(container, seen, urls)

    # 2) ê°™ì€ ì¹´ë“œì˜ <li> ê¸°ì¤€ swiper-wrapper ì „ì²´ ì´ë¯¸ì§€ (ë³´ì´ì§€ ì•ŠëŠ” ìŠ¬ë¼ì´ë“œ í¬í•¨)
    try:
        li = card_el.find_element(By.XPATH, "./ancestor::li[1]")
        for img in li.find_elements(By.CSS_SELECTOR, ".swiper-wrapper img"):
            _push_candidates_from_img(img, seen, urls)
        _push_background_images(li, seen, urls)
    except Exception:
        pass

    return urls

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ë¡¤ë§ (ì„¸ì´í”„ê°€ë“œ ì €ì¥ í¬í•¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_one_market(driver: webdriver.Chrome, wait: WebDriverWait, user_url: str):
    market_id = market_id_from_url(user_url)
    ws = ensure_ws(market_id)
    prod_map, last_no = load_index(market_id)

    print(f"\n=== {market_id} ===")
    driver.get(user_url)
    input("âœ… ìº¡ì°¨ í’€ì—ˆìœ¼ë©´ ì—”í„°â€¦ ")
    ok = ensure_products_loaded(driver, wait)
    if not ok:
        print("âš ï¸ ìƒí’ˆ ëª©ë¡ ë¡œë”© ì‹¤íŒ¨")
        return

    cards = driver.find_elements(By.CSS_SELECTOR, 'a.linkAnchor[data-shp-contents-dtl]')
    print("ìƒí’ˆ ìˆ˜:", len(cards))

    row, autosave_counter = 1, 0

    try:
        for tag in cards:
            try:
                detail_json = tag.get_attribute("data-shp-contents-dtl") or ""
                data = json.loads(html.unescape(detail_json)) if detail_json else []
                kv = {d.get("key"): d.get("value") for d in data if isinstance(d, dict)}
                title = kv.get("chnl_prod_nm", "")
                price = kv.get("price", "")
                href = tag.get_attribute("href") or ""
                if not href:
                    continue

                product_id = extract_product_id(href, kv)
                product_no, last_no = assign_product_no(product_id, prod_map, last_no)

                # li ê¸°ì¤€ ë¶€ê°€ í•„ë“œ
                sale_price = shipping_fee = review_count = rating = ""
                try:
                    li = tag.find_element(By.XPATH, "./ancestor::li[1]")
                except:
                    li = None

                if li:
                    # í• ì¸ê°€
                    try:
                        sale_el = li.find_element(By.CSS_SELECTOR, "span.zIK_uvWc6D")
                        sale_price = re.sub(r"[^\d]", "", sale_el.text)
                    except:
                        try:
                            sale_el = li.find_element(By.XPATH, './/*[contains(text(),"ì›") and not(contains(text(),"ë°°ì†¡"))]')
                            sale_price = re.sub(r"[^\d]", "", sale_el.text)
                        except:
                            pass

                    # ë°°ì†¡ë¹„
                    try:
                        if li.find_elements(By.XPATH, './/*[contains(text(),"ë¬´ë£Œë°°ì†¡")]'):
                            shipping_fee = "0"
                        else:
                            fee_nodes = li.find_elements(By.CSS_SELECTOR, "div.UVrxHKBc0E")
                            got_fee = False
                            for node in fee_nodes:
                                txt = (node.text or "").strip()
                                digits = re.sub(r"[^\d]", "", txt)
                                if digits:
                                    shipping_fee = digits
                                    got_fee = True
                                    break
                            if not got_fee:
                                try:
                                    ship_el = li.find_element(By.XPATH, './/*[contains(text(),"ë°°ì†¡")]')
                                    t = ship_el.text
                                    m = re.search(r"(\d[\d,]*)\s*ì›", t)
                                    shipping_fee = m.group(1).replace(",", "") if m else re.sub(r"[^\d]", "", t)
                                except:
                                    shipping_fee = ""
                    except:
                        shipping_fee = ""

                    # ë¦¬ë·°/í‰ì 
                    try:
                        rv = li.find_element(By.XPATH, './/*[contains(text(),"ë¦¬ë·°") or contains(@class,"GF9")]')
                        review_count = re.sub(r"[^\d]", "", rv.text)
                    except: pass
                    try:
                        rt = li.find_element(By.XPATH, './/*[contains(text(),"í‰ì ") or contains(text(),"â˜…")]')
                        m = re.search(r"[\d\.]+", rt.text)
                        rating = m.group(0) if m else ""
                    except: pass

                # ì´ë¯¸ì§€ â€” ì—¬ëŸ¬ ì¥ ëª¨ë‘ ì €ì¥
                img_folder = os.path.join(IMG_ROOT, market_id, product_no)
                urls = collect_card_img_urls(tag)
                saved_rel = []
                if DOWNLOAD_IMAGES and urls:
                    for i, u in enumerate(urls, start=1):
                        p = save_image_as_seq(u, img_folder, product_no, i, referer=user_url)
                        if p:
                            saved_rel.append(os.path.relpath(p, BASE_DIR).replace("\\", "/"))

                first_rel = saved_rel[0] if saved_rel else ""
                folder_rel = os.path.relpath(img_folder, BASE_DIR).replace("\\", "/")

                ws.append([
                    row, product_no, title, price, sale_price, shipping_fee,
                    review_count, rating, href, product_id,
                    folder_rel, first_rel, "; ".join(saved_rel), len(saved_rel)
                ])

                print(f"âœ… {row}: {title} | ì •ê°€:{price} | í• ì¸ê°€:{sale_price} | ë°°ì†¡:{shipping_fee} | ë¦¬ë·°:{review_count} | í‰ì :{rating} | imgs:{len(saved_rel)}")

                row += 1
                autosave_counter += 1
                if autosave_counter >= AUTOSAVE_EVERY:
                    safe_save_workbook(wb, EXCEL_PATH)
                    save_index(market_id, prod_map, last_no)
                    autosave_counter = 0

            except KeyboardInterrupt:
                print("\nğŸŸ¥ ì‚¬ìš©ì ì¤‘ë‹¨ â€” í˜„ì¬ê¹Œì§€ ì§„í–‰ë¶„ ì €ì¥í•©ë‹ˆë‹¤â€¦")
                safe_save_workbook(wb, EXCEL_PATH)
                save_index(market_id, prod_map, last_no)
                raise
            except Exception as e:
                print("[SKIP]", e)
                continue
    finally:
        safe_save_workbook(wb, EXCEL_PATH)
        save_index(market_id, prod_map, last_no)
        print(f"ğŸ“ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {EXCEL_PATH}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¸ë±ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_index(market_id: str) -> Tuple[Dict[str, str], int]:
    path = os.path.join(BASE_DIR, f"product_index_{market_id}.json")
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data.get("map", {}), int(data.get("last_no", 0))
    return {}, 0

def save_index(market_id: str, mapping: Dict[str, str], last_no: int):
    path = os.path.join(BASE_DIR, f"product_index_{market_id}.json")
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump({"map": mapping, "last_no": last_no}, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

def assign_product_no(prod_id: str, mapping: Dict[str, str], last_no: int) -> Tuple[str, int]:
    if prod_id in mapping:
        return mapping[prod_id], last_no
    new_no = last_no + 1
    prod_no = f"P{new_no:06d}"
    mapping[prod_id] = prod_no
    return prod_no, new_no

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    driver, wait = make_driver()
    try:
        for url in MARKETS:
            collect_one_market(driver, wait, url)
        safe_save_workbook(wb, EXCEL_PATH)
        print("\nğŸ’¾ ì—‘ì…€ ì €ì¥ ì™„ë£Œ (ì •ìƒ ì¢…ë£Œ).")
    except KeyboardInterrupt:
        print("\nğŸŸ¥ ë©”ì¸: ì‚¬ìš©ì ì¤‘ë‹¨ ê°ì§€ â€” ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        safe_save_workbook(wb, EXCEL_PATH)
    except Exception as e:
        print(f"\nğŸŸ  ë©”ì¸ ì˜ˆì™¸: {e} â€” ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        safe_save_workbook(wb, EXCEL_PATH)
    finally:
        try:
            safe_save_workbook(wb, EXCEL_PATH)
        except Exception as e:
            print(f"[SAVE ERROR] ìµœì¢… ì €ì¥ ì‹¤íŒ¨: {e}")
        try:
            driver.quit()
        except:
            pass
        print("\nâœ… ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ.")
